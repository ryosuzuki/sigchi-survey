{
  "schemeVersion": 7,
  "cc_licence": "Content of this file is licensed under a CC BY-NC-SA 4.0 license. For details see https://creativecommons.org/licenses/by-nc-sa/4.0/",
  "conference": {
    "id": 10071,
    "shortName": "VRST",
    "year": 2021,
    "startDate": 1638921600000,
    "endDate": 1639094400000,
    "name": "VRST 2021",
    "fullName": "ACM Symposium on Virtual Reality Software and Technology",
    "url": "https://vrst.acm.org/vrst2021/",
    "location": "Osaka, Japan + virtual",
    "timeZoneOffset": 540,
    "timeZoneName": "Asia/Tokyo",
    "logoUrl": "https://files.sigchi.org/conference/logo/60a9b5b5-29f0-678d-6e7c-389053d7492d.png",
    "accessibilityFaqUrl": "https://files.sigchi.org/conference/accessibility/62d2e0b1-0f00-9ad6-cb81-f092cf4d6573.html"
  },
  "sponsors": [
    {
      "id": 10175,
      "name": "Cyber Agent",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/47e2b40f-4e1d-b0c8-033a-46e8e5d43924.png",
      "levelId": 10123,
      "order": 1
    },
    {
      "id": 10176,
      "name": "Association for Computing Machinery",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/dd84478d-2baf-91ff-1003-c4ff4e04f352.png",
      "levelId": 10118,
      "order": 3
    },
    {
      "id": 10177,
      "name": "SIGCHI",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/c315225d-441a-4167-9660-ffa02ed3681d.png",
      "levelId": 10118,
      "order": 7
    },
    {
      "id": 10178,
      "name": "ACM SIGGRAPH",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/1276657b-220d-d65b-d99a-d19b066c9802.png",
      "levelId": 10118,
      "order": 2
    },
    {
      "id": 10179,
      "name": "Virtual Reality Software and Technology",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/ff60dd20-561f-5fff-d00c-246523c31047.png",
      "levelId": 10118,
      "order": 10
    },
    {
      "id": 10180,
      "name": "CG-ARTS",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/42e0ade1-d1b3-a343-a2a9-dd06247bb5d7.png",
      "levelId": 10118,
      "order": 4
    },
    {
      "id": 10181,
      "name": "Tateisi Science and Technology Foundation",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/475d122f-afd7-355a-a2ab-a1b9958fe9e7.png",
      "levelId": 10118,
      "order": 8
    },
    {
      "id": 10182,
      "name": "The Telecommunications Advancement Foundation",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/744a1e71-3a19-bd79-0af0-44b8f5bcd4c4.png",
      "levelId": 10118,
      "order": 9
    },
    {
      "id": 10183,
      "name": "Osaka Electro-Communication University",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/429944ab-9c6e-b578-2550-14482925d3d8.png",
      "levelId": 10118,
      "order": 6
    },
    {
      "id": 10184,
      "name": "Inoue Foundation for Science",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/414a26c0-1c6f-552b-4321-0c7cbfc91a82.png",
      "levelId": 10118,
      "order": 5
    }
  ],
  "sponsorLevels": [
    {
      "id": 10118,
      "name": "Sponsors",
      "rank": 2,
      "isDefault": true
    },
    {
      "id": 10123,
      "name": "Platinum Sponsor",
      "rank": 1,
      "isDefault": false
    }
  ],
  "floors": [],
  "rooms": [
    {
      "id": 10548,
      "name": "Paper Zoom room",
      "typeId": 11912,
      "setup": "Theatre"
    },
    {
      "id": 10549,
      "name": "Demo/poster Gather.Town room",
      "typeId": 11912,
      "setup": "Theatre"
    },
    {
      "id": 10550,
      "name": "Physical venue",
      "typeId": 11924,
      "setup": "Special"
    }
  ],
  "tracks": [
    {
      "id": 11343,
      "name": "VRST 2021 Papers",
      "typeId": 11912
    },
    {
      "id": 11344,
      "name": "VRST 2021 Posters and Demos",
      "typeId": 11924
    },
    {
      "id": 11350,
      "typeId": 11924
    },
    {
      "id": 11351,
      "typeId": 11925
    }
  ],
  "contentTypes": [
    {
      "id": 11905,
      "name": "Course",
      "color": "#66c2a4",
      "duration": 90,
      "displayName": "Courses"
    },
    {
      "id": 11906,
      "name": "Demo",
      "color": "#006d2c",
      "duration": 5,
      "displayName": "Demos"
    },
    {
      "id": 11907,
      "name": "Doctoral Consortium",
      "color": "#6baed6",
      "duration": 5
    },
    {
      "id": 11908,
      "name": "Event",
      "color": "#ffc034",
      "duration": 0,
      "displayName": "Events"
    },
    {
      "id": 11909,
      "name": "Late-Breaking Work",
      "color": "#8e008b",
      "duration": 5
    },
    {
      "id": 11910,
      "name": "Poster",
      "color": "#ff99ca",
      "duration": 5,
      "displayName": "Posters"
    },
    {
      "id": 11911,
      "name": "Work-in-Progress",
      "color": "#7d6bef",
      "duration": 5
    },
    {
      "id": 11912,
      "name": "Paper",
      "color": "#0d42cc",
      "duration": 15,
      "displayName": "Papers"
    },
    {
      "id": 11913,
      "name": "Workshop",
      "color": "#f60000",
      "duration": 240,
      "displayName": "Workshops"
    },
    {
      "id": 11924,
      "name": "Poster/demo",
      "color": "#8e008b",
      "duration": 0
    },
    {
      "id": 11925,
      "name": "Invited talk",
      "color": "#ff7a00",
      "duration": 0
    }
  ],
  "timeSlots": [
    {
      "id": 12197,
      "type": "SESSION",
      "startDate": 1638963000000,
      "endDate": 1638964800000
    },
    {
      "id": 12198,
      "type": "SESSION",
      "startDate": 1638975600000,
      "endDate": 1638981000000
    },
    {
      "id": 12199,
      "type": "SESSION",
      "startDate": 1638982800000,
      "endDate": 1638988200000
    },
    {
      "id": 12200,
      "type": "SESSION",
      "startDate": 1638990000000,
      "endDate": 1638993600000
    },
    {
      "id": 12202,
      "type": "SESSION",
      "startDate": 1639042200000,
      "endDate": 1639048500000
    },
    {
      "id": 12203,
      "type": "SESSION",
      "startDate": 1639049400000,
      "endDate": 1639053000000
    },
    {
      "id": 12204,
      "type": "SESSION",
      "startDate": 1639067400000,
      "endDate": 1639071900000
    },
    {
      "id": 12205,
      "type": "SESSION",
      "startDate": 1639072800000,
      "endDate": 1639078200000
    },
    {
      "id": 12206,
      "type": "SESSION",
      "startDate": 1639128600000,
      "endDate": 1639132200000
    },
    {
      "id": 12207,
      "type": "SESSION",
      "startDate": 1639134000000,
      "endDate": 1639140300000
    },
    {
      "id": 12208,
      "type": "SESSION",
      "startDate": 1639148400000,
      "endDate": 1639153800000
    },
    {
      "id": 12209,
      "type": "SESSION",
      "startDate": 1639154700000,
      "endDate": 1639160100000
    },
    {
      "id": 12210,
      "type": "SESSION",
      "startDate": 1639160100000,
      "endDate": 1639161900000
    },
    {
      "id": 12211,
      "type": "SESSION",
      "startDate": 1638964800000,
      "endDate": 1638968400000
    }
  ],
  "sessions": [
    {
      "id": 66996,
      "name": "Paper 1 Tracking, Rendering, and Social Interaction",
      "typeId": 11912,
      "roomId": 10548,
      "chairIds": [
        67061
      ],
      "contentIds": [
        66989,
        66983,
        66972,
        66990,
        66966,
        66970
      ],
      "timeSlotId": 12198
    },
    {
      "id": 66998,
      "name": "Paper 2 Visualization",
      "typeId": 11912,
      "roomId": 10548,
      "chairIds": [
        67324
      ],
      "contentIds": [
        66960,
        66959,
        66961,
        66965,
        66987,
        66981
      ],
      "timeSlotId": 12199
    },
    {
      "id": 66999,
      "name": "Paper 3 Applications",
      "typeId": 11912,
      "roomId": 10548,
      "chairIds": [
        67320
      ],
      "contentIds": [
        66958,
        66954,
        66986,
        66979,
        66992,
        66991,
        66971
      ],
      "timeSlotId": 12202
    },
    {
      "id": 67000,
      "name": "Paper 4 Interaction Design",
      "typeId": 11912,
      "roomId": 10548,
      "chairIds": [
        67321
      ],
      "contentIds": [
        66951,
        66962,
        66984,
        66953,
        66975,
        66963
      ],
      "timeSlotId": 12205
    },
    {
      "id": 67001,
      "name": "Paper 5 Sensing Devices and Haptics",
      "typeId": 11912,
      "roomId": 10548,
      "chairIds": [
        67322
      ],
      "contentIds": [
        66949,
        66952,
        66968,
        66957,
        66976,
        66967,
        66969
      ],
      "timeSlotId": 12207
    },
    {
      "id": 67003,
      "name": "Paper 6 Perception",
      "typeId": 11912,
      "roomId": 10548,
      "chairIds": [
        67129
      ],
      "contentIds": [
        66964,
        66974,
        66973,
        66977,
        66980,
        66956
      ],
      "timeSlotId": 12208
    },
    {
      "id": 67004,
      "name": "Paper 7 Input Methods",
      "typeId": 11912,
      "roomId": 10548,
      "chairIds": [
        67323
      ],
      "contentIds": [
        66950,
        66978,
        66982,
        66985,
        66988,
        66955
      ],
      "timeSlotId": 12209
    }
  ],
  "events": [
    {
      "id": 66756,
      "name": "Opening",
      "typeId": 11908,
      "roomId": 10548,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1638963000000,
      "endDate": 1638964800000,
      "presenterIds": []
    },
    {
      "id": 67007,
      "name": "Keynote by Sriram Subramanian and Diego Plasencia",
      "typeId": 11908,
      "roomId": 10548,
      "chairIds": [],
      "contentIds": [
        67310
      ],
      "startDate": 1639067400000,
      "endDate": 1639071900000,
      "link": {
        "href": "https://subramaniansri.github.io/",
        "label": "See more"
      },
      "presenterIds": [
        67006,
        67325
      ]
    },
    {
      "id": 67008,
      "name": "Keynote/Live Demo by Aimi Sekiguchi",
      "typeId": 11908,
      "roomId": 10548,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1638964800000,
      "endDate": 1638968400000,
      "link": {
        "href": "https://www.creativevillage.ne.jp/lp/aimi_sekiguchi/",
        "label": "See more"
      },
      "presenterIds": []
    },
    {
      "id": 67009,
      "name": "Closing",
      "typeId": 11908,
      "roomId": 10548,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1639160100000,
      "endDate": 1639161900000,
      "presenterIds": []
    },
    {
      "id": 67010,
      "name": "Poster/Demo 1",
      "typeId": 11908,
      "roomId": 10549,
      "chairIds": [],
      "contentIds": [
        67298,
        67237,
        67277,
        67243,
        67241,
        67245,
        67259,
        67261,
        67265,
        67240,
        67233,
        67250,
        67260,
        67269,
        67256,
        67262,
        67272,
        67268,
        67279,
        67273,
        67276,
        67283
      ],
      "startDate": 1638990000000,
      "endDate": 1638993600000,
      "presenterIds": []
    },
    {
      "id": 67011,
      "name": "Poster/Demo 2",
      "typeId": 11908,
      "roomId": 10549,
      "chairIds": [],
      "contentIds": [
        67274,
        67284,
        67257,
        67285,
        67264,
        67270,
        67290,
        67244,
        67251,
        67255,
        67266,
        67238,
        67235,
        67234,
        67254,
        67275,
        67289,
        67296,
        67246,
        67247,
        67253,
        67263
      ],
      "startDate": 1639049400000,
      "endDate": 1639053000000,
      "presenterIds": []
    },
    {
      "id": 67012,
      "name": "Poster/Demo 3",
      "typeId": 11908,
      "roomId": 10549,
      "chairIds": [],
      "contentIds": [
        67292,
        67295,
        67294,
        67286,
        67280,
        67306,
        67236,
        67242,
        67239,
        67267,
        67288,
        67291,
        67252,
        67258,
        67278,
        67281,
        67282,
        67297,
        67249,
        67248,
        67271,
        67287
      ],
      "startDate": 1639128600000,
      "endDate": 1639132200000,
      "presenterIds": []
    },
    {
      "id": 67307,
      "name": "Industry Session by CyberAgent, Inc.",
      "typeId": 11908,
      "roomId": 10548,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1639065600000,
      "endDate": 1639067400000,
      "link": {
        "href": "https://vrst.acm.org/vrst2021/industry-session/",
        "label": "See more"
      },
      "presenterIds": []
    },
    {
      "id": 67308,
      "name": "In-person Poster/Demo Session (for domestic participants only)",
      "typeId": 11908,
      "roomId": 10550,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1639143000000,
      "endDate": 1639148400000,
      "presenterIds": []
    },
    {
      "id": 67311,
      "name": "Excursion (Lab. Tour at Osaka University for domestic participants)",
      "typeId": 11908,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1639056600000,
      "endDate": 1639063800000,
      "presenterIds": []
    }
  ],
  "contents": [
    {
      "id": 66949,
      "typeId": 11912,
      "title": "RotoWrist: Continuous Infrared Wrist Angle Tracking using a Wristband",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489886"
        }
      },
      "trackId": 11343,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        67001
      ],
      "eventIds": [],
      "abstract": "We introduce RotoWrist, an infrared (IR) light-based solution for continuously and reliably tracking the 2-degree-of-freedom (DoF) relative angle of the wrist with respect to the forearm using a wristband. The tracking system consists of eight time-of-flight (ToF) IR light modules distributed around a wristband. We developed a computationally simple tracking approach to reconstruct the orientation of the wrist without any runtime training, ensuring user and session independence. An evaluation study demonstrated that RotoWrist achieves a cross-user median tracking error of 5.9° in flexion/extension and 6.8° in radial and ulnar deviation with no calibration required as measured with optical ground truth. We further demonstrate the performance of RotoWrist for a pointing task and compare it against ground truth tracking.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Electrical and Computer Engineering"
            }
          ],
          "personId": 66923
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Facebook",
              "dsl": "Facebook Reality Labs"
            }
          ],
          "personId": 66948
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Facebook",
              "dsl": "Facebook Reality Labs"
            }
          ],
          "personId": 66813
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Facebook Inc",
              "dsl": "Facebook Reality Labs"
            }
          ],
          "personId": 66871
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Facebook",
              "dsl": "Facebook Reality Labs"
            }
          ],
          "personId": 66906
        }
      ]
    },
    {
      "id": 66950,
      "typeId": 11912,
      "title": "Pressing a Button You Cannot See: Evaluating Visual Designs to Assist Persons with Low Vision through Augmented Reality",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489873"
        }
      },
      "trackId": 11343,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        67004
      ],
      "eventIds": [],
      "abstract": "Partial vision loss occurs in several medical conditions and affects persons of all ages. \r\nIt compromises many daily activities, such as reading, cutting vegetables, or identifying and accurately pressing buttons, e.g., on ticket machines or ATMs. \r\nTouchscreen interfaces pose a particular challenge because they lack haptic feedback from interface elements and often require people with impaired vision to rely on others for help.\r\nWe propose a smartglasses-based solution to utilize the user's residual vision. \r\nTogether with visually-impaired individuals, we designed assistive augmentations for touchscreen interfaces and evaluated their suitability to guide attention towards interface elements and to increase the accuracy of manual inputs.\r\nWe show that augmentations improve interaction performance and decrease cognitive load, particularly for unfamiliar interface layouts. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "LMU Munich",
              "dsl": ""
            }
          ],
          "personId": 66764
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Dortmund",
              "institution": "TU Dortmund",
              "dsl": "Assistive Technologies"
            }
          ],
          "personId": 66762
        }
      ]
    },
    {
      "id": 66951,
      "typeId": 11912,
      "title": "Image-Based Texture Styling for Motion Effect Rendering",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489854"
        }
      },
      "trackId": 11343,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        67000
      ],
      "eventIds": [],
      "abstract": "A motion platform provides the vestibular stimuli that elicit the sensations of self-motion and thereby improves the immersiveness. A representative example is 4D Ride, which presents a video of POV shots and motion effects synchronized with the camera motion in the video. Previous research efforts resulted in a few automatic motion effect synthesis algorithms for POV shots. Although effective in generating gross motion effects, they do not consider fine features on the ground, such as a rough or bumpy road. In this paper, we propose an algorithm for styling the gross motion effects using a texture image. Our algorithm transforms a texture image into a high-frequency style motion and merges it with the original motion while respecting both perceptual and device constraints. A user study demonstrated that texture styling can increase immersiveness, realism, and harmony.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Pohang-si",
              "institution": "Pohang University of Science and Technology",
              "dsl": "Interaction Laboratory"
            }
          ],
          "personId": 66852
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Pohang",
              "institution": "Pohang University of Science and Technology (POSTECH)",
              "dsl": ""
            }
          ],
          "personId": 66942
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "Gyeongbuk",
              "city": "Pohang",
              "institution": "Pohang University of Science and Technology (POSTECH)",
              "dsl": "Computer Science and Engineering / Interaction Laboratory "
            }
          ],
          "personId": 66932
        }
      ]
    },
    {
      "id": 66952,
      "typeId": 11912,
      "title": "PAIR: Phone as an Augmented Immersive Reality Controller",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489878"
        }
      },
      "trackId": 11343,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        67001
      ],
      "eventIds": [],
      "abstract": "Immersive head-mounted augmented reality allows users to overlay 3D digital content on a user's view of physical reality. Current-generation devices primarily support interaction modalities such as gesture, gaze and voice, which are readily available to most users yet lack precision and tactility, rendering them fatiguing for extended interactions. We propose using smartphones, which are also readily available, as companion devices complementing existing AR interaction modalities. We leverage user familiarity with smartphone interactions, coupled with their support for precise, tactile touch input, to unlock a broad range of interaction techniques. For instance, by combining the phone's spatial position and orientation, touch input and high-resolution screen, the ordinary smartphone can be turned into an interior design palette, touch-enabled catapult or AR-rendered sword. We describe a prototype implementation of our interaction techniques using an off-the-shelf AR headset and smartphone, demonstrate enabled applications, and report on the results of a positional accuracy study.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Vancouver",
              "institution": "The University of British Columbia",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 66879
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Vancouver",
              "institution": "University of British Columbia",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 66783
        }
      ]
    },
    {
      "id": 66953,
      "typeId": 11912,
      "title": "InteractML: Making machine learning accessible for creative practitioners working with movement interaction in immersive media",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489879"
        }
      },
      "trackId": 11343,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        67000
      ],
      "eventIds": [],
      "abstract": "Interactive Machine Learning offers a method for designing movement interaction that supports creators in implementing even complex movement designs in their immersive applications by simply performing them with their bodies. We introduce a new tool, InteractML, and an accompanying ideation method, which makes movement interaction design faster, adaptable and accessible to creators of varying experience and backgrounds, such as artists, dancers and independent game developers. The tool is specifically tailored to non-experts as creators configure and train machine learning models via a node-based graph and VR interface, requiring minimal programming. We aim to democratise machine learning for movement interaction to be used in the development of a range of creative and immersive applications. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "Goldsmiths",
              "dsl": ""
            }
          ],
          "personId": 66887
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "Goldsmith University of London",
              "dsl": "Computing Department"
            }
          ],
          "personId": 66827
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "York",
              "institution": "University of York",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 66931
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "University of the Arts London",
              "dsl": "Creative Computing Institute"
            }
          ],
          "personId": 66895
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Coventry",
              "institution": "Coventry University",
              "dsl": "Centre for Dance Research"
            }
          ],
          "personId": 66807
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "Independent Artist",
              "dsl": "Gibson/Martelli"
            }
          ],
          "personId": 66901
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "Goldsmiths University of London",
              "dsl": ""
            }
          ],
          "personId": 66917
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "University of the Arts London",
              "dsl": "Creative Computing Institute"
            },
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "Goldsmiths University of London",
              "dsl": "Department of Computing"
            }
          ],
          "personId": 66925
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "Goldsmiths, University of London",
              "dsl": "Computing"
            }
          ],
          "personId": 66812
        }
      ]
    },
    {
      "id": 66954,
      "typeId": 11912,
      "title": "The Effect of Increased Body Motion in Virtual Reality on a Placement-Retrieval Task",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489888"
        }
      },
      "trackId": 11343,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66999
      ],
      "eventIds": [],
      "abstract": "Previous work has shown that increased effort and use of one's body can improve memory. When positioning windows inside a virtual reality, does the use of a larger volume, and using one's legs to move around, improve ability to later find the windows? The results of our experiment indicate there can be a modest benefit for spatial memory and retrieval time, but at the cost of increased time spent initially positioning the windows.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Quebec",
              "city": "Montreal",
              "institution": "ETS",
              "dsl": ""
            }
          ],
          "personId": 66784
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Bordeaux",
              "institution": "INRIA",
              "dsl": ""
            }
          ],
          "personId": 66903
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Quebec",
              "city": "Montreal",
              "institution": "ETS",
              "dsl": "Dept Software and IT Engineering"
            }
          ],
          "personId": 66767
        }
      ]
    },
    {
      "id": 66955,
      "typeId": 11912,
      "title": "Actions, not gestures: contextualising embodied controller interactions in immersive virtual reality",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489892"
        }
      },
      "trackId": 11343,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        67004
      ],
      "eventIds": [],
      "abstract": "Modern immersive virtual reality (IVR) often uses embodied controllers for interacting with virtual objects. It is not clear, however, how we should conceptualise these interactions - as gestures or actions. These interactions could be categorised as either gestures, as there is no interaction with a physical object, or as actions, as the experience is convincing enough to cognitively substitute for a physical world action. This distinction is important, as in the physical world, literature has shown that sensorimotor-based learning produces distinct cognitive outcomes depending on whether an experience uses an action (stronger memorisation) or a gesture (more generalizable learning). This study attempts to understand whether sensorimotor-embodied controller interactions in IVR can cognitively be considered as actions or gestures. It does this by comparing verb-learning outcomes between two conditions: (1) where participants move the controllers without touching virtual objects (gesture condition); and (2) where participants move the controllers and manipulate virtual objects (action condition). We found that (1) users can have cognitively distinct outcomes in IVR based on whether the interactions are actions or gestures, with actions providing stronger memorisation outcomes; and (2) embodied controller actions in IVR behave more similarly to actions in the physical-world than gestures in terms of verb memorization benefits.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "Queen Mary University of London",
              "dsl": ""
            }
          ],
          "personId": 66842
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "Queen Mary University of London",
              "dsl": ""
            }
          ],
          "personId": 66939
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "Queen Mary University of London",
              "dsl": "EECS"
            }
          ],
          "personId": 66843
        }
      ]
    },
    {
      "id": 66956,
      "typeId": 11912,
      "title": "Perceived realism of pedestrian crowds trajectories in VR",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489860"
        }
      },
      "trackId": 11343,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        67003
      ],
      "eventIds": [],
      "abstract": "Crowd simulation algorithms play an essential role in populating Virtual Reality (VR) environments (such as urban environments) with several autonomous humanoid agents. \r\nPerceptually plausible trajectories with a low computational cost are significant, especially in untethered and mobile devices, like for example portable VR devices. \r\nPrevious research explores the plausibility and realism of the VR environment on desktop computers but fail to account for the effect of immersion. Unlike desktop simulations, VR simulations use stereography and head tracking to give a sense of complete immersion in the VR environment to users.\r\nThis study explores how immersion affects the perceived realism of crowds' trajectories. We do so by running a psychophysical experiment in which participants rate the realism of real/synthetic trajectories data.\r\nResults show that trajectories from real data and synthetic rectilinear constant speed trajectories achieve the same level of perceived realism.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "University College London",
              "dsl": "Computer Science"
            }
          ],
          "personId": 66775
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "Imperial College London",
              "dsl": "Computer Science"
            }
          ],
          "personId": 66900
        },
        {
          "affiliations": [
            {
              "country": "Cyprus",
              "state": "",
              "city": "Nicosia",
              "institution": "CYENS - Centre of Excellence",
              "dsl": "V-EUPNEA: Living, Breathing Virtual Worlds"
            }
          ],
          "personId": 66924
        },
        {
          "affiliations": [
            {
              "country": "Cyprus",
              "state": "",
              "city": "Nicosia",
              "institution": "CYENS - Centre of Excellence",
              "dsl": ""
            }
          ],
          "personId": 66835
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "Imperial College London",
              "dsl": ""
            }
          ],
          "personId": 66828
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "GE",
              "city": "Genoa",
              "institution": "Istituto Italiano di Tecnologia",
              "dsl": "Visual Geometry and Modelling Lab"
            }
          ],
          "personId": 66815
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "University College London",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 66877
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "Imperial College",
              "dsl": ""
            }
          ],
          "personId": 66947
        }
      ]
    },
    {
      "id": 66957,
      "typeId": 11912,
      "title": "TangibleData: Interactive Data Visualization with Mid-Air Haptics",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489890"
        }
      },
      "trackId": 11343,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        67001
      ],
      "eventIds": [],
      "abstract": "In this paper, we present an interactive 3D data visualization framework called TangibleData. TangibleData adapts hand gestures and mid-air haptics to provide tangible interaction in VR using ultrasound haptic feedback on 3D data visualization. We consider various 3D visualization data sets and provide different data encoding methods for haptic representations. Two user experiments are conducted to evaluate the effectiveness of TangibleData. The first experiment results show that adding a mid-air haptic modality can be beneficial regardless of noise conditions and useful for handling occlusion or discerning density and volume information. The second experiment results further show the strengths and weaknesses of direct touch and indirect touch modes. Our findings can shed light on designing and implementing a tangible interaction on 3D data visualization with mid-air haptic feedback.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Richardson",
              "institution": "The University of Texas at Dallas",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 66847
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Tennessee",
              "city": "KNOXVILLE",
              "institution": "Computer Science and Mathematics Division",
              "dsl": "Oak Ridge National Laboratory"
            }
          ],
          "personId": 66927
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Richardson",
              "institution": "The University of Texas at Dallas",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 66763
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Richardson",
              "institution": "The University of Texas at Dallas",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 66902
        }
      ]
    },
    {
      "id": 66958,
      "typeId": 11912,
      "title": "Towards Context-aware Automatic Haptic Effect Generation for Home Theatre Environments",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489887"
        }
      },
      "trackId": 11343,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66999
      ],
      "eventIds": [],
      "abstract": "The application of haptic technology in entertainment systems, such as Virtual Reality and 4D cinema, enables novel experiences for users and drives the demand for efficient haptic authoring systems. Here, we propose an automatic multimodal vibrotactile content creation pipeline that substantially improves the overall hapto-audiovisual (HAV) experience based on contextual audio and visual content from movies. Our algorithm is implemented on a low-cost system with nine actuators attached to a viewing chair and extracts significant features from video files to generate corresponding haptic stimuli. We implemented this pipeline and used the resulting system in a user study (n=16), quantifying user experience according to the sense of immersion, preference, harmony, and discomfort. The results indicate that the haptic patterns generated by our algorithm complement the movie content and provide an immersive and enjoyable HAV user experience. This further suggests that the pipeline can facilitate the efficient creation of 4D effects and could therefore be applied to improve the viewing experience in home theater environments.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Quebec",
              "city": "Montreal",
              "institution": "McGill University",
              "dsl": "Electrical and Computer Engineering"
            }
          ],
          "personId": 66832
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Quebec",
              "city": "Montreal",
              "institution": "McGill University",
              "dsl": "Department of Electrical and Computer Engineering"
            }
          ],
          "personId": 66886
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Quebec",
              "city": "Montréal",
              "institution": "McGill University",
              "dsl": "Department of Electrical and Computer Engineering"
            }
          ],
          "personId": 66790
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Quebec",
              "city": "Montreal",
              "institution": "McGill University",
              "dsl": "Centre for Intelligent Machines"
            }
          ],
          "personId": 66776
        }
      ]
    },
    {
      "id": 66959,
      "typeId": 11912,
      "title": "Spatial Augmented Reality Visibility and Line-of-Sight Cues for Building Design",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489868"
        }
      },
      "trackId": 11343,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66998
      ],
      "eventIds": [],
      "abstract": "Despite the technological advances in building design, visualizing 3D building layouts can be especially difficult for novice and expert users alike, who must take into account design constraints including line-of-sight and visibility. Using CADwalk, a commercial building design tool that utilizes floor-facing projectors to show 1:1 scale building plans, this work presents and evaluates two floor-based visual cues for assisting with evaluating line-of-sight and visibility. Additionally, we examine the impact of using virtual cameras looking from the inside-out (from user's location to objects of interest) and outside-in (looking from an object of interest's location back towards the user). Results show that floor-based cues led to participants more correctly rating visibility, despite taking longer to complete the task. This is an effective trade-off, given the final outcome (the building design) where accuracy is paramount.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "South Australia",
              "city": "Mawson Lakes",
              "institution": "University of South Australia",
              "dsl": "Australian Research Centre for Interactive and Virtual Environments"
            }
          ],
          "personId": 66873
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "South Australia",
              "city": "Mawson Lakes",
              "institution": "University of South Australia",
              "dsl": "Australian Research Centre for Interactive and Virtual Environments"
            }
          ],
          "personId": 66907
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "South Australia",
              "city": "Mawson Lakes",
              "institution": "University of South Australia",
              "dsl": "Australian Research Centre for Interactive and Virtual Environments"
            }
          ],
          "personId": 66769
        }
      ]
    },
    {
      "id": 66960,
      "typeId": 11912,
      "title": "Investigating the Effect of Sensor Data Visualization Variances in Virtual Reality",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489877"
        }
      },
      "trackId": 11343,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66998
      ],
      "eventIds": [],
      "abstract": "This paper investigates the effect that variances in real-time sensor data, such as resolution, compression and frame-rate, have on humans performing straightforward assembly tasks in a Virtual Reality-based (VR-based) training system. A VR-based training system has been developed that transfers RGB and depth image and constructs colored point clouds data to represent objects in front of a participant. Various parameters that affect sensor data acquisition and visualization of remotely operated robots in the real-world are varied, and the associated task performance is observed. Experimental results conducted by 12 participants performing a cumulative total of 95 VR-guided puzzle assembly tasks showed that a combination of low resolution and uncolored points has the most significant effect on participants' performance, where an increase in both stress and time was observed. Participants mentioned that they needed to rely upon tactile feedback when the pure perceptual feedback was unhelpful. The most insignificant parameter was found to be the resolution of the image and point cloud, which, when varied within the experimental bounds, only resulted in a 5% average change in completion time. Participants also indicated in surveys that they felt their performance had improved, and frustration was reduced when given the color data compared to when they received a point cloud without the true color applied.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "NSW",
              "city": "Chippendale",
              "institution": "University of Technology Sydney",
              "dsl": "Faculty of Engineering and IT"
            }
          ],
          "personId": 66885
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "",
              "city": "Sydney",
              "institution": "University of Technology Sydney",
              "dsl": "School of Mechanical and Mechatronic Engineering"
            }
          ],
          "personId": 66759
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "",
              "city": "Sydney",
              "institution": "University of Technology Sydney",
              "dsl": "Centre for Autonomous System"
            }
          ],
          "personId": 66860
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "New South Wales",
              "city": "Sydney",
              "institution": "University of Technology Sydney",
              "dsl": ""
            }
          ],
          "personId": 66891
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "NSW",
              "city": "Ulitmo",
              "institution": "University of Technology, Sydney",
              "dsl": ""
            }
          ],
          "personId": 66789
        }
      ]
    },
    {
      "id": 66961,
      "typeId": 11912,
      "title": "ImNDT: Immersive Workspace for the Analysis of Multidimensional Material Data From Non-Destructive Testing",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489851"
        }
      },
      "trackId": 11343,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66998
      ],
      "eventIds": [],
      "abstract": "An analysis of large multidimensional volumetric data as generated by non-destructive testing (NDT) techniques, e.g., X-ray computed tomography (XCT), can hardly be evaluated using standard 2D visualization techniques on desktop monitors. The analysis of fiber-reinforced polymers (FRPs) is currently a time-consuming and cognitively demanding task, as FRPs have a complex spatial structure, consisting of several hundred thousand fibers, each having more than twenty different extracted features. This paper presents ImNDT, a novel visualization system, which offers material experts an immersive exploration of multidimensional secondary data of FRPs. Our system is based on a virtual reality (VR) head-mounted device (HMD) to enable fluid and natural explorations through embodied navigation, the avoidance of menus, and manual mode switching. We developed immersive visualization and interaction methods tailored to the characterization of FRPs, such as a Model in Miniature, a similarity network, and a histo-book. An evaluation of our techniques with domain experts showed advantages in discovering structural patterns and similarities. Especially novices can benefit strongly from our intuitive representation and spatial rendering.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "Upper Austria",
              "city": "Wels",
              "institution": "University of Applied Sciences Upper Austria",
              "dsl": "Research Group Computed Tomography"
            },
            {
              "country": "Austria",
              "state": "",
              "city": "Vienna",
              "institution": "TU Wien",
              "dsl": "Institute of Visual Computing & Human-Centered Technology"
            }
          ],
          "personId": 66878
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Vienna",
              "institution": "TU Wien",
              "dsl": "Institute of Visual Computing & Human-Centered Technology"
            },
            {
              "country": "Austria",
              "state": "",
              "city": "Vienna",
              "institution": "VRVis",
              "dsl": ""
            }
          ],
          "personId": 66788
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "Upper Austria",
              "city": "Wels",
              "institution": "University of Applied Sciences Upper Austria",
              "dsl": "Research Group Computed Tomography"
            }
          ],
          "personId": 66821
        }
      ]
    },
    {
      "id": 66962,
      "typeId": 11912,
      "title": "Virtual Rotations for Maneuvering in Immersive Virtual Environments",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489893"
        }
      },
      "trackId": 11343,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        67000
      ],
      "eventIds": [],
      "abstract": "In virtual navigation, maneuvering around an object of interest is a common task which requires simultaneous changes in both rotation and translation. In this paper, we present Anchored Jumping, a teleportation technique for maneuvering that allows the explicit specification of a new viewing direction by selecting a point of interest as part of the target specification process. A first preliminary study showed that naive Anchored Jumping can be improved by an automatic counter rotation that that preserves the user's relative orientation towards their point of interest. In our second, qualitative study, this extended technique was compared with two common approaches to specifying virtual rotations. Our results indicate that Anchored Jumping allows precise and comfortable maneuvering and is compatible with techniques that primarily support virtual exploration and search tasks. Equipped with a combination of such complementary techniques, seated users generally preferred virtual over physical rotations for indoor navigation.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Weimar",
              "institution": "Bauhaus-Universitaet Weimar",
              "dsl": "Virtual Reality and Visualization Research"
            }
          ],
          "personId": 66846
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Weimar",
              "institution": "Bauhaus-Universitaet Weimar",
              "dsl": "Virtual Reality and Visualization Research"
            }
          ],
          "personId": 66872
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Weimar",
              "institution": "Bauhaus-Universität Weimar",
              "dsl": "Visualization Research and Virtual Reality"
            }
          ],
          "personId": 66944
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Weimar",
              "institution": "Bauhaus-Universität Weimar",
              "dsl": "Virtual Reality and Visualization Research"
            }
          ],
          "personId": 66890
        }
      ]
    },
    {
      "id": 66963,
      "typeId": 11912,
      "title": "Qualitative Dimensions of Technology-Mediated Reflective Learning: The Case of VR Experience of Psychosis ",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489869"
        }
      },
      "trackId": 11343,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        67000
      ],
      "eventIds": [],
      "abstract": "Self-reflection is evaluation of one’s inferential processes often triggered by complex social and emotional experiences, characterized by their ambiguity and unpredictability, pushing one to re-interpret the experience, and update existing knowledge. Using immersive Virtual Reality (VR), we aimed to support social and emotional learning (SEL) through reflection in psychology education. We used the case of psychosis as it involves ambiguous perceptual experiences. With a codesign workshop, we designed a VR prototype that simulates the perceptual, cognitive, affective, and social elements of psychotic experiences, followed by a user-study with psychology students to evaluate the potential of this technology to support reflection. Our analyses suggested that technology-mediated reflection in SEL involves two dimensions: spontaneous perspective-taking and shared state of affect. By exploring the subjective qualities of reflection with the said dimensions, our work contributes to the literature on technology-supported learning and VR developers designing for reflection.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Turkey",
              "state": "",
              "city": "Istanbul",
              "institution": "Koç University",
              "dsl": "KARMA Mixed Reality Lab"
            }
          ],
          "personId": 66915
        },
        {
          "affiliations": [
            {
              "country": "Turkey",
              "state": "",
              "city": "Istanbul",
              "institution": "Koç University",
              "dsl": ""
            }
          ],
          "personId": 66771
        },
        {
          "affiliations": [
            {
              "country": "Turkey",
              "state": "",
              "city": "Istanbul",
              "institution": "Koç University",
              "dsl": "KARMA Mixed Reality Lab"
            },
            {
              "country": "Turkey",
              "state": "Select Region",
              "city": "Istanbul",
              "institution": "Koç University",
              "dsl": "KUAR, Research Center for Creative Industries"
            }
          ],
          "personId": 66856
        },
        {
          "affiliations": [
            {
              "country": "Turkey",
              "state": "",
              "city": "Istanbul",
              "institution": "Koç University",
              "dsl": ""
            }
          ],
          "personId": 66880
        },
        {
          "affiliations": [
            {
              "country": "Turkey",
              "state": "",
              "city": "Istanbul",
              "institution": "Koc University",
              "dsl": "Koc University Arcelik Research Center (KUAR)"
            }
          ],
          "personId": 66806
        },
        {
          "affiliations": [
            {
              "country": "Turkey",
              "state": "",
              "city": "Istanbul",
              "institution": "Koç University",
              "dsl": ""
            }
          ],
          "personId": 66857
        },
        {
          "affiliations": [
            {
              "country": "Turkey",
              "state": "",
              "city": "İstanbul",
              "institution": "Koç University",
              "dsl": "Media and Visual Arts"
            }
          ],
          "personId": 66761
        },
        {
          "affiliations": [
            {
              "country": "Turkey",
              "state": "",
              "city": "İstanbul",
              "institution": "Koç University",
              "dsl": "KARMA Mixed Reality Lab"
            }
          ],
          "personId": 66795
        },
        {
          "affiliations": [
            {
              "country": "Turkey",
              "state": "",
              "city": "Istanbul",
              "institution": "Koc University",
              "dsl": ""
            }
          ],
          "personId": 66777
        }
      ]
    },
    {
      "id": 66964,
      "typeId": 11912,
      "title": "Presenting Sense of Loud Vocalization Using Vibratory Stimuli to the Larynx and Auditory Stimuli",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489891"
        }
      },
      "trackId": 11343,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        67003
      ],
      "eventIds": [],
      "abstract": "In recent years, COVID-19 has reduced the environment in which people can loudly vocalize. However, loud vocalization is expected to be useful in reducing the stress enhanced by COVID-19. Moreover, loud vocalization seems to benefit humans in many ways with a sense of exhilaration. We believe that loud vocalization itself is not necessary for such benefits; but the sense of loud vocalization plays an important role. Therefore, we focused on a method of substituting experience by presenting sensory stimuli. In this paper, we proposed a way to present the sense of loud vocalization through vibratory stimuli to the larynx and auditory stimuli to participants with the intention of producing a loud vocalization who are actually vocalizing quietly. Our user study showed that the proposed method can extend the sense of vocalization and realize pseudo-loud vocalization. In addition, it was also shown that the proposed method can create a sense of exhilaration. By contrast, excessively strong vibratory stimuli spoil the sense of loud vocalization, and thus the intensity of the vibration should be appropriately determined.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 66829
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Chiba",
              "city": "Kashiwanoha",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 66899
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kashiwa",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 66913
        }
      ]
    },
    {
      "id": 66965,
      "typeId": 11912,
      "title": "Effects of Image Realism on the Stress Response in Virtual Reality",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489885"
        }
      },
      "trackId": 11343,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66998
      ],
      "eventIds": [],
      "abstract": "Safety critical situations, as they occur in professions such as medicine, nursing, and aviation, are often trained in simulators to prevent damages to personnel and material. These jobs often come with a high amount of stress, to which prolonged exposure can have devastating effects. \r\nOver the past years, stress inoculation training in conjunction with Virtual Reality has become focus of the research community and software companies. Especially the nursing profession can benefit from it, since stress-related illnesses are often the reason for an early exit from the workforce. However, since training facilities often need to compromise on their simulations due to monetary reasons, trade-offs must be made in the degree of detail of such simulations in order to keep development and acquisition costs low. One such possibility is in using low graphical fidelity.\r\n\r\nWe present a psycho-physiological study on the influence of image realism of virtual environments on the stress response. In a within subject design study, we ask participants to complete nursing related, virtually recreated tasks in an artificial intensive care unit, whilst exposed to different stress factors. We provide our findings in the form of objective and subjective measures. Results show that one can elicit different stress responses by manipulating image realism in a sufficiently drastic manner. However, a life-like reaction does not seem to depend on a highly realistic environment. Our results can provide opportunities for the establishment of more and improved nursing related stress training.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Oldenburg",
              "institution": "OFFIS - Institute for Information Technology",
              "dsl": ""
            }
          ],
          "personId": 66945
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Oldenburg",
              "institution": "Faculty 2",
              "dsl": "Carl von Ossietzky University of Oldenburg"
            }
          ],
          "personId": 66858
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Oldenburg",
              "institution": "OFFIS - Institute for Information Technology",
              "dsl": ""
            }
          ],
          "personId": 66849
        }
      ]
    },
    {
      "id": 66966,
      "typeId": 11912,
      "title": "Non-isomorphic Interaction Techniques for Controlling Avatar Facial Expressions in VR",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489867"
        }
      },
      "trackId": 11343,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66996
      ],
      "eventIds": [],
      "abstract": "The control of an avatar's facial expressions in virtual reality is mainly based on the automated recognition and transposition of the user's facial expressions. These isomorphic techniques are limited to what users can convey with their own face and have recognition issues. To overcome these limitations, non-isomorphic techniques rely on interaction techniques using input devices to control the avatar's facial expressions. Such techniques need to be designed to quickly and easily select and control an expression, and not disrupt a main task such as talking. We present the design of a set of new non-isomorphic interaction techniques for controlling an avatar facial expression in VR using a standard VR controller. These techniques have been evaluated through two controlled experiments to help designing an interaction technique combining the strengths of each approach. This technique was evaluated in a final ecological study showing it can be used in contexts such as social applications.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Lille",
              "institution": "Inria",
              "dsl": ""
            },
            {
              "country": "France",
              "state": "",
              "city": "Lille",
              "institution": "University of Lille",
              "dsl": ""
            }
          ],
          "personId": 66840
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Lille",
              "institution": "University of Lille",
              "dsl": ""
            }
          ],
          "personId": 66875
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Bordeaux",
              "institution": "Inria",
              "dsl": ""
            }
          ],
          "personId": 66781
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Lille",
              "institution": "Université de Lille",
              "dsl": "UMR 9189 - CRIStAL"
            }
          ],
          "personId": 66766
        }
      ]
    },
    {
      "id": 66967,
      "typeId": 11912,
      "title": "Ellipses Ring Marker for High-speed Finger Tracking",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489856"
        }
      },
      "trackId": 11343,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        67001
      ],
      "eventIds": [],
      "abstract": "High-speed finger tracking is necessary for augmented reality and operation in human-machine cooperation without latency discomfort, but conventional markerless finger tracking methods are not fast enough and the marker-based methods have low wearability. In this paper, we propose an ellipses ring marker (ERM), a finger-ring marker consisting of multiple ellipses and its high-speed image recognition algorithm. The finger-ring shape has highly wearing continuity, and the surface shape is suitable for various viewing angle observation. The invariance of the ellipse in the perspective projection enables accurate and low-latency posture estimation. We have experimentally investigated the advantage in normal distribution, validated the sufficient accuracy and computational cost in the marker tracking, and showed a demonstration of dynamic projection mapping on a palm.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 66850
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 66822
        }
      ]
    },
    {
      "id": 66968,
      "typeId": 11912,
      "title": "Mid-Air Thermo-Tactile Feedback using Ultrasound Haptic Display",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489889"
        }
      },
      "trackId": 11343,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        67001
      ],
      "eventIds": [],
      "abstract": "This paper presents a mid-air thermo-tactile feedback system using an ultrasound haptic display. We design a proof-of-concept thermo-tactile feedback system consisting of an open-top chamber, heat modules, and an ultrasound display to concurrently deliver both thermal and tactile stimuli. Our approach is to provide heated airflow along the path to the focused pressure point created from the ultrasound display to generate thermal and vibrotactile cues simultaneously. We confirm that our system can generate the thermo-tactile stimuli up to 54.2°C with 3.43 mN when the ultrasonic haptic signal was set to 100 Hz with a 12 mm radius of the cue size. We also confirm that our system can provide a stable temperature at each level (mean error=0.25%). We measure the warm detection threshold (WDT) and the heat-pain detection threshold (HPDT). The results show that the mean WDT was 32.8°C (SD=1.12) whereas the mean HPDT was 44.6°C (SD=1.64), which are consistent with the contact-based thermal thresholds. We also found that the accuracy of haptic pattern identification is similar for non-thermal (98.1%) and thermal conditions (97.2%), showing a non-significant effect of high temperature. We finally confirmed that thermo-tactile feedback further enhances the user experience compared to other modalities.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Richardson",
              "institution": "The University of Texas at Dallas",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 66765
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Richardson",
              "institution": "The University of Texas at Dallas",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 66941
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Ulsan",
              "institution": "Ulsan National Institute of Science and Technology",
              "dsl": "Department of Human Factors Engineering"
            }
          ],
          "personId": 66910
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Richardson",
              "institution": "The University of Texas at Dallas",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 66902
        }
      ]
    },
    {
      "id": 66969,
      "typeId": 11912,
      "title": "Enabling Robot-assisted Motion Capture with Human Scale Tracking Optimization",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489881"
        }
      },
      "trackId": 11343,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        67001
      ],
      "eventIds": [],
      "abstract": "Motion tracking systems with viewpoint concerns or whose marker data include unreliable states have proven difficult to use despite many impactful benefits. We propose a technique inspired by active vision and using a customized hill-climbing approach to control a robot-sensor setup and apply it to a magnetic induction system capable of occlusion-free motion tracking. Our solution reduces the impact of displacement and orientation issues for markers which inherently present a dead-angle range that disturbs usability and accuracy. Our implementation of the resulting interface is successful in stabilizing previously unexploitable data in a volume 26 times larger than the baseline, while preventing sub-optimal states for up to hundreds of occurrences per recording and featuring an approximate 40\\% decrease in tracking error.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Sendai",
              "institution": "Tohoku University",
              "dsl": "Research Institute of Electrical Communication"
            },
            {
              "country": "France",
              "state": "",
              "city": "Lyon",
              "institution": "INSA de Lyon",
              "dsl": "Department of Computer Sciences"
            }
          ],
          "personId": 66757
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Miyagi",
              "city": "Sendai",
              "institution": "Tohoku University",
              "dsl": "Research Institute of Electrical Communication"
            }
          ],
          "personId": 66800
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Sendai",
              "institution": "Tohoku University",
              "dsl": "Research Institute of Electrical Communication"
            }
          ],
          "personId": 66883
        }
      ]
    },
    {
      "id": 66970,
      "typeId": 11912,
      "title": "Ubiq: A System to Build Flexible Social Virtual Reality Experiences",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489871"
        }
      },
      "trackId": 11343,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66996
      ],
      "eventIds": [],
      "abstract": "While they have long been a subject of academic study, social virtual reality (SVR) systems are now attracting increasingly large audiences on current consumer virtual reality systems. The design space of SVR systems is very large, and relatively little is known about how these systems should be constructed in order to be usable and efficient. In this paper we present AnonVR, a toolkit that focuses on facilitating the construction of SVR systems. We argue for the design strategy of AnonVR and its scope. AnonVR is built on the Unity platform. It provides core functionality of many SVR systems such as connection management, voice, avatars, etc. However, its design remains easy to extend. We demonstrate examples built on AnonVR and how it has been successfully used in classroom teaching. AnonVR is open source (Apache License) and thus enables several use cases that commercial systems cannot.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "University College London",
              "dsl": "Computer Science"
            }
          ],
          "personId": 66851
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "University College London",
              "dsl": ""
            }
          ],
          "personId": 66803
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "University College London",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 66909
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "University College London",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 66848
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "University College London",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 66946
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "London",
              "city": "London",
              "institution": "University College London",
              "dsl": "Computer Science"
            }
          ],
          "personId": 66934
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "University College London",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 66937
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "University College London",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 66770
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "University College London",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 66877
        }
      ]
    },
    {
      "id": 66971,
      "typeId": 11912,
      "title": "EntangleVR: a Visual Programming Interface for Virtual Reality Interactive Scene Generation",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489872"
        }
      },
      "trackId": 11343,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66999
      ],
      "eventIds": [],
      "abstract": "Entanglement is a unique phenomenon in quantum physics that describes a correlated relationship in the measurement of a group of spatially separated particles. In the fields of science fiction, game design, art and philosophy, it has inspired the creation of numerous innovative works. We present EntangleVR, a novel method to create entanglement-inspired virtual scenes with the goal to simplify representing this phenomenon in the design of interactive VR games and experiences. By providing a reactive visual programming interface, users can integrate entanglement into their design without requiring prior knowledge of quantum computing or quantum physics. Our system enables fast creation of complex scenes that are composed of virtual objects with easily manipulated correlated behaviors. We evaluated EntangleVR with 16 participants. Our results indicate that our system enables the user to easily create a virtual environment based on the concept of entanglement. Our participants also gave positive feedback on our system's capacity to facilitate learning and creativity.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "University of California Santa Barbara",
              "dsl": "Media Arts and Technology"
            }
          ],
          "personId": 66859
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "University of California Santa Barbara",
              "dsl": "Media Arts and Technology"
            }
          ],
          "personId": 66905
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "UCSB",
              "dsl": "Computer Science "
            }
          ],
          "personId": 66889
        }
      ]
    },
    {
      "id": 66972,
      "typeId": 11912,
      "title": "Impostor-based Rendering Acceleration for Virtual, Augmented, and Mixed Reality",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489865"
        }
      },
      "trackId": 11343,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66996
      ],
      "eventIds": [],
      "abstract": "This paper presents an image-based rendering approach to accelerate rendering time of virtual scenes containing a large number of complex high poly count objects. Our approach replaces complex objects by impostors, light-weight image-based representations leveraging geometry and shading  related processing costs. In contrast to their classical implementation, our impostors are specifically designed to work in Virtual-, Augmented- and Mixed Reality scenarios (XR for short), as they support stereoscopic rendering to provide correct depth perception. Motion parallax of typical head movements is compensated by using a ray marched parallax correction step. Our approach provides a dynamic run-time recreation of impostors as necessary for larger changes in view position.\r\nThe dynamic run-time recreation is decoupled from the actual rendering process. Hence, its associated processing cost is therefore distributed over multiple frames. This avoids any unwanted frame drops or latency spikes even for impostors of objects with complex geometry and many polygons. In addition to the significant performance benefit, our impostors compare favorably against the original mesh representation, as geometric and textural temporal aliasing artifacts are heavily suppressed.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "NRW",
              "city": "Würzburg",
              "institution": "University of Würzburg",
              "dsl": "Department of Computer Science, HCI Group"
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Köln",
              "institution": "Institute of Media and Imaging Technology",
              "dsl": "TH Köln"
            }
          ],
          "personId": 66933
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Köln",
              "institution": "TH Köln",
              "dsl": "Institute of Media and Imaging Technology"
            }
          ],
          "personId": 66870
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "Department of Computer Science, HCI Group",
              "dsl": "University of Würzburg"
            }
          ],
          "personId": 66797
        }
      ]
    },
    {
      "id": 66973,
      "typeId": 11912,
      "title": "Analysis of Detection Thresholds for Hand Redirection during Mid-Air Interactions in Virtual Reality",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489866"
        }
      },
      "trackId": 11343,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        67003
      ],
      "eventIds": [],
      "abstract": "Avatars in virtual reality (VR) with fully articulated hands enable the user to naturally interact with the virtual environment (VE).\r\nInteractions are often performed in a one-to-one mapping between the movements of the user's real body, for instance, the hands, and the displayed body of the avatar.\r\nHowever, VR also allows manipulating this mapping to introduce non-isomorphic techniques.\r\nIn this context, research on manipulations of virtual hand movements typically focuses on increasing the user's interaction space to improve the overall efficiency of hand-based interactions.\r\n\r\nIn this paper, we investigate a hand retargeting method for decelerated hand movements.\r\nWith this technique, users need to perform larger movements to reach for an object in the VE, which can be utilized, for example, in therapeutic applications.\r\nIf these gain-based redirections of virtual hand movements are small enough, users become unable to reliably detect them due to the dominance of the visual sense.\r\nIn a psychophysical experiment, we analyzed detection thresholds for six different motion paths in mid-air for both hands.\r\nWe found significantly different detection thresholds between movement directions on each spatial axis.\r\nTo verify our findings, we applied the identified gains in a playful application in a confirmatory study.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Hamburg",
              "institution": "Universität Hamburg",
              "dsl": ""
            }
          ],
          "personId": 66863
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Hamburg",
              "institution": "University of Hamburg",
              "dsl": "Human Computer Interaction"
            }
          ],
          "personId": 66845
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Hamburg",
              "institution": "Universität Hamburg",
              "dsl": "Human-Computer Interaction"
            }
          ],
          "personId": 66819
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Hamburg",
              "institution": "Universität Hamburg",
              "dsl": ""
            }
          ],
          "personId": 66773
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Hamburg",
              "institution": "Universitätsklinikum Hamburg-Eppendorf (UKE)",
              "dsl": ""
            }
          ],
          "personId": 66874
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Hamburg",
              "institution": "Universitätsklinikum Hamburg-Eppendorf",
              "dsl": "Zentrum für Psychosoziale Medizin"
            }
          ],
          "personId": 66831
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Hamburg",
              "institution": "Universität Hamburg",
              "dsl": ""
            }
          ],
          "personId": 66794
        }
      ]
    },
    {
      "id": 66974,
      "typeId": 11912,
      "title": "Absolute And Differential Thresholds Of Motion Effects In Cardinal Directions",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489870"
        }
      },
      "trackId": 11343,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        67003
      ],
      "eventIds": [],
      "abstract": "In this paper, we report both absolute and differential thresholds for motion in the six cardinal directions as comprehensively as possible. As with general 4D motion effects, we used sinusoidal motions with low intensity and large frequency as stimuli. Hence, we could also compare the effectiveness of motion types in delivering motion effects. We found that the thresholds for the z-axis (up-down) were higher than those for the x-axis (front-back) and y-axis (left-right) in both kinds of thresholds and that the type of motion significantly affected both thresholds. Further, between differential thresholds and reference intensities, we found a strong linear relationship for roll, yaw, and surge. Compared to them, a relatively weak linear relationship was observed for the rest of the motion types. Our results can be useful for generating motion effects for 4D contents while considering the human sensitivity to motion feedback.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "Gyeongsangbuk-do",
              "city": "Pohang",
              "institution": "Pohang University of Science and Technology",
              "dsl": "Department of Computer Science and Engineering"
            }
          ],
          "personId": 66786
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Pohang",
              "institution": "Pohang University of Science and Technology",
              "dsl": "Department of Computer Science and Engineering"
            }
          ],
          "personId": 66823
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "Gyeongbuk",
              "city": "Pohang",
              "institution": "Pohang University of Science and Technology (POSTECH)",
              "dsl": "Computer Science and Engineering / Interaction Laboratory "
            }
          ],
          "personId": 66932
        }
      ]
    },
    {
      "id": 66975,
      "typeId": 11912,
      "title": "Research and Practice Recommendations for Mixed Reality Design - Different Perspectives from the Community",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489876"
        }
      },
      "trackId": 11343,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        67000
      ],
      "eventIds": [],
      "abstract": "Different kinds of design guides for developing interactive systems have been created for several decades to maintain consistency and usability. However, practitioners from research and industry either have difficulties finding them or perceive such guides as lacking relevance, practicability, and applicability for spatial application development. In this paper, we present the current state of scientific research and industry practice by investigating currently used design recommendations for MR system development. Through a literature review, we analyzed and compared 875 design recommendations for MR applications elicited from 89 scientific papers and from six industry practitioner's documentations. In doing so, we identified differences regarding four key topics: Focus on MR unique design challenges, abstraction regarding devices and ecosystems, level of detail and abstraction of content, and covered topics. Based on that, we contribute to the MR design research by providing three factors for perceived irrelevance as well as six main implications for design recommendations that are applicable in scientific and industry practice.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Siegen",
              "institution": "University of Siegen",
              "dsl": "Verbraucherinformatik Research Group"
            }
          ],
          "personId": 66929
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Siegen",
              "institution": "University of Siegen",
              "dsl": "Cyber-Physical Systems"
            }
          ],
          "personId": 66908
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Siegen",
              "institution": "University of Siegen",
              "dsl": "Institute for Information Systems"
            }
          ],
          "personId": 66868
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Siegen",
              "institution": "University of Siegen",
              "dsl": "Cyber-Physical Systems"
            }
          ],
          "personId": 66935
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Sankt Augustin",
              "institution": "Bonn-Rhein Sieg University of Applied Science",
              "dsl": "Institut für Verbraucherinformatik"
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Sankt Augustin",
              "institution": "Fraunhofer Institute for Applied Information Technology FIT",
              "dsl": "Human-Centered Engineering and Design"
            }
          ],
          "personId": 66796
        }
      ]
    },
    {
      "id": 66976,
      "typeId": 11912,
      "title": "PneuMod: A Modular Haptic Device with Localized Pressure and Thermal Feedback",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489857"
        }
      },
      "trackId": 11343,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        67001
      ],
      "eventIds": [],
      "abstract": "Humans have tactile sensory organs distributed all over the body. However, haptic devices are often only created for one part (e.g., hands, wrist, or face). We propose PneuMod, a wearable modular haptic device that can simultaneously and independently present pressure and thermal (warm and cold) cues to different parts of the body. The module in PneuMod is a pneumatically-actuated silicone bubble with an integrated Peltier device that can render thermo-pneumatic feedback through shapes, locations, patterns, and motion effects. The modules can be arranged with varying resolutions on fabric to create sleeves, headbands, leg wraps, and other forms that can be worn on multiple parts of the body. In this paper, we describe the system design, the module implementation and results from a technical evaluation and a user study. Results from our experiments quantify device behavior and study results show that users are able to distinguish between different shapes, patterns, motions and thermal effects of actuation while wearing the device as a sleeve on the forearm.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "University of California",
              "dsl": "Computer Science"
            }
          ],
          "personId": 66810
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "UCSB",
              "dsl": "Computer Science "
            }
          ],
          "personId": 66889
        }
      ]
    },
    {
      "id": 66977,
      "typeId": 11912,
      "title": "Virtual Reality platform for functional Magnetic Resonance Imaging in Ecologically valid conditions",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489894"
        }
      },
      "trackId": 11343,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        67003
      ],
      "eventIds": [],
      "abstract": "Functional magnetic resonance Brain Imaging (fMRI) is a key non-invasive imaging technique for the study of human brain activity. Its millimetric spatial resolution is at the cost of several constraints: participants must remain static and experience artificial stimuli, making it difficult to generalize neuroscientific results to naturalistic and ecological conditions. Here, we report how we implemented a flexible immersive Virtual Reality (VR) platform capitalizing on state-of-the-art MR-compatible display and motion capture systems, and how we adapted immersion and embodiment principles to the MRI constraints to provide participants with a strong sense of presence and, eventually, simulate ecologically valid conditions in the MRI. We validate the functionality of the platform with a pilot study where healthy participants performed several basic research tasks in an MR-specific immersive virtual environment. Our results show that using our platform can successfully manipulate the sense of presence, the body ownership for a virtual avatar, and agency of movement. Moreover, we reproduce several behavioral and fMRI results in the perceptual, motor, and cognitive domain exemplifying the versatility of the platform. We discuss how to leverage such technology for neuroscience research and provide recommendations on efficient ways to implement and develop it successfully.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "Geneva",
              "city": "Geneva",
              "institution": "Ecole Polytechnique Fédérale de Lausanne",
              "dsl": "Laboratory of Cognitive Neuroscience, Center for Neuroprosthetics and Brain and Mind Institute"
            }
          ],
          "personId": 66772
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "Geneva",
              "city": "Geneva",
              "institution": "Ecole Polytechnique Fédérale de Lausanne",
              "dsl": "Laboratory of Cognitive Neuroscience, Center for Neuroprosthetics and Brain and Mind Institute"
            },
            {
              "country": "Switzerland",
              "state": "Geneva",
              "city": "Geneva",
              "institution": "Fondation Campus Biotech Geneva",
              "dsl": "Human Neuroscience Platform"
            }
          ],
          "personId": 66774
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Geneva",
              "institution": "Fondation Campus Biotech Geneva",
              "dsl": "Human Neuroscience Platform"
            }
          ],
          "personId": 66869
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Lausanne",
              "institution": "École polytechnique fédérale de Lausanne",
              "dsl": "Center for Neuroprosthetics and Laboratory of Cognitive Neuroscience"
            }
          ],
          "personId": 66824
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "Geneva",
              "city": "Geneva",
              "institution": "Ecole Polytechnique Fédérale de Lausanne",
              "dsl": "Laboratory of Cognitive Neuroscience, Center for Neuroprosthetics and Brain Mind Institute"
            },
            {
              "country": "Switzerland",
              "state": "Geneva",
              "city": "Geneva",
              "institution": "University of Geneva",
              "dsl": "Department of Neurology"
            }
          ],
          "personId": 66867
        }
      ]
    },
    {
      "id": 66978,
      "typeId": 11912,
      "title": "Flyables: Haptic Input Devices for Virtual Reality using Quadcopters",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489855"
        }
      },
      "trackId": 11343,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        67004
      ],
      "eventIds": [],
      "abstract": "Virtual Reality (VR) has made its way into everyday life. While VR delivers an ever-increasing level of immersion, controls and their haptics are still limited. Current VR headsets come with dedicated controllers that are used to control every virtual interface element. However, the controller input mostly differs from the virtual interface. This reduces immersion. To provide a more realistic input, we present Flyables, a toolkit that provides matching haptics for virtual user interface elements using quadcopters. We took five common virtual UI elements and built their physical counterparts. We attached them to quadcopters to deliver on-demand haptic feedback. In a user study, we compared Flyables to controller-based VR input. While controllers still outperform Flyables in terms of precision and task completion time, we found that Flyables present a more natural and playful way to interact with VR environments. Based on the results from the study, we outline research challenges that could improve interaction with Flyables in the future.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Essen",
              "institution": "University of Duisburg-Essen",
              "dsl": "HCI Group"
            }
          ],
          "personId": 66876
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "NRW",
              "city": "Essen",
              "institution": "Universität Duisburg-Essen",
              "dsl": "Library"
            }
          ],
          "personId": 66898
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "LMU Munich",
              "dsl": ""
            }
          ],
          "personId": 66826
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Essen",
              "institution": "University of Duisburg-Essen",
              "dsl": "HCI Group"
            }
          ],
          "personId": 66911
        }
      ]
    },
    {
      "id": 66979,
      "typeId": 11912,
      "title": "Investigating the Effects of Virtual Patients’ Nonsensical Responses on Users’ Facial Expressions in Mental Health Training Scenarios",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489864"
        }
      },
      "trackId": 11343,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66999
      ],
      "eventIds": [],
      "abstract": "This report investigates how clinician-participants react to virtual patients’ sensical vs. nonsensical responses in a training simulation that aims to help clinicians acquire empathetic skills toward high-risk patients with symptoms of the Suicide Crisis Syndrome (SCS). Two suicidal virtual patients were developed, and clinician-participants interactions with them were recorded. Their facial emotions were analyzed in three key moments: after a baseline sensical response, after a nonsensical response, and after the following sensical response. We compared their basic facial emotions aggregated into Negative and Positive facial affective behaviors (FABs). We describe our study involving ten clinician-participants and the results of the facial expression analysis with Noldus FaceReader. Our results suggest that nonsensical responses from virtual humans have an overall impact on both Positive and Negative facial affective emotions, and may lead to an increased percentage of time participants demonstrate Negative facial affective behaviors when interacting with virtual humans. We discuss several aspects regarding the impacts and importance of considering nonsensical responses in the context of virtual human-based interactions.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Gainesville",
              "institution": "University of Florida",
              "dsl": "Department of Computer and Information Sciences and Engineering/Virtual Experiences Research Group"
            }
          ],
          "personId": 66780
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Gainesville",
              "institution": "University of Florida",
              "dsl": "Department of Computer and Information Science and Engineering"
            }
          ],
          "personId": 66811
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Icahn school of Medicine",
              "dsl": "Mount Sinai Suicide Research Lab"
            }
          ],
          "personId": 66809
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Icahn school of Medicine",
              "dsl": "Mount Sinai Suicide Research Lab"
            }
          ],
          "personId": 66805
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Mount Sinai Beth Israel",
              "dsl": "Department of Psychiatry"
            }
          ],
          "personId": 66920
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Mount Sinai Beth Israel",
              "dsl": "Psychiatry"
            }
          ],
          "personId": 66865
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Miami",
              "institution": "Florida International University",
              "dsl": ""
            }
          ],
          "personId": 66758
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Miami",
              "institution": "Florida International University",
              "dsl": ""
            }
          ],
          "personId": 66928
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Miami",
              "institution": "Florida International University",
              "dsl": ""
            }
          ],
          "personId": 66768
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Gainesville",
              "institution": "University of Florida",
              "dsl": ""
            }
          ],
          "personId": 66855
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Mount Sinai Beth Israel",
              "dsl": "Department of Psychiatry"
            }
          ],
          "personId": 66938
        }
      ]
    },
    {
      "id": 66980,
      "typeId": 11912,
      "title": "Enhancing In-game Immersion Using BCI-controlled Mechanics",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489862"
        }
      },
      "trackId": 11343,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        67003
      ],
      "eventIds": [],
      "abstract": "Modern gaming reaches beyond the well-known control modes, requiring players to use the entirety of their senses and body parts to interact with the gameplay. Due to multimodal approach, the virtual reality experiences become increasingly more immersive and entertaining. New control modalities, such as brain-computer interfaces (BCIs), enable the players to engage in the game with both their bodies and minds. In our work, we investigate the influence of employing BCI-driven mechanics on player's in-game immersion.  We designed and implemented an escape room-themed game which employed player's mental states of focus and relaxation as input for selected game mechanisms. Through a between-subject user study, we found that controlling the game with mental states enhances the in-game immersion and attracts the player's engagement. At the same time, using BCIs did not impose additional cognitive workload. Our work contributes to HCI for games and VR through real-case analysis of psychocognitive effects of using BCIs in gaming and provides insights into designing immersive game experiences employing players' mental states.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Oldenburg",
              "institution": "University of Oldenburg",
              "dsl": ""
            }
          ],
          "personId": 66904
        },
        {
          "affiliations": [
            {
              "country": "Poland",
              "state": "",
              "city": "Lodz",
              "institution": "Lodz University of Technology",
              "dsl": ""
            }
          ],
          "personId": 66833
        },
        {
          "affiliations": [
            {
              "country": "Poland",
              "state": "",
              "city": "Lodz",
              "institution": "Lodz University of Technology",
              "dsl": "Institute of Applied Computer Science"
            }
          ],
          "personId": 66844
        },
        {
          "affiliations": [
            {
              "country": "Poland",
              "state": "",
              "city": "Lodz",
              "institution": "Lodz University of Technology",
              "dsl": "Institute of Applied Computer Science"
            }
          ],
          "personId": 66792
        },
        {
          "affiliations": [
            {
              "country": "Poland",
              "state": "",
              "city": "Łódź",
              "institution": "Lodz University of Technology",
              "dsl": ""
            }
          ],
          "personId": 66801
        },
        {
          "affiliations": [
            {
              "country": "Poland",
              "state": "",
              "city": "Łódź",
              "institution": "Lodz University of Technology",
              "dsl": "Institute of Applied Computer Science"
            }
          ],
          "personId": 66798
        },
        {
          "affiliations": [
            {
              "country": "Poland",
              "state": "",
              "city": "Lodz",
              "institution": "Lodz University of Technology",
              "dsl": "Institute of Applied Computer Science"
            }
          ],
          "personId": 66866
        }
      ]
    },
    {
      "id": 66981,
      "typeId": 11912,
      "title": "The Influence of in-VR Questionnaire Design on the User Experience",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489884"
        }
      },
      "trackId": 11343,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66998
      ],
      "eventIds": [],
      "abstract": "Typically, researchers study the user experience in VR by collecting either sensory data or using questionnaires.\r\nWhile traditional questionnaire formats present it through web-based survey tools (out-VR), recent studies investigate the effects of presenting questionnaires directly in the virtual environment (in-VR). \r\nThe in-VR questionnaire can be defined as an implemented user-interface object that allows interaction with questionnaires in VR to not break the immersion. However, integrating questionnaires directly into the virtual environment also challenges design decisions.\r\nWhile most previous research presents in-VR questionnaires in the form of 2D panels in the virtual environment, we want to investigate the difference from such traditional formats to a presentation of a questionnaire format in the form of an interactive object as part of the environment.\r\nAccordingly, we evaluate and compare two different in-VR questionnaire designs and a traditional web-based form (out-VR) to assess user experience, the effect on presence, duration of completing the questionnaires, and users' preferences.\r\nTo achieve this goal, we developed an immersive questionnaire toolkit that provides a general solution for implementing in-VR questionnaires and exchanging data with popular survey services.\r\nThis toolkit enables us to run our study both on-site and remotely.\r\nAs a first study, 16 users, either on-site or remotely, attended by completing the System Usability Scale, NASA TLX, and the iGroup Presence Questionnaire after a playful activity.  \r\nThe first results indicate that there is no significant difference in the case of usability and presence between different design layouts. We could not find a significant difference also for the task load except between 2D and web-based layout for mental demand and frustration as well as duration of completing the questionnaire. However, the results also indicate that the users mainly prefer in-VR questionnaire designs over the traditional ones.\r\nFor future works, we will expand the study with more questionnaire design alternatives and comparing the survey results for on-site and remote experiences.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "Styria",
              "city": "Graz",
              "institution": "Institute of Interactive Systems and Data Science",
              "dsl": "Graz University of Technology"
            }
          ],
          "personId": 66841
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "Styria",
              "city": "Graz",
              "institution": "Graz University of Technology",
              "dsl": ""
            }
          ],
          "personId": 66892
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Graz",
              "institution": "Graz University of Technology",
              "dsl": "Institute of Interactive Systems and Data Science"
            }
          ],
          "personId": 66916
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Graz",
              "institution": "TU Graz",
              "dsl": ""
            }
          ],
          "personId": 66837
        }
      ]
    },
    {
      "id": 66982,
      "typeId": 11912,
      "title": "Object Manipulations in VR Show Task- and Object-Dependent Modulation of Motor Patterns",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489858"
        }
      },
      "trackId": 11343,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        67004
      ],
      "eventIds": [],
      "abstract": "Humans can perform object manipulations in VR in spite of missing haptic and acoustic information. Whether their movements under these artificial conditions do still rely on motor programs based on natural experience or are impoverished due to the restrictions imposed by VR is unclear. We investigated whether reach-to-place and reach-to-grasp movements in VR can still be adapted to the task and to the specific properties of the objects being handled, or whether they reflect a stereotypic, task- and object-independent motor program.  We analyzed reach-to-grasp and reach-to-place movements from participants performing an unconstrained \"set-the-table\" task involving a variety of different objects in virtual reality. These actions were compared based on their kinematic features. We encountered significant differences in peak speed and the duration of the deceleration phase which are modulated depending on the action and on the manipulated object. The flexibility of natural human sensorimotor control thus is at least partially transferred and exploited in impoverished VR conditions. We discuss possible explanations of this behavior and the implications for the design of object manipulations in VR.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bremen",
              "institution": "University of Bremen",
              "dsl": ""
            }
          ],
          "personId": 66804
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bremen",
              "institution": "University of Bremen",
              "dsl": ""
            }
          ],
          "personId": 66818
        }
      ]
    },
    {
      "id": 66983,
      "typeId": 11912,
      "title": "Fusing Semantic Segmentation and Object Detection for Visual SLAM in Dynamic Scenes",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489882"
        }
      },
      "trackId": 11343,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66996
      ],
      "eventIds": [],
      "abstract": "The assumption of static scenes limits the performance of traditional visual SLAM. Many existing solutions adopt deep learning methods or geometric constraints to cope with the dynamic objects in the scenes, but these schemes have problems of efficiency or robustness to a certain extent. In this paper, we propose a synergistic solution of object detection and semantic segmentation to obtain the prior contours of potential dynamic objects. With this prior information, strategies of geometric constraints are utilized to assist with removing dynamic feature points. Finally, the evaluation with the public datasets demonstrates that our proposed method can improve the accuracy of pose estimation and robustness of visual SLAM with no efficiency loss in high dynamic scenarios.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Wuhan",
              "institution": "WuHan University",
              "dsl": "School of Computer Science"
            }
          ],
          "personId": 66834
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Wuhan",
              "institution": "WuHan University",
              "dsl": "Global Navigation Satellite System Research Center"
            }
          ],
          "personId": 66802
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Wuhan",
              "institution": "Wuhan University",
              "dsl": "Global Navigation Satellite System Research Center"
            }
          ],
          "personId": 66912
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Wuhan",
              "institution": "WuHan University",
              "dsl": "School of Computer Science"
            }
          ],
          "personId": 66940
        }
      ]
    },
    {
      "id": 66984,
      "typeId": 11912,
      "title": "Using Gaze Behavior and Head Orientation for Implicit Identification in Virtual Reality",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489880"
        }
      },
      "trackId": 11343,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        67000
      ],
      "eventIds": [],
      "abstract": "Identifying users of a Virtual Reality (VR) headset provides designers of VR content with the opportunity to adapt the user interface, set user-specific preferences, or adjust the level of difficulty either for games or training applications.\r\nWhile most identification methods currently rely on explicit input, implicit user identification is less disruptive and does not impact the immersion of the users.\r\nIn this work, we introduce a biometric identification system that employs the user’s gaze behavior as a unique, individual characteristic.\r\nIn particular, we focus on the user’s gaze behavior and head orientation while following a moving stimulus. \r\nWe verify our approach in a user study. A hybrid post-hoc analysis results in an identification accuracy of up to 75 % for an explainable machine learning algorithm and up to 100 % for a Deep Learning approach.\r\nWe conclude with discussing application scenarios in which our approach can be used to implicitly identify users.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Essen",
              "institution": "University of Duisburg-Essen",
              "dsl": "HCI Group"
            }
          ],
          "personId": 66799
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Essen",
              "institution": "University of Duisburg-Essen",
              "dsl": "HCI Group"
            }
          ],
          "personId": 66914
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Essen",
              "institution": "University of Duisburg-Essen",
              "dsl": "HCI Group"
            }
          ],
          "personId": 66838
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Essen",
              "institution": "University of Duisburg-Essen",
              "dsl": "HCI Group"
            }
          ],
          "personId": 66785
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Essen",
              "institution": "University of Duisburg-Essen",
              "dsl": "HCI Group"
            }
          ],
          "personId": 66911
        }
      ]
    },
    {
      "id": 66985,
      "typeId": 11912,
      "title": "Modeling Pointing for 3D Target Selection in VR",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489853"
        }
      },
      "trackId": 11343,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        67004
      ],
      "eventIds": [],
      "abstract": "Virtual reality (VR) allows users to interact similarly to how they do in the physical world, such as touching, moving, and pointing at objects. To select objects at a distance, most VR techniques rely on casting a ray through one or two points located on the user's body (e.g., on the head and a finger), and placing a cursor on that ray. However, previous studies show that such rays do not help users achieve optimal pointing accuracy nor correspond to how they would naturally point. We seek to find features, which would best describe natural pointing at distant targets. We collect motion data from seven locations on the hand, arm, and body, while participants point at 27 targets across a virtual room. We evaluate the features of pointing and analyse sets of those for predicting pointing targets. Our analysis shows an 87% classification accuracy between the 27 targets for the best feature set and a mean distance of 23.56 cm in predicting pointing targets across the room. The feature sets can inform the design of more natural and effective VR pointing techniques for distant object selection.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Copenhagen",
              "institution": "Department of Computer Science",
              "dsl": "University of Copenhagen"
            },
            {
              "country": "Denmark",
              "state": "",
              "city": "Copenhagen",
              "institution": "Department of Computer Science",
              "dsl": "University of Copenhagen"
            }
          ],
          "personId": 66793
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "",
              "city": "Melbourne",
              "institution": "University of Melbourne",
              "dsl": "School of Computing and Information Systems"
            },
            {
              "country": "Australia",
              "state": "",
              "city": "Melbourne",
              "institution": "University of Melbourne",
              "dsl": "School of Computing and Information Systems"
            }
          ],
          "personId": 66760
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Copenhagen",
              "institution": "University of Copenhagen",
              "dsl": "Department of Computer Science"
            },
            {
              "country": "Denmark",
              "state": "",
              "city": "Copenhagen",
              "institution": "University of Copenhagen",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 66926
        }
      ]
    },
    {
      "id": 66986,
      "typeId": 11912,
      "title": "VRGaitAnalytics: Visualizing Dual Task Cost for VR Gait Assessment",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489874"
        }
      },
      "trackId": 11343,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66999
      ],
      "eventIds": [],
      "abstract": "Among its many promising applications, Virtual Reality (VR) can simulate diverse real-life scenarios and therefore help experimenters assess individuals' gait performance (i.e., walking) under controlled functional contexts. VR-based gait assessment may provide low-risk, reproducible and controlled virtual environments, enabling experimenters to investigate underlying causes for imbalance by manipulating experimental conditions such as multi-sensory loads, mental processing loads (cognitive load), and/or motor tasks. We present a low-cost novel VR gait assessment system that simulates virtual obstacles, visual, auditory, and cognitive loads while using motion tracking to assess participants’ walking performance. The system utilizes in-situ spatial visualization for trial playback and instantaneous outcome measures which enable experimenters and participants to observe and interpret their performance. The trial playback can visualize any moment in the trial with embodied graphic segments including the head, waist, and feet. It can also replay two trials at the same time frame for trial-to-trial comparison, which helps visualize the impact of different experimental conditions. The outcome measures, i.e., the metrics related to walking performance, are calculated in real-time and displayed as data graphs in VR. The system can help experimenters get specific gait information on balance performance beyond a typical clinical gait test, making it  clinically relevant and potentially applicable to gait rehabilitation. We conducted a feasibility study with physical therapy students, research graduate students, and licensed physical therapists. They evaluated the system and provided feedback on the outcome measures, the spatial visualizations, and the potential use of the system in the clinic. The study results indicate that the system was feasible and the immediate spatial visualization features were seen as clinically relevant and useful. Limitations and considerations for future work are discussed. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Courant, NYU",
              "dsl": "Future Reality Lab"
            },
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Courant, NYU",
              "dsl": "Future Reality Lab"
            }
          ],
          "personId": 66787
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Manhattan ",
              "institution": "New York University",
              "dsl": ""
            },
            {
              "country": "United States",
              "state": "New York",
              "city": "Manhattan ",
              "institution": "New York University",
              "dsl": ""
            }
          ],
          "personId": 66943
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "New York University",
              "dsl": ""
            },
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "New York University",
              "dsl": ""
            }
          ],
          "personId": 66817
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Dept of Computer Science, NYU",
              "dsl": "Future Reality Lab"
            },
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Dept of Computer Science, NYU",
              "dsl": "Future Reality Lab"
            }
          ],
          "personId": 66814
        }
      ]
    },
    {
      "id": 66987,
      "typeId": 11912,
      "title": "opticARe - Augmented Reality Mobile Patient Monitoring in Intensive Care Units",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489852"
        }
      },
      "trackId": 11343,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66998
      ],
      "eventIds": [],
      "abstract": "The German healthcare sector is in crisis, struggling with an increasing shortage of skilled workers, ultimately putting patients safety at risk. In order to contribute to overcoming these challenges, this paper evaluates the application of Augmented Reality (AR) in critical care. To address this intention, several prototypes were implemented and evaluated on the basis of the Human Centered Design Process (HCDP). Grounded on a holistic research approach, consisting of an observational study, various semi-structured interviews as well as a quantitative analysis, possible fields of application of AR were initially identified. The results obtained from these studies specified supporting Intensive Care Unit (ICU) nurses in patient monitoring during patient transport as a particular potential context of use of AR. Furthermore, numerous user requirements were derived from the elaborated findings, which were utilized to coordinate the subsequent development of the prototypes. Using a quantitative and qualitative user evaluation of the three resulting prototypes, the potentials of AR for use during patient monitoring were illustrated. Additionally, the evaluation revealed a variety of advantages of the developed prototypes over conventional monitoring systems. Moreover, it became evident that future implementations of a corresponding system for patient monitoring ought to integrate a context-dependent data presentation in particular, as this combines a multitude of preferred features. These results were finally critically discussed, among other things with regard to their generalizability.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Oldenburg",
              "institution": "OFFIS - Institute for Information Technology",
              "dsl": ""
            }
          ],
          "personId": 66825
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Oldenburg",
              "institution": "OFFIS - Institute for Information Technology",
              "dsl": ""
            }
          ],
          "personId": 66816
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Oldenburg",
              "institution": "OFFIS - Institute for Information Technology",
              "dsl": ""
            }
          ],
          "personId": 66849
        }
      ]
    },
    {
      "id": 66988,
      "typeId": 11912,
      "title": "Virtual Object Categorisation Methods: Towards a Richer Understanding of Object Grasping for Virtual Reality",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489875"
        }
      },
      "trackId": 11343,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        67004
      ],
      "eventIds": [],
      "abstract": "Object categorisation methods have been historically used in literature for understanding and collecting real objects together into meaningful groups. Once categorised, objects can be used to define human interaction patterns (i.e grasping) and lead to improved design of real objects and environments. While real environment grasping studies could be viable for determining design and development of VR systems, virtual environments currently do not offer the same level of fidelity in visualisation and interaction. Therefore, studies are needed to determine the potential route towards a richer understanding for VR to inform design of future interactive environments. To address this, our work presents a study into three categorisation methods for virtual objects which can be applied in designing virtual object interaction. We employ Zingg’s object categorisation as a benchmark against existing real and virtual object interaction work and introduce two new categorisation methods. These focus on virtual object equilibrium, which allows virtual objects to be categorised based on their perceived stability, and virtual object component parts, which supports categorisation based on object elements. We evaluate these categorisation methods using a dataset of 1872 grasps from a VR grasping, translation and docking task on 16 virtual representations of real objects. We report on findings on the user’s grasp choice given the virtual object categorisation showing differences for the object categorisation methods.We conclude by detailing recommendations and future ideas on how these categorisation methods can betaken forward to inform a richer understanding of grasping in VR.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "England",
              "city": "Birmingham",
              "institution": "Birmingham City University",
              "dsl": "DMT Lab"
            },
            {
              "country": "United Kingdom",
              "state": "England",
              "city": "Birmingham",
              "institution": "Birmingham City University",
              "dsl": "DMT Lab"
            }
          ],
          "personId": 66853
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Birmingham",
              "institution": "Birmingham City University",
              "dsl": "DMT Lab"
            },
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Birmingham",
              "institution": "Birmingham City University",
              "dsl": "DMT Lab"
            }
          ],
          "personId": 66888
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Birmingham",
              "institution": "Birmingham City University",
              "dsl": ""
            },
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Birmingham",
              "institution": "Birmingham City University",
              "dsl": ""
            }
          ],
          "personId": 66839
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "West Midlands",
              "city": "Birmingham",
              "institution": "Birmingham City University",
              "dsl": "DMT Lab"
            },
            {
              "country": "United Kingdom",
              "state": "West Midlands",
              "city": "Birmingham",
              "institution": "Birmingham City University",
              "dsl": "DMT Lab"
            }
          ],
          "personId": 67013
        }
      ]
    },
    {
      "id": 66989,
      "typeId": 11912,
      "title": "Avatar Tracking Control with Generations of Physically Natural Responses on Contact to Reduce Performers' Loads",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://dl.acm.org/doi/10.1145/3489849.3489859"
        }
      },
      "trackId": 11343,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66996
      ],
      "eventIds": [],
      "abstract": "Real-time performance with motion-captured avatars in virtual space is increasingly popular, with applications including social virtual realities (VRs), virtual performers (e.g., virtual YouTubers), and VR games. Such applications often include contact between multiple avatars or between avatars and objects as communication or gameplay. However, most current applications do not solve the effects of contact for avatars, causing penetration or unnatural behavior to occur. In reality, no contact with the player's body occurs; nonetheless, the player must perform as if contact occurred. While physics simulation can solve the contact issue, the naive use of physics simulation causes tracking delay.\r\nWe propose a novel avatar tracking controller with feedforward control. Our method enables quick, accurate tracking and flexible motion in response to contacts. Furthermore, the technique frees avatar performers from loads of performing as if contact occurred.\r\nWe implemented our method and experimentally evaluated the naturalness of the resulting motions and our approach's effectiveness in reducing performers' load. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Yokohama",
              "institution": "Tokyo Institute of Technology",
              "dsl": "Department of Information and Communications Engineering / Tokyo Institute of Technology / Hasegawa Shoichi Laboratory"
            }
          ],
          "personId": 66861
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Yokohama",
              "institution": "Tokyo Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 66921
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Yokohama",
              "institution": "Tokyo Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 66830
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Yokohama",
              "institution": "Tokyo Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 66919
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Yokohama",
              "institution": "Tokyo Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 66894
        }
      ]
    },
    {
      "id": 66990,
      "typeId": 11912,
      "title": "Inside-Out Instrument Tracking for Surgical Navigation in Augmented Reality",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489863"
        }
      },
      "trackId": 11343,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66996
      ],
      "eventIds": [],
      "abstract": "Surgical navigation requires tracking of surgical instruments with respect to the patient's anatomy during an intervention. Conventionally, this is done with stationary tracking cameras, and the navigation information is displayed on a stationary display, away from the patient. In contrast, an augmented reality (AR) headset can superimpose surgical navigation information directly in the surgeon's view of the patient. However, the AR scenario is complicated by the need to track the headset in addition to the instruments and the patient, resulting in many AR prototypes still relying on stationary tracking infrastructure. In this paper, we show that tracking with six degrees of freedom can be obtained without any stationary, external system by purely utilizing the on-board stereo cameras of a current AR headset, the HoloLens 2 (Microsoft, Redmond, WA, USA), to track the same retro-reflective marker spheres used by current optical navigation systems. Our implementation is based on two tracking pipelines complementing each other, one using conventional stereo vision techniques, the other relying on a single-constraint-at-a-time extended Kalman filter. In a technical evaluation of our tracking approach, we show that clinically relevant accuracy of 1.70 mm / 1.11° and real-time performance is achievable with the HoloLens 2 running our system. We further describe an example application of our system  for untethered end-to-end surgical navigation.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Graz",
              "institution": "Graz University of Technology",
              "dsl": ""
            }
          ],
          "personId": 66896
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Essen",
              "institution": "Essen University Hospital",
              "dsl": ""
            }
          ],
          "personId": 66836
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Graz",
              "institution": "Graz University of Technology",
              "dsl": "Institute of Computer Graphics and Vision"
            }
          ],
          "personId": 66936
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Graz",
              "institution": "Graz University of Technology",
              "dsl": ""
            }
          ],
          "personId": 66862
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Essen",
              "institution": "Essen University Hospital",
              "dsl": ""
            },
            {
              "country": "Austria",
              "state": "",
              "city": "Graz",
              "institution": "TU Graz",
              "dsl": ""
            }
          ],
          "personId": 66778
        }
      ]
    },
    {
      "id": 66991,
      "typeId": 11912,
      "title": "BreachMob: Detecting Vulnerabilities in Physical Environments Using Virtual Reality",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489883"
        }
      },
      "trackId": 11343,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66999
      ],
      "eventIds": [],
      "abstract": "BreachMob is a virtual reality (VR) tool that applies open design principles from information security to physical buildings and structures. BreachMob uses a detailed 3D digital model of a property owner's building. The model is then published as a virtual environment (VE), complete with all applicable security measures and released to the public to test the building's security and find any potential vulnerabilities by completing specified objectives. Our paper contributes a new method of applying VR to crowd-source detection of physical environment vulnerabilities. We detail the technical realization of two\r\nBreachMob prototypes (a house and an airport) reflecting on static and dynamic vulnerabilities. The findings from a design critique suggest that the immersion BreachMob allows participants to demonstrate behaviours that lend themselves well to the experience of breaching physical security protocols.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Calgary",
              "institution": "University of Calgary",
              "dsl": "Computer Science"
            }
          ],
          "personId": 66854
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Calgary",
              "institution": "University of Calgary",
              "dsl": "Computational Media Design"
            }
          ],
          "personId": 66897
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Calgary",
              "institution": "University of Calgary",
              "dsl": "Computational Media Design"
            }
          ],
          "personId": 66930
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Calgary",
              "institution": "University of Calgary",
              "dsl": ""
            }
          ],
          "personId": 66922
        }
      ]
    },
    {
      "id": 66992,
      "typeId": 11912,
      "title": "Catching Jellies in Immersive Virtual Reality: A Comparative Teleoperation Study of ROVs in Underwater Capture Tasks",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489861"
        }
      },
      "trackId": 11343,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66999
      ],
      "eventIds": [],
      "abstract": "Remotely Operated Vehicles (ROVs) are essential to almost any human-operated underwater expedition in the deep sea. However, piloting an ROV is an expensive and cognitively demanding task requiring extensive maneuvering and situational awareness to safely interact with live ecosystems. Immersive Virtual Reality (VR) Head-Mounted Displays (HMDs) potentially afford a unique opportunity to ease many of these challenges. This paper investigates how VR HMDs influence operator performance through a novel telepresence system for piloting ROVs in real-time. We present a within-subjects user study [N=12] that examines common midwater creature capture tasks between Stereoscopic-VR, Monoscopic-VR, and Desktop teleoperation conditions. Our findings indicate that Stereoscopic-VR can outperform Monoscopic-VR and Desktop ROV capture tasks, effectively doubling the efficacy of operators along with significant differences in presence, task load, usability, intrinsic motivation, and cybersickness. Our research points to new opportunities for utilizing VR in underwater ROV tasks from design to in-the-wild deployment.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Cruz",
              "institution": "University of California, Santa Cruz",
              "dsl": ""
            }
          ],
          "personId": 66882
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Cruz",
              "institution": "University of California, Santa Cruz",
              "dsl": ""
            }
          ],
          "personId": 66884
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Cruz",
              "institution": "University of California, Santa Cruz",
              "dsl": ""
            }
          ],
          "personId": 66791
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Cruz",
              "institution": "University of California, Santa Cruz",
              "dsl": ""
            }
          ],
          "personId": 66864
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Needham",
              "institution": "Olin College of Engineering",
              "dsl": ""
            }
          ],
          "personId": 66893
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Needham",
              "institution": "Olin College of Engineering",
              "dsl": ""
            }
          ],
          "personId": 66808
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Moss Landing",
              "institution": "Monterey Bay Aquarium Research Institute",
              "dsl": "Research & Engineering"
            }
          ],
          "personId": 66820
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Moss Landing",
              "institution": "Monterey Bay Aquarium Research Institute",
              "dsl": "Research & Engineering"
            }
          ],
          "personId": 66782
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Moss Landing",
              "institution": "Monterey Bay Aquarium Research Institute",
              "dsl": "Research & Development"
            }
          ],
          "personId": 66918
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Moss Landing",
              "institution": "Monterey Bay Aquarium Research Institute",
              "dsl": "Operations"
            }
          ],
          "personId": 66881
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Cruz",
              "institution": "UC Santa Cruz",
              "dsl": "Computational Media"
            }
          ],
          "personId": 66779
        }
      ]
    },
    {
      "id": 67233,
      "typeId": 11924,
      "title": "Immersive Visual Interaction with Autonomous Multi-Vehicle Systems",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489917"
        }
      },
      "trackId": 11344,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        67010
      ],
      "abstract": "With the emergence of multi-vehicular autonomous systems, such as AI controlled multiple fully autonomous vehicles, we need novel systems that provide tools for planning, executing, and reviewing of missions and keeping humans in the loop during all phases.\r\nWe therefore present an immersive visualization system for interacting with these systems at a higher cognitive level than direct remote piloting of individual vehicles. Our system provides both desktop and VR modes for visual interaction with the robotic multi-vehicle AI system.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "ostergotland",
              "city": "Norrköping ",
              "institution": "Linköping University",
              "dsl": "Dept. of Science and Technology"
            }
          ],
          "personId": 67166
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Norrköping",
              "institution": "Linköping University",
              "dsl": "Department of Science and Technology"
            }
          ],
          "personId": 67016
        }
      ]
    },
    {
      "id": 67234,
      "typeId": 11924,
      "title": "GazeMOOC: A Gaze Data Driven Visual Analytics System for MOOC with XR Content",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489923"
        }
      },
      "trackId": 11344,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        67011
      ],
      "abstract": "MOOC is widely used and more popular after COVID-19.In order to improve the learning effect, MOOC is evolving with XR technologies such as avatars, virtual scenes and experiments. This paper proposes a novel visual analytics system GazeMOOC, that can evaluate learners' learning engagement in MOOC with XR content. For same MOOC content, gaze data of all learners are recorded and clustered. By differentiating gaze data of absent mind learners and active learners, GazeMOOC can help evaluate MOOC content and learners' learning engagement.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Guangzhou",
              "institution": "Guangdong University of Technology",
              "dsl": ""
            }
          ],
          "personId": 67206
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Guangzhou",
              "institution": "Guangdong University of Technology",
              "dsl": ""
            }
          ],
          "personId": 67119
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Guangzhou",
              "institution": "Guangdong University of Technology",
              "dsl": ""
            }
          ],
          "personId": 67165
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Guangzhou",
              "institution": "Guangdong University of Technology",
              "dsl": ""
            }
          ],
          "personId": 67017
        }
      ]
    },
    {
      "id": 67235,
      "typeId": 11924,
      "title": "Effect of Visual Feedback on Understanding Timbre with Shapes Based on Crossmodal Correspondences",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489912"
        }
      },
      "trackId": 11344,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        67011
      ],
      "abstract": "Timbre is a crucial element in playing musical instruments, and it is difficult for beginners to learn it independently. Therefore, external feedback (FB) is required. However, conventional FB methods lack intuitiveness in visualization. In this study, we propose a novel FB method that adopts crossmodal correspondence to enhance the intuitive visualization of timbre with visual shapes. Based on the experiments, it was inferred that the FB based on crossmodal correspondence prevents dependence on FB and promotes learning.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Bunkyo-ku",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 67032
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Bunkyo-ku",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 67105
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Bunkyo-ku",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 67180
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Bunkyo-ku",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 67198
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Bunkyo-ku",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 67178
        }
      ]
    },
    {
      "id": 67236,
      "typeId": 11924,
      "title": "Efficient mapping technique under various spatial changes for SLAM-based AR services",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489916"
        }
      },
      "trackId": 11344,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        67012
      ],
      "abstract": "Recently, many attempts have been made to apply real-time simultaneous localization and mapping (SLAM) technology to augmented reality (AR) applications. Such AR systems based on SLAM technology are generally implemented by augmenting virtual objects onto a diorama or three-dimensional sculpture. However, a new SLAM map needs to be generated if the space or lighting where the diorama is installed changes. This leads to the problem of updating the coordinate system each time a new SLAM map is generated. Updates to the coordinate system signify that the positions of the virtual objects placed in the AR space change as well. Therefore, we proposed a SLAM map regeneration technique in which the existing coordinate system is maintained even if a new map is generated. We also verified the proposed technique experimentally. Furthermore, we applied the proposed technique to an existing AR service to check whether it can be utilized in a real environment.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "Electronics and Telecommunications Research Institute",
              "dsl": ""
            }
          ],
          "personId": 67200
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "Electronics and Telecommunications Research Institute",
              "dsl": ""
            }
          ],
          "personId": 67065
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "Electronics and Telecommunications Research Institute",
              "dsl": ""
            }
          ],
          "personId": 67107
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "Electronics and Telecommunications Research Institute",
              "dsl": ""
            }
          ],
          "personId": 67033
        }
      ]
    },
    {
      "id": 67237,
      "typeId": 11924,
      "title": "HapticPanel: An Open System to Render Haptic Interfaces in Virtual Reality for Manufacturing Industry",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489901"
        }
      },
      "trackId": 11344,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        67010
      ],
      "abstract": "Virtual Reality (VR) allows simulation of machine control panels without physical access to the machine, enabling easier and faster initial exploration, testing, and validation of machine panel designs. However, haptic feedback is indispensable if we want to interact with these simulated panels in a realistic manner. We present HapticPanel, an encountered-type haptic system that provides realistic haptic feedback for machine control panels in VR. To ensure a realistic manipulation of input elements, the user's hand is continuously tracked during interaction with the virtual interface. Based on which virtual element the user intends to manipulate, a motorized panel with stepper motors moves a corresponding physical input element in front of the user's hand, enabling realistic physical interaction.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Diepenbeek",
              "institution": "Expertise Centre for Digital Media, Hasselt University - tUL - Flanders Make",
              "dsl": ""
            }
          ],
          "personId": 67051
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Diepenbeek",
              "institution": "Expertise Centre for Digital Media, Hasselt University - tUL - Flanders Make",
              "dsl": ""
            },
            {
              "country": "United States",
              "state": "Florida",
              "city": "Miami",
              "institution": "Magic Leap",
              "dsl": ""
            }
          ],
          "personId": 67189
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Diepenbeek",
              "institution": "Expertise Centre for Digital Media, Hasselt Universy - tUL - Flanders Make",
              "dsl": ""
            }
          ],
          "personId": 67028
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Diepenbeek",
              "institution": "Hasselt University - tUL - Flanders Make",
              "dsl": "Expertise Centre for Digital Media"
            }
          ],
          "personId": 67224
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Diepenbeek",
              "institution": "Hasselt University - tUL - Flanders Make",
              "dsl": "Expertise Centre for Digital Media"
            }
          ],
          "personId": 67063
        }
      ]
    },
    {
      "id": 67238,
      "typeId": 11924,
      "title": "Remote Visual Line-of-Sight: A Remote Platform for the Visualisation and Control of an Indoor Drone using Virtual Reality",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489910"
        }
      },
      "trackId": 11344,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        67011
      ],
      "abstract": "The COVID-19 pandemic has created a distinct challenge for the piloting of drones/other UAVs for researchers and educators who are restricted to working remotely. We propose a Remote Visual Line-of-Sight system that leverages the advantages of Virtual Reality (VR) and motion capture to allow users to fly a real-world drone from a remote location. The system was developed while our researcher (VR operator) was remotely working in Vietnam with the enclosed real-world environment located in Australia. Our paper will present the system design and the challenges found during the development of our system. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "New South Wales",
              "city": "Sydney",
              "institution": "University Of Technology Sydney",
              "dsl": ""
            }
          ],
          "personId": 67123
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "NSW",
              "city": "Sydney",
              "institution": "University of Technology Sydney",
              "dsl": "Computational Intelligence and Brain Computer Interface Lab"
            }
          ],
          "personId": 67117
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "South Australia",
              "city": "Adelaide",
              "institution": "University of Adelaide",
              "dsl": "School of Computer Science"
            }
          ],
          "personId": 67096
        }
      ]
    },
    {
      "id": 67239,
      "typeId": 11924,
      "title": "Swaying Locomotion: A VR-based Locomotion System through Head Movements",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489897"
        }
      },
      "trackId": 11344,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        67012
      ],
      "abstract": "Locomotion systems used in virtual reality (VR) content have a significant impact on the content user experience. One of the most important factors of a walking system in VR is whether it can provide a plausible walking sensation because it is considered directly related to the user’s sense of presence. However, joystick-based and teleportation-based locomotion systems, which are commonly used today, can hardly provide an appropriate sense of presence to a user. To solve this problem, we present Swaying Locomotion, which is a novel VR-based locomotion system that uses head movements to support a user walking in a VR space while actually sitting in real space. Our user study suggests that Swaying Locomotion provides a better walking sensation than the traditional joystick-based approach.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Waseda University",
              "dsl": ""
            }
          ],
          "personId": 67086
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Waseda University",
              "dsl": ""
            }
          ],
          "personId": 67156
        }
      ]
    },
    {
      "id": 67240,
      "typeId": 11924,
      "title": "ProMVR-Protein Multiplayer VR Tool",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489935"
        }
      },
      "trackId": 11344,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        67010
      ],
      "abstract": "Due to the pandemic limitations caused by Covid-19, people need to work at home and carry on the meetings virtually. Virtual meeting tools start popularizing and thriving. Those tools allow users to see each other through screen and camera, chat through voice and text, and share content or ideas through screen share. However, screen sharing protein models through virtual meetings is not easy due to the difficulty of viewing protein 3D (Three Dimensional) structures from a 2D (Two Dimensional) screen. Moreover, interactions upon a protein are also limited. ProMVR is a tool the author developed to tackle the issue that protein designers may find limitations working in a traditional 2D or 3D environment and they may find it hard to communicate their ideas with other designers. Since ProMVR is a VR tool, it allows users to “jump into” a virtual environment, take a close look at protein models, and have intuitive interactions.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Ireland",
              "state": "",
              "city": "Cork",
              "institution": "University College Cork",
              "dsl": ""
            }
          ],
          "personId": 67154
        },
        {
          "affiliations": [
            {
              "country": "Ireland",
              "state": "",
              "city": "Cork",
              "institution": "University College Cork",
              "dsl": "CancerResearch"
            },
            {
              "country": "Ireland",
              "state": "",
              "city": "Cork",
              "institution": "Munster Technological University",
              "dsl": "Centre for Advanced Photonics and Process Analysis"
            }
          ],
          "personId": 67148
        },
        {
          "affiliations": [
            {
              "country": "Ireland",
              "state": "",
              "city": "Cork",
              "institution": "University College Cork",
              "dsl": "CancerResearch"
            },
            {
              "country": "Ireland",
              "state": "",
              "city": "Cork",
              "institution": "University College Cork",
              "dsl": "SynBioCentre"
            }
          ],
          "personId": 67146
        },
        {
          "affiliations": [
            {
              "country": "Ireland",
              "state": "",
              "city": "Cork",
              "institution": "University College Cork",
              "dsl": ""
            }
          ],
          "personId": 67021
        }
      ]
    },
    {
      "id": 67241,
      "typeId": 11924,
      "title": "A Tangible Haptic Feedback Box for Mixed Reality Billiard in Tight Spaces",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489898"
        }
      },
      "trackId": 11344,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        67010
      ],
      "abstract": "This paper presents a system for a simulated billiard game with two players and an emphasis on haptic feedback. We devised a feedback box that is responsible for generating the inputs and providing immediate haptic feedback to the user. The simulation runs as an AR application and the player can use a real queue to hit the real ball. Although the haptic feedback is precise due to the usage of a real billiard ball and queue, the input accuracy of the angle and impulse measurement is limited.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Cologne",
              "institution": "TH Köln",
              "dsl": "Computer Graphics Group"
            }
          ],
          "personId": 67125
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Cologne",
              "institution": "TH Köln",
              "dsl": "Computer Graphics Group"
            }
          ],
          "personId": 67171
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Cologne",
              "institution": "TH Köln",
              "dsl": "Computer Graphics Group"
            }
          ],
          "personId": 67054
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Cologne",
              "institution": "TH Köln",
              "dsl": "Computer Graphics Group"
            }
          ],
          "personId": 66870
        }
      ]
    },
    {
      "id": 67242,
      "typeId": 11924,
      "title": "Interactive Visualization of Deep Learning Models in an Immersive Environment",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489956"
        }
      },
      "trackId": 11344,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        67012
      ],
      "abstract": "The development of Deep Learning (DL) models has been prevalent in software engineers. \r\nHowever, analyzing and understanding their behavior is a difficult task for non-experts. \r\nThus, we propose an interactive visualization system of DL models in an immersive environment. \r\nSince an immersive environment offers unlimited displays and visualization of high-dimensional data, it enables interactive visualization of a DL model to analyze in detail how the data propagates through the layers and compares the performance metrics of each model. \r\nIn this study, we implemented a prototype system, demonstrated this to machine learning engineers and discussed the future usefulness of visualizing DL models in an immersive environment.\r\nAs a result, our concept received positive feedback, but we found more consider the visualization technology unique to the immersive environment.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Advanced Technology Lab, Recruit Co., Ltd.",
              "dsl": ""
            }
          ],
          "personId": 67120
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Recruit Co., Ltd.",
              "dsl": "Advanced Technology Lab"
            }
          ],
          "personId": 67037
        }
      ]
    },
    {
      "id": 67243,
      "typeId": 11924,
      "title": "Study of Heart Rate Visualizations on a Virtual Smartwatch",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489913"
        }
      },
      "trackId": 11344,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        67010
      ],
      "abstract": "In this paper, we present three visualizations showing heart rate (HR) data collected over time. Two visualizations present a summary chart (bar or radial chart), summarizing the amount of time spent per HR zone (i.e., low, moderate, high intensity). We conducted a pilot study with five participants to evaluate the efficiency of the visualizations when monitoring the intensity of an activity while playing a tennis-like Virtual Reality game. Preliminary results show that participants were performing better with and preferred the bar chart summary.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Stuttgart",
              "institution": "University of Stuttgart",
              "dsl": ""
            }
          ],
          "personId": 67132
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Stuttgart",
              "institution": "University of Stuttgart",
              "dsl": ""
            }
          ],
          "personId": 67047
        }
      ]
    },
    {
      "id": 67244,
      "typeId": 11924,
      "title": "VRBT: A Non-pharmacological VR Approach towards Hypertension",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489934"
        }
      },
      "trackId": 11344,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        67011
      ],
      "abstract": "Hypertension is a prevalent disease that is known to affect the vascular system especially to people with poor living habits and lifestyles. Virtual reality (VR) is effective at relieving users pressure and improve therapeutic efficiency, which however is less conducted towards manipulating blood pressure and hypertension. In this paper, we consider how hypertension can be treated with VR devices and design virtual reality river  therapy (VRBT) with respect to a combination of traditional methods through sensory stimulation, audio interventions, and motor training.\r\n\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shenzhen",
              "institution": "Tsinghua University",
              "dsl": "Shenzhen International Graduate School"
            }
          ],
          "personId": 67163
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shenzhen",
              "institution": "Tsinghua University",
              "dsl": "Shenzhen International Graduate School"
            }
          ],
          "personId": 67080
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Beijing Tongren Hospital, Capital Medical University",
              "dsl": ""
            }
          ],
          "personId": 67100
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Beijing Tongren Hospital, Capital Medical University",
              "dsl": ""
            }
          ],
          "personId": 67112
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Beijing Tongren Hospital, Capital Medical University",
              "dsl": ""
            }
          ],
          "personId": 67220
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shenzhen",
              "institution": "Tsinghua University",
              "dsl": "Shenzhen International Graduate School"
            }
          ],
          "personId": 67082
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shenzhen",
              "institution": "Tsinghua University",
              "dsl": "Shenzhen International Graduate School"
            }
          ],
          "personId": 67038
        }
      ]
    },
    {
      "id": 67245,
      "typeId": 11924,
      "title": "Conference Talk Training With a Virtual Audience System",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489939"
        }
      },
      "trackId": 11344,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        67010
      ],
      "abstract": "This paper presents the first prototype of a virtual audience system (VAS) specifically designed as a training tool for conference talks. This system has been tailored for university seminars dedicated to the preparation and delivery of scientific talks. We describe the required features which have been identified during the development process. We also summarize the preliminary feedback received from lecturers and students during the first deployment of the system in seminars for bachelor and doctoral students. Finally, we discuss future work and research directions. We believe our system architecture and features are providing interesting insights on the development and integration of VR-based educational tools into university curriculums.  ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "Department of Computer Science, HCI Group",
              "dsl": "University of Würzburg"
            },
            {
              "country": "France",
              "state": "Bretagne",
              "city": "Plouzané",
              "institution": "Lab-STICC UMR CNRS 6285",
              "dsl": "ENIB"
            }
          ],
          "personId": 67043
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "Department of Computer Science, HCI Group",
              "dsl": "University of Würzburg"
            }
          ],
          "personId": 67079
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Plouzané",
              "institution": "Lab-STICC",
              "dsl": "ENIB, CERV"
            }
          ],
          "personId": 67188
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Plouzané",
              "institution": "Lab-STICC",
              "dsl": "ENIB, CERV"
            }
          ],
          "personId": 67045
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "Department of Computer Science, HCI Group",
              "dsl": "University of Würzburg"
            }
          ],
          "personId": 66797
        }
      ]
    },
    {
      "id": 67246,
      "typeId": 11924,
      "title": "Force-Based Foot Gesture Navigation in Virtual Reality",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489945"
        }
      },
      "trackId": 11344,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        67011
      ],
      "abstract": "Navigation is a primary interaction in virtual reality. Previous research has explored different forms of artificial locomotion techniques for navigation, including hand gestures and body motions. However, few studies have investigated force-based foot gestures as a locomotion technique. In this paper, we present three force-based foot gestures (Foot Fly, Foot Step and Foot Teleportation) for navigation in a virtual environment, relying on surface electromyography sensors readings from leg muscles.  A pilot study comparing our techniques with controller-based techniques indicates that force-based foot gestures can provide a fun and engaging alternative. Of all six input techniques evaluated, Foot Fly was often most preferred despite requiring more exertion than the Controller Fly technique. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "Victoria",
              "city": "Melbourne",
              "institution": "Monash University",
              "dsl": ""
            }
          ],
          "personId": 67179
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "SA",
              "city": "Adelaide",
              "institution": "University of South Australia",
              "dsl": "UniSA STEM"
            }
          ],
          "personId": 67044
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "",
              "city": "Melbourne",
              "institution": "Monash University",
              "dsl": ""
            }
          ],
          "personId": 67175
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "Canterbury",
              "city": "Christchurch",
              "institution": "University of Canterbury",
              "dsl": "School of Product Design"
            }
          ],
          "personId": 67129
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "",
              "city": "Melbourne",
              "institution": "Monash University",
              "dsl": "Faculty of Information Technology"
            }
          ],
          "personId": 67084
        }
      ]
    },
    {
      "id": 67247,
      "typeId": 11924,
      "title": "Validating Social Distancing through Deep Learning and VR-Based Digital Twins",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489959"
        }
      },
      "trackId": 11344,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        67011
      ],
      "abstract": "The Covid-19 pandemic resulted in a catastrophic loss to global economies, and social distancing was consistently found to be an effective means to curb the virus's spread. However, it is only as effective when every individual partakes in it with equal alacrity. Past literature outlined scenarios where computer vision was used to detect people and to enforce social distancing automatically. We have created a Digital Twin (DT) of an existing laboratory space for remote monitoring of room occupancy and automatically detecting violation of social distancing. To evaluate the proposed solution, we have implemented a Convolutional Neural Network (CNN) model for detecting people, both in a limited-sized dataset of real humans, and a synthetic dataset of humanoid figures. Our proposed computer vision models are validated for both real and synthetic data in terms of accurately detecting persons, posture, and intermediate distances among people. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "India",
              "state": "",
              "city": "Bangalore",
              "institution": "Indian Institute of Science",
              "dsl": ""
            },
            {
              "country": "India",
              "state": "West Bengal",
              "city": "Kalyani",
              "institution": "Indian Institute of Information Technology",
              "dsl": ""
            }
          ],
          "personId": 67222
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "Karnataka",
              "city": "Bengaluru",
              "institution": "Indian Institute of Science",
              "dsl": "CPDM"
            }
          ],
          "personId": 67127
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "Karnataka",
              "city": "Bengaluru",
              "institution": "Indian Institute of Science",
              "dsl": "I3D"
            }
          ],
          "personId": 67141
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "",
              "city": "Bengaluru",
              "institution": "Indian Institute of Science",
              "dsl": "I3D Lab, Centre for Product Design and Manufacturing"
            }
          ],
          "personId": 67026
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "Kanataka",
              "city": "Bangalore",
              "institution": "Indian Institute of Science",
              "dsl": "CPDM"
            }
          ],
          "personId": 67046
        }
      ]
    },
    {
      "id": 67248,
      "typeId": 11924,
      "title": "Estimate the Difference Threshold for Curvature Gain of Redirected Walking",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489942"
        }
      },
      "trackId": 11344,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        67012
      ],
      "abstract": "Redirected walking (RDW) allows users to navigate a large virtual world in a small physical space. At this time, if the applied redirection is below the detection threshold, the human hardly notice. However, some papers reported that users perceived changes in curvature gain even when redirections smaller than the detection threshold were applied. This means that the change in curvature gain caused human perception. Therefore, in this paper, we identified a threshold for the change in curvature gain, which was found to be 3.06°/m. Further experiments using different variation methods for variations in curvature gain will follow.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Gyeonggi-do",
              "institution": "Korea Institute of Industrial Technology",
              "dsl": "CT Convergence Group"
            },
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Gyeonggi-do",
              "institution": "Korea Institute of Industrial Technology",
              "dsl": "CT Convergence Group"
            }
          ],
          "personId": 67231
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "KITECH",
              "dsl": "CT Convergence Group"
            },
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "KITECH",
              "dsl": "CT Convergence Group"
            }
          ],
          "personId": 67161
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "KITECH",
              "dsl": "CT Convergence Group"
            },
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "KITECH",
              "dsl": "CT Convergence Group"
            }
          ],
          "personId": 67191
        }
      ]
    },
    {
      "id": 67249,
      "typeId": 11924,
      "title": "Immersive Analytics: A User-Centered Perspective",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489951"
        }
      },
      "trackId": 11344,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        67012
      ],
      "abstract": "Researchers have explored using VR and 3D data visualizations for understanding, analyzing, and presenting data for several decades. Surveys of the literature in the field usually adopt a technical or systemic lens. We propose a survey of the Immersive Analytics literature from the user’s perspective that relates the purpose of the visualization to its technical qualities. We present our preliminary review to describe how device technologies, kinds of representation, collaborative features, and research design have been utilized to accomplish the purpose of the visualization. This poster demonstrates our preliminary investigation, inviting feedback from the VRST community. Our hope is the final version of our review will benefit designers, developers, and practitioners who want to implement immersive visualizations from a Human-Centered Design perspective, and help Immersive Analytics researchers get a better understanding of the gaps in current literature.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "North Carolina",
              "city": "Raleigh",
              "institution": "North Carolina State University",
              "dsl": "College of Design"
            }
          ],
          "personId": 67101
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Kansas",
              "city": "Manhattan",
              "institution": "Kansas State University",
              "dsl": ""
            }
          ],
          "personId": 67139
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "North Carolina",
              "city": "Raleigh",
              "institution": "North Carolina State University",
              "dsl": "College of Design"
            }
          ],
          "personId": 67097
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "North Carolina",
              "city": "Raleigh",
              "institution": "North Carolina State University",
              "dsl": "College of Design"
            }
          ],
          "personId": 67143
        }
      ]
    },
    {
      "id": 67250,
      "typeId": 11924,
      "title": "Using Hand Tracking and Voice Commands to Physically Align Virtual Surfaces in AR for Handwriting and Sketching]{Using Hand Tracking and Voice Commands to Physically Align Virtual Surfaces in AR for Handwriting and Sketching with HoloLens 2",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489940"
        }
      },
      "trackId": 11344,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        67010
      ],
      "abstract": "In this paper, we adapt an existing VR framework for handwriting and sketching on physically aligned virtual surfaces to AR environments using the Microsoft HoloLens 2. We demonstrate a multimodal input metaphor to control the framework's calibration features using hand tracking and voice commands. Our technical evaluation of fingertip/surface accuracy and precision on physical tables and walls is in line with existing measurements on comparable hardware, albeit considerably lower compared to previous work using controller-based VR devices. We discuss design considerations and the benefits of our unified input metaphor suitable for controller tracking and hand tracking systems. We encourage extensions and replication by providing a publicly available reference implementation (http://link-to-the-repo.com, anonymized for review).",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Wuerzburg",
              "institution": "Human-Computer Interaction (HCI) Group",
              "dsl": "University of Wuerzburg"
            }
          ],
          "personId": 67071
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Wuerzburg",
              "institution": "Human-Computer Interaction (HCI) Group",
              "dsl": "University of Wuerzburg"
            }
          ],
          "personId": 67023
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "University of Wuerzburg",
              "institution": "Human-Computer Interaction (HCI) Group",
              "dsl": "University of Wuerzburg"
            }
          ],
          "personId": 67205
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Wuerzburg",
              "institution": "Human-Computer Interaction (HCI) Group",
              "dsl": "University of Wuerzburg"
            }
          ],
          "personId": 66797
        }
      ]
    },
    {
      "id": 67251,
      "typeId": 11924,
      "title": "A System for Practicing Ball/Strike Judgment in VR Environment",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489931"
        }
      },
      "trackId": 11344,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        67011
      ],
      "abstract": "The purpose of this study is to develop an easy-to-use ball/strike judgment practice system for inexperienced baseball umpires. The main idea is to provide a practice environment in a Virtual Reality (VR) space. With our system, users observe a pitched ball, perform ball/strike judgment, and review their judgment in a VR space. Since the whole process is completed in VR, users can practice the judgments without preparing a pitcher and catcher. A user investigation in which participants practiced with our system and judged balls thrown by a pitching machine was conducted. The participants responded positively when asked about the usefulness of our system.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Koto-ku",
              "institution": "Shibaura Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 67190
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Shibaura Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 67140
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Shibaura Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 67019
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Kanagawa",
              "city": "Sagamihara",
              "institution": " Kitasato University",
              "dsl": "College of Liberal Arts and Sciences"
            }
          ],
          "personId": 67072
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "TOkyo",
              "institution": "Shibaura tech",
              "dsl": "computer science and engineering"
            }
          ],
          "personId": 67040
        }
      ]
    },
    {
      "id": 67252,
      "typeId": 11924,
      "title": "Safety First: A Study of Users’ Perception of VR Adoption in Vehicles",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489914"
        }
      },
      "trackId": 11344,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        67012
      ],
      "abstract": "The increasing ubiquity and mobility of VR devices has introduced novel use cases, one of which is using VR while in dynamic, on-the-go environments. Hence, there is a need to examine the perceptual, cognitive, and behavioral aspects of both the driving experience and VR immersion, and how they influence each other. As an initial step towards this goal, we report on the results of an online survey that investigated users' perceived safety of using VR in an AV. The results of the survey show a mix of expected and surprising attitudes towards VR-in-the-car.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Interactive Computing"
            }
          ],
          "personId": 67014
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology ",
              "dsl": "Psychology "
            }
          ],
          "personId": 67228
        }
      ]
    },
    {
      "id": 67253,
      "typeId": 11924,
      "title": "Table-Based Interactive System for Augmenting Japanese Food Culture Experience",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489941"
        }
      },
      "trackId": 11344,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        67011
      ],
      "abstract": "Washoku, traditional Japanese food culture, was evaluated as a social custom for food that embodies the Japanese spirit of respect for nature and was registered as a UNESCO Intangible Cultural Heritage in 2013. However, an actual meal is limited to taste and visual information such as taste, ingredients, tableware, and arrangements; it is difficult to become thoroughly familiar with the cultural characteristics of Japanese cuisine. This study achieved a system that conveys the characteristics of Japanese cuisine, such as the importance of seasonality and ingredients, by displaying the cultural background related to food in text form. The natural environment is projected by a projector on a table, and seasons progress as the meal advances. The food was created in consultation with the chef to be suitable for the system. The users who participated in our survey and experienced the system were conveyed that Japanese cuisine is supported by the richness and seasons of nature and that it also affects traditional events.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Ibaraki",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": ""
            }
          ],
          "personId": 67164
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": ""
            }
          ],
          "personId": 67015
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Ibaraki",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": "Entertainment Computing Labratory"
            }
          ],
          "personId": 67102
        }
      ]
    },
    {
      "id": 67254,
      "typeId": 11924,
      "title": "Fishtank Sandbox : A Software Framework for Collaborative Usability Testing of Fish Tank Virtual Reality Interaction Techniques",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489915"
        }
      },
      "trackId": 11344,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        67011
      ],
      "abstract": "Human-computer interaction researchers have been studying how we can interact with virtual objects in a virtual environment efficiently. Many usability experiments do not have the same control parameters. The lack of consistency makes comparing different interaction techniques difficult. In this article, we present a software framework for usability study in FTVR interaction techniques. The software framework provides fixed control parameters (e.g., task, graphic settings, and measuring parameters), the ability for other researchers to incorporate their interaction techniques as an add-on, and enabling individuals to participate in the experiment over the internet. The article explores a new way for VR/AR researchers to approach usability experiments using the framework and discuss the challenges that it brings.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Thailand",
              "state": "Rangsit",
              "city": "Pathum thani",
              "institution": "Thammasat University",
              "dsl": "Faculty of Engineering"
            },
            {
              "country": "Thailand",
              "state": "Rangsit",
              "city": "Pathum thani",
              "institution": "Thammasat University",
              "dsl": "Faculty of Engineering"
            }
          ],
          "personId": 67027
        },
        {
          "affiliations": [
            {
              "country": "Thailand",
              "state": "",
              "city": "Pathum Thani",
              "institution": "Thammasat University",
              "dsl": "Faculty of Engineering"
            },
            {
              "country": "Thailand",
              "state": "",
              "city": "Pathum Thani",
              "institution": "Thammasat University",
              "dsl": "Faculty of Engineering"
            }
          ],
          "personId": 67069
        }
      ]
    },
    {
      "id": 67255,
      "typeId": 11924,
      "title": "Visual Transition of Avatars Improving Speech Comprehension in Noisy VR Environments",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489932"
        }
      },
      "trackId": 11344,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        67011
      ],
      "abstract": "In order to construct a comfortable communication in the VR space, it is important to improve the speech comprehension in environmental noise. Although there have been many reports on the interaction between vision and acoustic, few studies using noisy VR spaces. In this study, sixteen Japanese male and female were tested to listen to a some sentence in a VR space with environmental noise, to evaluate the effect of the visual stimulus to the avatar speech comprehension against the environmental noise, with using the up-and-down method. The results showed that the cocktail party effect was also observed in the VR avatars, and the cocktail party effect continued even if the avatar vanished visually. In addition, it was suggested that the cocktail party effect was enhanced if the lip of the avatar synchronized correctly.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": ""
            }
          ],
          "personId": 67111
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": ""
            }
          ],
          "personId": 67186
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": "Graduate School of Science and Technology"
            }
          ],
          "personId": 67067
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": "College of Engineering Systems"
            }
          ],
          "personId": 67218
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": "Faculty of Engineering, Information and Systems"
            }
          ],
          "personId": 67081
        }
      ]
    },
    {
      "id": 67256,
      "typeId": 11924,
      "title": "Managing a Crisis in Virtual Reality - Tackling a Wildfire",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489957"
        }
      },
      "trackId": 11344,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        67010
      ],
      "abstract": "In this paper we present a virtual reality application, where multiple users can observe and interact with a portion of geo-referenced terrain where a real wildfire took place. The application presents a layout with two maps, one is a three-dimensional view with terrain elevation and the other is a conventional two-dimensional view. The VR users can control different layers (roads, waterways, etc), control the wildfire's playback, command vehicles to change positions and paint the terrain conveying information to one-another. This work explores how users interact with map visualizations and plan for a crisis management scenario within a virtual environment.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Caparica",
              "institution": "Faculdade de Ciências e Tecnologia, Universidade Nova de Lisboa",
              "dsl": "NOVA-LINCS, DI"
            }
          ],
          "personId": 67062
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Caparica",
              "institution": "Faculdade de Ciências e Tecnologia, Universidade Nova de Lisboa",
              "dsl": "NOVA-LINCS, DI"
            }
          ],
          "personId": 67089
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisbon",
              "institution": "Nova School of Science and Technology",
              "dsl": "NOVA LINCS, Department of Informatics"
            }
          ],
          "personId": 67213
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Caparica",
              "institution": "Faculdade de Ciências e Tecnologia, Universidade Nova de Lisboa",
              "dsl": "NOVA-LINCS"
            }
          ],
          "personId": 67035
        }
      ]
    },
    {
      "id": 67257,
      "typeId": 11924,
      "title": "ALiSE: Non-wearable AR display through the looking glass, and what looks solid there",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489929"
        }
      },
      "trackId": 11344,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        67011
      ],
      "abstract": "On the method of Augmented Reality mirror display using a half-mirror, there is a difference in focal length between the mirror image and the AR image. Therefore, the observer perceives a mismatch in depth perception, which impairs usability. In this study, we developed an optical-reflection AR display, ALiSE (Augment Layer interweaved Semi-reflecting Existence), which enhances the depth perception experience of AR images by adding a gap zone of the same depth as the target depth between the display and the half-mirror. We conducted an experiment to view 3D objects and virtual fitting using the existing AR with video synthesis and the proposed ALiSE method. As a result of the questionnaire survey, although the comfortless of wearing virtual objects below existing methods, it was confirmed the terms of presence and solidity are superior in the proposed method. This is an attempt to create a sense of the stereoscopic effect despite the 2D projection, as the object to be projected is simultaneously reflected in the mirror along with the observer oneself.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": ""
            }
          ],
          "personId": 67116
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": "Information and Systems"
            }
          ],
          "personId": 67185
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": ""
            }
          ],
          "personId": 67226
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": "Faculty of Engineering, Information and Systems"
            }
          ],
          "personId": 67081
        }
      ]
    },
    {
      "id": 67258,
      "typeId": 11924,
      "title": "The Effect of 2D Stylized Visualization of the Real World for Obstacle Avoidance and Safety in Virtual Reality System Usage",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489943"
        }
      },
      "trackId": 11344,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        67012
      ],
      "abstract": "Using virtual reality systems with the head-mounted display can incur interaction difficulties and safety problems because of the user's view being isolated from the real world operating space. One possible solution is to super-impose the real world objects or environment information onto the virtual scene. A variety of such visualization methods have been proposed, all in hopes of minimizing the negative effects of introducing foreign elements to the original virtual scene. In this poster, we propose to apply the neural style transfer technique to blend in the real world operating environment in the style of the given virtual space to make the super-imposed resulting image as natural as possible, maintaining the sense of immersion with the least level of distraction. Our pilot experimental study has shown that the stylization obscured the clear presentation of the environment and worsened or did not improve the safe user performance, and was neither considered sufficiently natural.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Korea University",
              "dsl": "Digital Experience Laboratory"
            },
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Korea University",
              "dsl": "Digital Experience Laboratory"
            }
          ],
          "personId": 67167
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Korea University",
              "dsl": "Digital Experience Laboratory"
            },
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Korea University",
              "dsl": "Digital Experience Laboratory"
            }
          ],
          "personId": 67136
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "NA",
              "city": "Seoul",
              "institution": "Korea University",
              "dsl": "Digital Experience Laboratory"
            },
            {
              "country": "Korea, Republic of",
              "state": "NA",
              "city": "Seoul",
              "institution": "Korea University",
              "dsl": "Digital Experience Laboratory"
            }
          ],
          "personId": 67209
        }
      ]
    },
    {
      "id": 67259,
      "typeId": 11924,
      "title": "Visualisation methods for patient monitoring in anaesthetic procedures using augmented reality",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489908"
        }
      },
      "trackId": 11344,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        67010
      ],
      "abstract": "In health  care, there are still many devices with poorly designed user interfaces that can lead to user errors. Especially in acute care, an error can lead to critical conditions in\r\npatients. Previous research has shown that the use of augmented reality can help to\r\nbetter monitor the condition of patients and better detect unforeseen events. The\r\nsystem created in this work is intended to aid in the detection of changes in patient and equipment-data in order to increase detection of critical conditions or errors.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "HCI Group, University of Würzburg",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 67114
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "University of Würzburg",
              "dsl": "Department of Computer Science, HCI Group"
            }
          ],
          "personId": 67202
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "HCI Group, University of Würzburg",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 67205
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "University Hospital of Würzburg",
              "dsl": ""
            }
          ],
          "personId": 67034
        }
      ]
    },
    {
      "id": 67260,
      "typeId": 11924,
      "title": "The Application of Virtual Reality in Student Recruitment",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489936"
        }
      },
      "trackId": 11344,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        67010
      ],
      "abstract": "In this paper we present details of a virtual tour and game for VR headset that are designed to investigate an interactive and engaging approach of applying VR to student recruitment for an undergraduate course. The VR tour employs a floating menu to navigate through a set of 360° panoramic photographs of the teaching environment and uses hotspot interaction to display more information about the course. The VR game is a fast-paced shooting game. The course information is embedded on cubes that the player needs to focus on and shoot. The game is expected to create a fun way to promote the course. This is work in progress and in this paper we outline the concept and development of the prototype, as well as discussing the next stages of testing in order to evaluate the effectiveness of applying VR to undergraduate student recruitment. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Poole",
              "institution": "Bournemouth University",
              "dsl": ""
            }
          ],
          "personId": 67142
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Poole",
              "institution": "Bournemouth University",
              "dsl": ""
            },
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Poole",
              "institution": "Invidar",
              "dsl": ""
            }
          ],
          "personId": 67169
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Poole",
              "institution": "Bournemouth University",
              "dsl": "Creative Technology Department"
            }
          ],
          "personId": 67131
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Poole",
              "institution": "Bournemouth University",
              "dsl": "Faculty of Science and Technology,"
            }
          ],
          "personId": 67204
        }
      ]
    },
    {
      "id": 67261,
      "typeId": 11924,
      "title": "Recreating a medieval mill as a virtual learning environment",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489899"
        }
      },
      "trackId": 11344,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        67010
      ],
      "abstract": "Historic buildings inside open-air museums often lack a good accessibility and visitors rarely can interact with tools to learn about processes. Providing these buildings in Virtual Reality could be a great supplement for museums to provide accessible and interactive offers. To investigate the effectiveness of this approach and to derive design guidelines, we developed a working and interactive virtual counterpart of a medieval mill. We present the design of the mill and the results of a preliminary usability evaluation.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "Julius-Maximilians University",
              "dsl": ""
            }
          ],
          "personId": 67048
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "University of Würzburg",
              "dsl": "Department of Computer Science, HCI Group"
            }
          ],
          "personId": 67202
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "Department of Computer Science, HCI Group",
              "dsl": "University of Würzburg"
            }
          ],
          "personId": 66797
        }
      ]
    },
    {
      "id": 67262,
      "typeId": 11924,
      "title": "VR Rehearse & Perform - A platform for rehearsing in Virtual Reality",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489896"
        }
      },
      "trackId": 11344,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        67010
      ],
      "abstract": "In this paper, we propose a new VR solution, VR Rehearse & Perform, built to enhance the rehearsal efforts of performers by accurately recreating the visual environment as well as real-time 3D acoustics of landmark venues.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "University Arts London",
              "dsl": "Creative Computing Institute"
            }
          ],
          "personId": 67053
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "University of the Arts",
              "dsl": "Creative Computing Institute"
            },
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Canterbury",
              "institution": "University of Kent",
              "dsl": "Engineering and Digital Arts"
            }
          ],
          "personId": 67099
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "Royal College of Music",
              "dsl": ""
            }
          ],
          "personId": 67195
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "Reactify Music",
              "dsl": ""
            }
          ],
          "personId": 67183
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "University of the Arts, London",
              "dsl": "Creative Computing Institute"
            }
          ],
          "personId": 67058
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Canterbury",
              "institution": "Uni of Kent",
              "dsl": "engineering and digital arts"
            }
          ],
          "personId": 67118
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "University of the Arts London",
              "dsl": "Creative Computing Insitute"
            }
          ],
          "personId": 67057
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Canterbury",
              "institution": "University of Kent",
              "dsl": ""
            }
          ],
          "personId": 67106
        }
      ]
    },
    {
      "id": 67263,
      "typeId": 11924,
      "title": "Natural walking speed prediction in Virtual Reality while using target selection-based locomotion",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489944"
        }
      },
      "trackId": 11344,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        67011
      ],
      "abstract": "Travelling speed plays an essential role in the overall user experience while navigating inside a virtual environment. Researchers have used various travelling speed that matches the user speed profile in order to give a natural walking experience. However, predicting a user's instantaneous walking speed can be challenging when there is no continuous input from the user. Target selection-based techniques are those where the user selects the target to reach there automatically. These techniques also lack naturalness due to their low interaction fidelity. In this work, we have proposed a mathematical model that can dynamically compute the instantaneous natural walking speed while moving from one point to another in a virtual environment. We formulated our model with the help of user studies. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "India",
              "state": "",
              "city": "Guwahati",
              "institution": "Indian Institute of Technology Guwahati",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 67024
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "Assam",
              "city": "Guwahati",
              "institution": "Indian Institute of Technology",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 67113
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "Assam",
              "city": "Guwahati",
              "institution": "Indian Institute of Technology",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 67109
        }
      ]
    },
    {
      "id": 67264,
      "typeId": 11924,
      "title": "A Compact and Low-cost VR Tooth Drill Training System using Mobile HMD and Stylus Smartphone",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489933"
        }
      },
      "trackId": 11344,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        67011
      ],
      "abstract": "Drilling teeth is a significant training element requiring a long time to acquire sufficient skills for dental learners. However, the existing VR tooth drill training simulators are physically large and costly, which cannot be used at home or classroom for a private study. This work presents a novel low-cost mobile VR dental simulator using off-the-shelf devices including a mobile HMD and a stylus smartphone. In this system, a 3D-printed physical teeth prop is placed on an EMR stylus smartphone where the stylus tracks the tip position and hover of a physical drill. Unlike existing solutions using haptic/force devices, our approach involving physical contact between the prop and drill tip enables the user's natural teeth hardness sensation. The use of smartphone stylus would enable significantly more accurate drill position sensing around the teeth than HMD's accompanying controllers. We also develop VR software to simulate tooth drilling on this setup. This demo will show how our new mobile simulator offers a realistic feeling of drilling teeth.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Sendai",
              "institution": "Tohoku University",
              "dsl": ""
            }
          ],
          "personId": 67094
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Sendai",
              "institution": "Tohoku University",
              "dsl": ""
            }
          ],
          "personId": 67208
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Sendai",
              "institution": "Tohoku University",
              "dsl": ""
            }
          ],
          "personId": 67042
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Sendai",
              "institution": "Tohoku University",
              "dsl": ""
            }
          ],
          "personId": 67176
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Yahoo Japan Corporation",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Sendai",
              "institution": "Tohoku University",
              "dsl": ""
            }
          ],
          "personId": 67203
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Sendai",
              "institution": "Tohoku University",
              "dsl": ""
            }
          ],
          "personId": 66883
        }
      ]
    },
    {
      "id": 67265,
      "typeId": 11924,
      "title": "Emotional Virtual Reality Stroop Task: Pilot Design",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489952"
        }
      },
      "trackId": 11344,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        67010
      ],
      "abstract": "Anxiety-inducing and assessment methods in Virtual Reality has been a topic of discussion in recent literature. The importance of the topic is related to the difficulty of getting accurate and timely measurements of anxiety without relying on self-report and breaking the immersion. To this end, the current study utilises the emotional version of a well-established cognitive task; the Stroop Color-Word Task and brings it to Virtual Reality. It consists of three levels; congruent which is used as control and corresponds with no anxiety, incongruent, which corresponds with mild anxiety and emotional, which corresponds with severe anxiety. This pilot serves two functions. The first is to validate the effects of the task using biosignal measurements. The second is to use the bio signal information and the labels to train a machine-learning algorithm. The information collected by the pilot will be used to decide what types of signals and devices to use in the final product, as well as what algorithm and time frame will be better suited for the purpose of accurately determining the user’s anxiety level within Virtual Reality without breaking the immersion.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Ireland",
              "state": "",
              "city": "Cork",
              "institution": "University College Cork",
              "dsl": ""
            },
            {
              "country": "Ireland",
              "state": "",
              "city": "Cork",
              "institution": "University College Cork",
              "dsl": ""
            }
          ],
          "personId": 67170
        },
        {
          "affiliations": [
            {
              "country": "Ireland",
              "state": "",
              "city": "Cork",
              "institution": "University College Cork",
              "dsl": ""
            },
            {
              "country": "Ireland",
              "state": "",
              "city": "Cork",
              "institution": "University College Cork",
              "dsl": ""
            }
          ],
          "personId": 67021
        },
        {
          "affiliations": [
            {
              "country": "Ireland",
              "state": "",
              "city": "Cork",
              "institution": "University College Cork",
              "dsl": ""
            },
            {
              "country": "Ireland",
              "state": "",
              "city": "Cork",
              "institution": "University College Cork",
              "dsl": ""
            }
          ],
          "personId": 67199
        }
      ]
    },
    {
      "id": 67266,
      "typeId": 11924,
      "title": "Miniature AR: Multi-view 6DOF Virtual Object Visualization  for a Miniature Diorama",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489938"
        }
      },
      "trackId": 11344,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        67011
      ],
      "abstract": "We describe a miniature diorama AR system called ‘Miniature AR’ which can be applied to a mechanical diorama to extend the content’s feasibility by overlapping virtual objects on the complex diorama structure. The previous AR researches for the diorama are usually based on 2D planar recognition, and multiple user experience cannot be considered due to multi-devices synchronization. To overcome these constraints, in this paper, we show a new diorama AR system suitable for a tiny complex structure. The contributions of our work are i) design of the diorama AR system, ii) AR space generation and 6DOF view device tracking for diorama, iii) multiple view and event synchronization for multiple users. The utility of the approach has been demonstrated under a real diorama environment (miniature of a ski slope) using mobile devices.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "Electronics and Telecommunications Research Institute (ETRI)",
              "dsl": "Content Research Division"
            }
          ],
          "personId": 67033
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "Electronics and Telecommunications Research Institute (ETRI)",
              "dsl": "Content Research Division"
            }
          ],
          "personId": 67065
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "Electronics and Telecommunications Research Institute (ETRI)",
              "dsl": "Content Research Division"
            }
          ],
          "personId": 67200
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "Electronics and Telecommunications Research Institute (ETRI)",
              "dsl": "Content Research Division"
            }
          ],
          "personId": 67149
        }
      ]
    },
    {
      "id": 67267,
      "typeId": 11924,
      "title": "Freehand Interaction in Virtual Reality: Bimanual Gestures for Cross-Workspace Interaction",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489900"
        }
      },
      "trackId": 11344,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        67012
      ],
      "abstract": "This work presents the design and evaluation of three bimanual interaction modalities for cross-workspace interaction in virtual reality (VR), in which the user can move items between a personal workspace and a shared workspace. In VR social platforms, personal and shared workspaces are commonly used to support virtual presentations, remote collaboration, and data sharing. In this work, interaction modalities are created by bimanually grouping freehand gestures including pointing, holding, and grabbing, which are known to be essential ones for interaction in VR. \r\nWe conducted an empirical study to understand advantages and drawbacks among the three modalities as well as their suitability for cross-workspace interaction in VR, which we hope are valuable to assist the design of future VR social platforms. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Rochester",
              "institution": "Rochester Institute of Technology",
              "dsl": "School of Interactive Games and Media"
            }
          ],
          "personId": 67214
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Rochester",
              "institution": "Rochester Institute of Technology",
              "dsl": "School of Interactive Games and Media"
            }
          ],
          "personId": 67221
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Rochester",
              "institution": "Rochester Institute of Technology",
              "dsl": "School of Interactive Games and Media"
            }
          ],
          "personId": 67182
        }
      ]
    },
    {
      "id": 67268,
      "typeId": 11924,
      "title": "Incorporating Human Behavior in VR Compartmental Simulation Models",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489946"
        }
      },
      "trackId": 11344,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        67010
      ],
      "abstract": "A novel strand of Coronavirus, SARS-CoV-2, has caused the COVID-19 respiratory syndrome to affect a large number of individuals worldwide, putting a considerable stress to national health services and causing many deaths. To prevent or at least mitigate the impact of this virus, many control measures have been put in place across different countries with the aim to save lives at the cost of personal freedom.\r\nComputer simulations have played a role in providing policy makers with critical information about the virus. However, despite their importance in applied epidemiology, general simulation models, are difficult to validate because of how hard it is to predict and model human behavior. To this end, we propose a different approach by developing a virtual reality (VR) multi-agent virus propagation system where a group of agents interact with the user in a university setting. We created a VR digital twin replica of a university building, to enhance the user's immersion in our study.\r\nOur work integrates human behavior seamlessly in a simulation model and we believe that this approach is crucial to have a deeper understanding on how such epidemics can be controlled. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Derby",
              "institution": "University of Derby",
              "dsl": "Department of Computing and Engineering"
            }
          ],
          "personId": 67083
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Derby",
              "institution": "University of Derby",
              "dsl": "Department of Computing and Engineering"
            }
          ],
          "personId": 67076
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Derby",
              "institution": "University of Derby",
              "dsl": "Department of Computing and Engineering"
            }
          ],
          "personId": 67018
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Derby",
              "institution": "University of Derby",
              "dsl": "Department of Computing and Engineering"
            }
          ],
          "personId": 67217
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Derby",
              "institution": "University of Derby",
              "dsl": "Department of Computing and Engineering"
            }
          ],
          "personId": 67133
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Derby",
              "institution": "University of Derby",
              "dsl": "Department of Computing and Engineering"
            }
          ],
          "personId": 67064
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "Derbyshire",
              "city": "Derby",
              "institution": "University of Derby",
              "dsl": "Department of Computing"
            }
          ],
          "personId": 67168
        }
      ]
    },
    {
      "id": 67269,
      "typeId": 11924,
      "title": "Evaluating the influence of interaction technology on procedural learning using Virtual Reality",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489918"
        }
      },
      "trackId": 11344,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        67010
      ],
      "abstract": "Within the context of industry 4.0, this paper studies the influence of interaction technology (Vive controller and Knuckles) on manufacturing assembly procedural training using Virtual Reality. To do so, an experiment with 24 volunteers have been conducted and these participants have been separated in two groups: one using Vive controller and the other using Knuckles. Our conclusions are based on two indicators: Time to realize all tasks and the number of manipulations. This study shows that, after get used to, volunteers using Knuckles are faster than the other group but for some very dedicated tasks, they need more manipulations to succeed",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Le Mans",
              "institution": "LINEACT - CESI",
              "dsl": ""
            }
          ],
          "personId": 67210
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Le Mans",
              "institution": "LINEACT CESI",
              "dsl": ""
            }
          ],
          "personId": 67159
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "Normandie",
              "city": "Rouen",
              "institution": "CESI",
              "dsl": "LINEACT"
            }
          ],
          "personId": 67039
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "Normandie",
              "city": "Rouen",
              "institution": "CESI",
              "dsl": "LINEACT"
            }
          ],
          "personId": 67187
        }
      ]
    },
    {
      "id": 67270,
      "typeId": 11924,
      "title": "A Pilot Study Examining the Unexpected Vection Hypothesis of Cybersickness.",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489895"
        }
      },
      "trackId": 11344,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        67011
      ],
      "abstract": "The relationship between vection (illusory self-motion) and cybersickness is complex. This pilot study examined whether only unexpected vection provokes sickness during head-mounted display (HMD) based virtual reality (VR). 20 participants ran through the tutorial of Mission: ISS (an HMD VR app) until they experienced notable sickness (maximum exposure was 15 minutes). We found that: 1) cybersickness was positively related to vection strength; and 2) cybersickness appeared to be more likely to occur during unexpected vection. Given the implications of these findings, future studies should attempt to replicate them and confirm the hypothesis with larger sample sizes and rigorous experimental designs.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "New South Wales",
              "city": "Wollongong",
              "institution": "University of Wollongong",
              "dsl": "School of Psychology"
            }
          ],
          "personId": 67108
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "",
              "city": "Wollongong",
              "institution": "University of Wollongong",
              "dsl": "School of Psychology"
            }
          ],
          "personId": 67130
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "NSW 2522",
              "city": "Wollongong",
              "institution": "University of Wollongong",
              "dsl": "School of Psychology"
            }
          ],
          "personId": 67193
        }
      ]
    },
    {
      "id": 67271,
      "typeId": 11924,
      "title": "Technical Factors Affecting Augmented Reality User Experiences in Sports Spectating",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489906"
        }
      },
      "trackId": 11344,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        67012
      ],
      "abstract": "The maturity of augmented reality (AR) technology and research now paves the way for dissemination of AR outside of the laboratory. However, it is still under-explored which factors are influencing the user experience of an AR application. In this poster, we describe some of the technical factors that could influence the user experience. We focus on a use-case in the field of on-site sports spectating with mobile AR. We present a study design which analyzes the influence of latency, registration accuracy, and jitter as factors on AR user experience.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "Otago",
              "city": "Dunedin",
              "institution": "University of Otago",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 67145
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "Otago",
              "city": "Dunedin",
              "institution": "University of Otago",
              "dsl": "Department of Information Science"
            }
          ],
          "personId": 67025
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Dunedin",
              "institution": "University of Otago",
              "dsl": "Computer Science"
            }
          ],
          "personId": 67184
        }
      ]
    },
    {
      "id": 67272,
      "typeId": 11924,
      "title": "3D Printing an Accessory Dock for XR Controllers and its Exemplary Use as XR Stylus",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489949"
        }
      },
      "trackId": 11344,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        67010
      ],
      "abstract": "This article introduces the accessory dock, a 3D printed multi-purpose extension for consumer-grade XR controllers that enables flexible mounting of self-made and commercial accessories. The uniform design of our concept opens new opportunities for XR systems being used for more diverse purposes, e.g., researchers and practitioners could use and compare arbitrary XR controllers within their experiments while ensuring access to buttons and battery housing. As a first example, we present a stylus tip accessory to build an XR Stylus, which can be directly used with frameworks for handwriting, sketching, and UI interaction on physically aligned virtual surfaces. For new XR controllers, we provide instructions on how to adjust the accessory dock to the controller’s form factor. The source files for 3D printing are publicly available for reuse, replication, and extension (https://link-to-the-repo.info/, anonymized for review).",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "Department of Computer Science, HCI Group",
              "dsl": "University of Würzburg"
            }
          ],
          "personId": 67071
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "Department of Computer Science",
              "dsl": "University of Wuerzburg"
            }
          ],
          "personId": 67036
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "Department of Computer Science, HCI Group",
              "dsl": "University of Wuerzburg"
            }
          ],
          "personId": 67215
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "Department of Computer Science, Mediainformatics Group",
              "dsl": "University of Würzburg"
            }
          ],
          "personId": 67216
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "Department of Computer Science, HCI Group",
              "dsl": "University of Würzburg"
            }
          ],
          "personId": 66797
        }
      ]
    },
    {
      "id": 67273,
      "typeId": 11924,
      "title": "CAVE applications: from craft manufacturing to product line engineering",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489948"
        }
      },
      "trackId": 11344,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        67010
      ],
      "abstract": "Product line engineering model is suitable for engineering related software products in an efficient manner, taking advantage of their similarities while managing their differences. Our feature driven software product line (SPL) solution based on that model allows for instantiation of different CAVE products based on the set of core assets and driven by a set of common VR features with the minimal budget and time to market.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Poland",
              "state": "",
              "city": "Gdansk",
              "institution": "Gdansk University of Technology",
              "dsl": "Faculty of Electronics, Telecommunications and Informatics"
            }
          ],
          "personId": 67059
        },
        {
          "affiliations": [
            {
              "country": "Poland",
              "state": "",
              "city": "Gdansk",
              "institution": "Gdansk University of Technology",
              "dsl": "Faculty of Electronics, Telecommunications and Informatics"
            }
          ],
          "personId": 67055
        }
      ]
    },
    {
      "id": 67274,
      "typeId": 11924,
      "title": "A Hat-shaped Pressure-Sensitive Multi-Touch Interface for Virtual Reality",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489909"
        }
      },
      "trackId": 11344,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        67011
      ],
      "abstract": "In this paper, we show a hat-shaped touch interface for viewpoint interaction in VR. It is made of conductive fabric and thus lightweight. \r\nIt allows the user to touch, drag, and push the surface, enabling various 3D viewpoint control.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Ibaraki",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": "IPLAB"
            }
          ],
          "personId": 67173
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Ibaraki",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": "IPLAB"
            }
          ],
          "personId": 67181
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Ibaraki",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": "IPLAB"
            }
          ],
          "personId": 67155
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Ibaraki",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": "IPLAB"
            }
          ],
          "personId": 67150
        }
      ]
    },
    {
      "id": 67275,
      "typeId": 11924,
      "title": "Fluid3DGuides: A Technique for Structured 3D Drawing in VR",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489955"
        }
      },
      "trackId": 11344,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        67011
      ],
      "abstract": "We propose Fluid3DGuides, a drawing guide technique to help users draw structured sketches more accurately in VR. The prototype system continuously infers visual guide lines for the user based on the user's instant stroke drawing intention and its potential constraint relationship with the existing strokes. We evaluated our prototype through a pilot user study with six participants by comparing the proposed guide technique against the non-guided drawing condition. Participants gave positive comments on ease of use and drawing accuracy. They found that the technique could reduce the time and effort required to find the corrected drawing perspective and obtain more accurate 3D structured sketches.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Xian，China",
              "institution": "NWPU",
              "dsl": ""
            },
            {
              "country": "China",
              "state": "",
              "city": "Xian，China",
              "institution": "NWPU",
              "dsl": ""
            }
          ],
          "personId": 67031
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Shaanxi",
              "city": "Xi'an",
              "institution": "Northwestern Polytechnical University",
              "dsl": "School of Mechanical Engineering"
            }
          ],
          "personId": 67075
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Shaanxi",
              "city": "Xian",
              "institution": "Northwestern Polytechnical University",
              "dsl": ""
            }
          ],
          "personId": 67041
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Shaanxi",
              "city": "Xi'an",
              "institution": "Northwestern Polytechnical university",
              "dsl": "Cyber-Physical Interaction Lab.（CPI lab）"
            }
          ],
          "personId": 67049
        }
      ]
    },
    {
      "id": 67276,
      "typeId": 11924,
      "title": "Of Leaders and Directors: A visual model to describe and analyse persistent visual cues directing to single out-of view targets",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489953"
        }
      },
      "trackId": 11344,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        67010
      ],
      "abstract": "Researchers have come up with many visual cues that can guide Virtual (VR) and Augmented Reality (AR) users to out of view objects. The paper provides a classification of cues and tasks and framework to analyse/describe visual cues to support comparisons and design. The paper applies the framework to compare the results of previous work done on visual cues and find that different cues have similar performance for the same task types.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Aalborg",
              "institution": "Aalborg University",
              "dsl": "Department of Architecture, Design and Media Technology"
            }
          ],
          "personId": 67095
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Aalborg",
              "institution": "Aalborg University",
              "dsl": "Department of Electronic Systems"
            }
          ],
          "personId": 67092
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Aalborg",
              "institution": "Aalborg University",
              "dsl": "Department of Architecture, Design and Media Technology"
            }
          ],
          "personId": 67174
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Aalborg",
              "institution": "Aalborg University",
              "dsl": "Department of Architecture, Design and Media Technology"
            }
          ],
          "personId": 67144
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Aalborg",
              "institution": "Aalborg University",
              "dsl": "Department of Architecture, Design and Media Technology"
            }
          ],
          "personId": 67068
        }
      ]
    },
    {
      "id": 67277,
      "typeId": 11924,
      "title": "An Interactive Flight Operation with 2-DOF Motion Platform",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489911"
        }
      },
      "trackId": 11344,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        67010
      ],
      "abstract": "We propose an interactive core training system that enables user to tilt greatly according to the user posture and VR environment in a VR flight. To increase the training load, we incorporate the planche exercise and our low-cost motion platform with balance board in a flight operation. By tilting the body back and forth and left and right while keeping the body horizontal based on the planche exercise, the virtual aircraft tilts in that direction and the motion platform also rolling movements. In addition, since our motion platform with the balance board swings by rolling motion, it is possible to realize a large swing at low-cost and safely.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Hiroshima",
              "institution": "Hiroshima City University",
              "dsl": ""
            }
          ],
          "personId": 67052
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Hiroshima",
              "city": "Hiroshima",
              "institution": "Hiroshima City University",
              "dsl": "Graduate School of Information Sciences"
            }
          ],
          "personId": 67172
        }
      ]
    },
    {
      "id": 67278,
      "typeId": 11924,
      "title": "Immersive Furnishing: Randomized Big Five Personality Traits Based Interior Layouts",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489919"
        }
      },
      "trackId": 11344,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        67012
      ],
      "abstract": "In this work, we created an application to generate different apartment settings based on the Big Five personality model and investigated user perception and level of immersion in VR. The goal was to achieve a believable, immersive randomization of apartments interior for open world applications. The Big Five Model is a modern personality theory that defines five central personality traits: extraversion, agreeableness, openness, conscientiousness, and neuroticism. For each apartment layout, we simulated a personality by randomizing values of each of the five traits. We then calculated a series of derived traits from these base five traits and used these base and derived traits to influence the interior layout of the apartment. To test how much the personality-based interior layout system affected perceived personality, we set up a key-finding game and asked participants a series of questions about their perception of the apartment tenant after they found the keys on each floor. We found that participants’ perceptions of tenants’ personalities generally matched the original Big Five personality model used to generate the apartment layout with a higher rate than random chance.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Luis Obispo",
              "institution": "Cal Poly San Luis Obispo",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 67073
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Luis Obispo",
              "institution": "California Polytechnic State University, San Luis Obispo",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 67192
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Luis Obispo",
              "institution": "California Polytechnic State University San Luis Obispo",
              "dsl": "Computer Science"
            }
          ],
          "personId": 67029
        }
      ]
    },
    {
      "id": 67279,
      "typeId": 11924,
      "title": "Content-rich and Expansive Virtual Environments Using Passive Props As World Anchors",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489947"
        }
      },
      "trackId": 11344,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        67010
      ],
      "abstract": "In this paper, we present a system that allows developers to add passive haptic feedback into their virtual reality applications by making use of existing physical objects in the user's real environment. Our approach has minimal dependence on procedural generation and does not limit the virtual space to the dimensions of the physical play-area.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "Canterbury",
              "city": "Christchurch",
              "institution": "University of Canterbury",
              "dsl": "HIT Lab NZ"
            }
          ],
          "personId": 67074
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "Canterbury",
              "city": "Christchurch",
              "institution": "University of Canterbury",
              "dsl": "HIT Lab NZ"
            },
            {
              "country": "New Zealand",
              "state": "Canterbury",
              "city": "Christchurch",
              "institution": "University of Canterbury",
              "dsl": "School of Product Design"
            }
          ],
          "personId": 67219
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Christchurch",
              "institution": "University of Canterbury",
              "dsl": "HIT Lab NZ"
            }
          ],
          "personId": 67225
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "Brunel University London",
              "dsl": "College of Engineering, Design and Physical Sciences"
            }
          ],
          "personId": 67022
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Canterbury",
              "institution": "University of Kent",
              "dsl": ""
            }
          ],
          "personId": 67106
        }
      ]
    },
    {
      "id": 67280,
      "typeId": 11924,
      "title": "XRSpectator: Immersive, Augmented Sports Spectating",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489930"
        }
      },
      "trackId": 11344,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        67012
      ],
      "abstract": "In-stadium sports spectating delivers a unique social experience in a variety of sports. However, in contrast to broadcast delivery, it lacks the provision of real-time information augmentation, like game statistics overlaid on screen. In an earlier iteration, we developed ARSpectator, a prototypical, mobile system which can be brought to the stadium to experience both, the live sport action and situated infographics spatially augmented into the scene. In some situations it is difficult or often impossible to go to the stadium though, for instance because of limited stadium access during pandemics or when wanting to conduct controlled user studies. We address this by turning our ARSpectator system into an indirect augmented reality experience deployed to an immersive, virtual reality head-mounted display: The live stadium experience is delivered by way of a surrounding 360 video recording while maintaining and extending the provision of interactive, situated infographics. With our XRSpectator demo prototype presented here, users can have an ARSpectator experience of a rugby game in our local stadium. Download, install, enjoy, and give us feedback please!",
      "authors": [
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "Otago",
              "city": "Dunedin",
              "institution": "University of Otago",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 67145
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Dunedin",
              "institution": "University of Otago",
              "dsl": "Computer Science"
            }
          ],
          "personId": 67184
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "Otago",
              "city": "Dunedin",
              "institution": "University of Otago",
              "dsl": "Department of Information Science"
            }
          ],
          "personId": 67025
        }
      ]
    },
    {
      "id": 67281,
      "typeId": 11924,
      "title": "IMAGEimate - An End-to-End Pipeline to Create Realistic Animatable 3D Avatars from a Single Image Using Neural Networks",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489922"
        }
      },
      "trackId": 11344,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        67012
      ],
      "abstract": "Creating realistic 3D avatars lies at the center of building immersive virtual environments. To create plausible human avatars using the traditional techniques of 3D modelling, character rigging and animation is both time consuming and requires expertise with interactive 3D software tools. An efficient and robust alternative approach is to extract features from images and build an automated pipeline for virtual human creation. To foster such a pipeline, we identified and adapted neural-network based solutions to solve the main sub-problems in this domain - 1) Image to Mesh, 2) Mesh Rigging and Skinning, and 3) Re-posing mesh and applying animations.\r\n\r\nCurrent pipelines such as Tex-An Meshare not automated end-to-end and require manual intervention using web-based application like Mixamo in their intermediate stages. We present a novel automated pipeline, IMAGEimate, which takes an input RGB image and creates a 3D animatable character. The pipeline predicts implicit functions from the provided image which are converted to a polygonal mesh, then learns to fit parametric models to this mesh, which allows for animatable 3D characters to be generated from images.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "University of California, Santa Barbara",
              "dsl": "Computer Science"
            }
          ],
          "personId": 67126
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "University of California, Santa Barbara",
              "dsl": "Computer Science"
            }
          ],
          "personId": 67138
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "UCSB",
              "dsl": "Computer Science "
            }
          ],
          "personId": 66889
        }
      ]
    },
    {
      "id": 67282,
      "typeId": 11924,
      "title": "Exploring Emotion Brushes for a Virtual Reality Painting Tool",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489925"
        }
      },
      "trackId": 11344,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        67012
      ],
      "abstract": "We present emoPaint, a virtual reality application that allows users to create paintings with expressive emotion-based brushes and shapes. % with the range of visual elements. \r\n  While previous systems have introduced painting in 3D space, emoPaint focuses on supporting emotional characteristics by allowing users to create their own brushes and paint with the corresponding visual elements. Our system provides different line textures, shape representations and color palettes for each emotion. This enables users to control expression of emotions in their paintings. We describe our implementation and illustrate paintings created using emoPaint. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "UC Santa Barbara",
              "dsl": ""
            }
          ],
          "personId": 67103
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "UCSB",
              "dsl": "Computer Science "
            }
          ],
          "personId": 66889
        }
      ]
    },
    {
      "id": 67283,
      "typeId": 11924,
      "title": "An Evaluation of Methods for Manipulating Virtual Objects at Different Scales",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489907"
        }
      },
      "trackId": 11344,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        67010
      ],
      "abstract": "Immersive Virtual Reality enables users to experience 3D models and other virtual content in ways that cannot be achieved on a flat screen, and several modern Virtual Reality applications now give users the ability to include or create their own content and objects. With user-generated content however, objects may come in all shapes and sizes. This necessitates the use of object manipulation methods that are effective regardless of object size. The aim of this work is to evaluate two methods for manipulating virtual objects of varying sizes. World Pull enables the user to directly manipulate and scale the virtual environment, while Pivot Manipulation enables the user to rotate objects around a set of predefined pivot points. The methods were compared to a traditional 6 degree of freedom manipulation method during a user study and the results showed that World Pull performed better in terms of precision for small and large objects, while Pivot Manipulation performed better for large objects.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Aarhus",
              "institution": "SynergyXR ApS",
              "dsl": ""
            },
            {
              "country": "Denmark",
              "state": "",
              "city": "Aalborg",
              "institution": "Aalborg University",
              "dsl": "Department of Architecture, Design and Media Technology"
            }
          ],
          "personId": 67020
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Aarhus",
              "institution": "SynergyXR ApS",
              "dsl": ""
            }
          ],
          "personId": 67162
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "Norther Jutland",
              "city": "Aalborg",
              "institution": "Aalborg University",
              "dsl": "Department of Architecture, Design and Media Technology"
            }
          ],
          "personId": 67077
        }
      ]
    },
    {
      "id": 67284,
      "typeId": 11924,
      "title": "An Infant-Like Device that Reproduces Hugging Sensation with Multi-Channel Haptic Feedback",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489927"
        }
      },
      "trackId": 11344,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        67011
      ],
      "abstract": "Proximity interaction, such as hugging, plays an essential role in building relationships between parents and children. However, parents and children cannot freely interact in the neonatal intensive care unit due to visiting restrictions imposed by COVID-19. In this study, we develop a system of pseudo-proximity interaction with a remote infant through a VR headset by using an infant-like device that reproduces the haptic feedback features of the hugging sensation, such as weight, body temperature, breathing, softness, and unstable neck.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Osaka",
              "city": "Suita",
              "institution": "Osaka University",
              "dsl": ""
            }
          ],
          "personId": 67151
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Osaka",
              "city": "Suita",
              "institution": "Osaka University",
              "dsl": ""
            }
          ],
          "personId": 67124
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Miyagi",
              "city": "Sendai",
              "institution": "Tohoku University",
              "dsl": ""
            }
          ],
          "personId": 67042
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Osaka",
              "city": "Suita",
              "institution": "Osaka University",
              "dsl": ""
            }
          ],
          "personId": 67030
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Kanagawa",
              "city": "Sagamihara",
              "institution": "Aoyama Gakuin University",
              "dsl": ""
            }
          ],
          "personId": 67197
        }
      ]
    },
    {
      "id": 67285,
      "typeId": 11924,
      "title": "UGRA in VR: A Virtual Reality Simulation for Training Anaesthetists",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489924"
        }
      },
      "trackId": 11344,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        67011
      ],
      "abstract": "In this paper we present a virtual reality training simulator for medical interns practicing ultrasound-guided regional anaesthesia (UGRA). UGRA is a type of nerve block procedure performed commonly by critical care doctors such as anaesthetists, emergency medicine physicians, and paramedics. This procedure is complex and requires intense training. It is traditionally taught one-on-one by experts and is performed on simulated models long before attempting the procedure on live patients. We present our virtual reality application that allows for training this procedure in a simulated environment. The use of virtual reality makes training future doctors performing UGRA safer and more cost efficient than current approaches.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "NSW",
              "city": "Parramatta",
              "institution": "Western Sydney University",
              "dsl": "School of Computer, Data and Mathematical Sciences"
            }
          ],
          "personId": 67088
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "NSW",
              "city": "Liverpool",
              "institution": "Liverpool Hospital",
              "dsl": ""
            }
          ],
          "personId": 67152
        }
      ]
    },
    {
      "id": 67286,
      "typeId": 11924,
      "title": "HoloKeys: Interactive Piano Education Using Augmented Reality and IoT",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489921"
        }
      },
      "trackId": 11344,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        67012
      ],
      "abstract": "The rise of online learning poses unique challenges in music education, where live demonstration and musical synchronization are critical for student success. We present HoloKeys, a music education interface that allows instructors to play remotely located pianos using an augmented reality headset and wifi-enabled microcontrollers. This approach allows students to receive distance education which is more direct, immersive,and comprehensive than conventional video conferencing allows for. HoloKeys enables remote students to observe live instructional demonstration on a physical keyboard in their immediate environment just as they would in traditional settings. HoloKeys consists of two separate components: an augmented reality user interface and a piano playing apparatus. Our system aims to extend online music education beyond desktop platforms into the physical world, thereby addressing crucial obstacles encountered by educators and students transitioning into online education.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Gainesville",
              "institution": "Digital Worlds Institute",
              "dsl": "University of Florida"
            },
            {
              "country": "United States",
              "state": "Florida",
              "city": "Gainesville",
              "institution": "Digital Worlds Institute",
              "dsl": "University of Florida"
            }
          ],
          "personId": 67201
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Gainesville",
              "institution": "University of Florida",
              "dsl": "Digital Worlds Institute"
            },
            {
              "country": "United States",
              "state": "Florida",
              "city": "Gainesville",
              "institution": "University of Florida",
              "dsl": "Digital Worlds Institute"
            }
          ],
          "personId": 67157
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Gainesville",
              "institution": "University of Florida",
              "dsl": "Digital Worlds Institute"
            },
            {
              "country": "United States",
              "state": "Florida",
              "city": "Gainesville",
              "institution": "University of Florida",
              "dsl": "Digital Worlds Institute"
            }
          ],
          "personId": 67227
        }
      ]
    },
    {
      "id": 67287,
      "typeId": 11924,
      "title": "Multi-View AR Streams for Interactive 3D Remote Teaching",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489950"
        }
      },
      "trackId": 11344,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        67012
      ],
      "abstract": "In this work, we present a system that adds augmented reality interaction and 3D-space utilization to educational videoconferencing for a more engaging distance learning experience. We developed infrastructure and user interfaces that enable the use of an instructor’s physical 3D space as a teaching stage, promote student interaction, and take advantage of the flexibility of adding virtual content to the physical world. The system is implemented using hand-held mobile augmented reality to maximize device availability, scalability, and ready deployment, elevating traditional video lectures to immersive mixed reality experiences. We use multiple devices on the teacher's end to provide different simultaneous views of a teaching space towards a better understanding of the 3D space.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "University of California, Santa Barbara",
              "dsl": ""
            }
          ],
          "personId": 67153
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "University of California Santa Barbara",
              "dsl": "Media Arts and Technology"
            }
          ],
          "personId": 67128
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "UCSB",
              "dsl": "Computer Science "
            }
          ],
          "personId": 66889
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "University of California, Santa Barbara",
              "dsl": "Computer Science"
            }
          ],
          "personId": 67138
        }
      ]
    },
    {
      "id": 67288,
      "typeId": 11924,
      "title": "Preservation and Reproduction of Real Soundscapes in Virtual Space for the \"100 Best Soundscapes in Japan\"",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489902"
        }
      },
      "trackId": 11344,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        67012
      ],
      "abstract": "In this study, we developed a soundscape system that reproduces a real soundscape in 3D in a virtual space, based on the \"100 Best Soundscapes in Japan\" selected by the Ministry of the Environment. Using the game engine \"Unity\", we proposed a method of embedding recorded sound sources into virtual objects placed in a virtual space, and sequentially playing back the sound when the user walks or turns around in the virtual space. By using this system, it is possible to reproduce the same sound field in the virtual space as in the real space, and it can be applied in various places.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Chigasaki",
              "institution": "Bunkyo University",
              "dsl": "Department of Information Systems"
            }
          ],
          "personId": 67056
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Chigasaki",
              "institution": "Bunkyo University",
              "dsl": "Department of Information Systems"
            }
          ],
          "personId": 67110
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Kanagawa Prefecture",
              "city": "Chigasaki city",
              "institution": "Bunkyo University",
              "dsl": "Department of Information Systems,  Faculty of Information & Communications"
            }
          ],
          "personId": 67066
        }
      ]
    },
    {
      "id": 67289,
      "typeId": 11924,
      "title": "Designing Obstacle Reminder for Safe AR Navigation",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489905"
        }
      },
      "trackId": 11344,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        67011
      ],
      "abstract": "AR navigation has been widely used in mobile devices. However, users sometimes immerse in the navigation interface and ignore probable dangers in the real environment. It is necessary to remind users of potential dangers and avoid accidents. Most of existing works focus on how to effectively guide users in AR but few concern about design of danger reminder.\r\nIn this paper, we build a virtual AR navigation system and compare user experience on different types of obstacle reminder. Furthermore, we compare the influence of color, motion and appearance distance on effectiveness of the reminder. Results show that red color and bi-color is more obvious than blue color for reminder. Motion such as flickering helps enhance remind effectiveness.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "BeiJing",
              "institution": "Beihang University",
              "dsl": "School of New Media Art and Design"
            },
            {
              "country": "China",
              "state": "CN",
              "city": "BeiJing",
              "institution": "Beihang University",
              "dsl": "State Key Laboratory of Virtual Reality Technology and Systems"
            }
          ],
          "personId": 67137
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "School of New Media Art and Design",
              "dsl": ""
            }
          ],
          "personId": 67098
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "School of New Media Art and Design",
              "dsl": ""
            }
          ],
          "personId": 67122
        }
      ]
    },
    {
      "id": 67290,
      "typeId": 11924,
      "title": "Multi-Componential Analysis of Emotions Using Virtual Reality",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489958"
        }
      },
      "trackId": 11344,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        67011
      ],
      "abstract": "In this study, we propose our data-driven approach to investigate the emotional experience triggered using Virtual Reality (VR) games. We considered a full Component Process Model (CPM) which theorise emotional experience as a multi-process phenomenon. We validated the possibility of the proposed approach through a pilot experiment and confirmed that VR games can be used to trigger a diverse range of emotions. Using hierarchical clustering, we showed a clear distinction between positive and negative emotion in the CPM space.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "New South Wales",
              "city": "Sydney",
              "institution": "University of New South Wales",
              "dsl": "School of Computer Science and Engineering"
            }
          ],
          "personId": 67196
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "NSW",
              "city": "Sydney",
              "institution": "CSIRO and UNSW",
              "dsl": ""
            }
          ],
          "personId": 67104
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "NSW",
              "city": "Sydney",
              "institution": "UNSW",
              "dsl": "School of Computer Science & Engineering"
            }
          ],
          "personId": 67078
        }
      ]
    },
    {
      "id": 67291,
      "typeId": 11924,
      "title": "Effects of User's Gaze on the Unintended Positional Drift in Walk-in-Place",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489928"
        }
      },
      "trackId": 11344,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        67012
      ],
      "abstract": "Walk-In-Place (WIP) is a widely used technique in which users perform walking or jogging-like movements in a stationary place to move around in virtual environments (VEs). However, unintended positional drift (UPD) while performing WIP often occurs, thus weakening its benefits of keeping users in a fixed position in physical space. In this paper, we present our preliminary study exploring whether users' gaze while WIP affects the direction of the UPD. Participants of the study jogged in a VE five times. Each time, we manipulated their gaze direction by displaying visual information in 5 different locations in their view. Although a correlation between the gaze and UPD direction was not found, we report the results we observed from this study, including the amount of observed drift, preferred location of visual information, and the potential of WIP on VR sports.  ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Busan",
              "institution": "Pusan National University",
              "dsl": ""
            }
          ],
          "personId": 67230
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Busan",
              "institution": "Pusan National University",
              "dsl": ""
            }
          ],
          "personId": 67093
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Busan",
              "institution": "Pusan National University",
              "dsl": ""
            }
          ],
          "personId": 67147
        }
      ]
    },
    {
      "id": 67292,
      "typeId": 11924,
      "title": "A sharing system for the annoyance of menstrual symptoms using electrical muscle stimulation and thermal stimulations",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489937"
        }
      },
      "trackId": 11344,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        67012
      ],
      "abstract": "Menstrual symptoms and the discomfort associated with menstrual symptoms are difficult to verbalize and communicate to others. Thus, we created a virtual reality (VR) system that reproduces menstrual symptoms to solve this problem. Through electrical stimulation, this system reproduces menstrual cramps in the abdomen of users during their daily activities. A warm sensation is presented using a Peltier device to reproduce the sensation of menstrual bleeding. In addition, electrical stimulation and warm sensations are presented to reproduce the discomfort of behavioral restriction based on the posture of the user obtained from a posture detection sensor. Our system allows users to experience not only the physical annoyance caused by menstrual symptoms, but also the mental annoyance due to the restriction of their daily activities by menstrual symptoms. Therefore, this system can be effectively used for a demonstration to understand the annoyance caused by menstrual symptoms comprehensively, and  helps eliminate the perception of menstrual symptoms as “inexpressible, subjective, and difficult to communicate.” ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Nara",
              "institution": "Nara Women's University",
              "dsl": "Cooperative Major in Human Centered Engineering"
            }
          ],
          "personId": 67115
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Nara",
              "institution": "Nara Women's University",
              "dsl": "Cooperative Major in Human Centered Engineering"
            }
          ],
          "personId": 67090
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Hyogo",
              "institution": "Konan University",
              "dsl": ""
            }
          ],
          "personId": 67087
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Ishikawa",
              "institution": "Japan Advanced Institute of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 67091
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Osaka",
              "institution": "Osaka University",
              "dsl": ""
            }
          ],
          "personId": 67229
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Hyogo",
              "institution": "University of Hyogo",
              "dsl": ""
            }
          ],
          "personId": 67134
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Nara",
              "city": "Nara",
              "institution": "Nara Women's University",
              "dsl": ""
            }
          ],
          "personId": 67160
        }
      ]
    },
    {
      "id": 67294,
      "typeId": 11924,
      "title": "SpArc: A VR Animating Tool at Your Fingertips",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489920"
        }
      },
      "trackId": 11344,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        67012
      ],
      "abstract": "3D animation is becoming a popular form of storytelling in many fields, bringing life to games, films, and advertising. However, the complexity of conventional 3D animation software, such as Maya and Blender, poses steep learning curves for novices who want to explore and learn 3D animation. Our work aims to lower such barriers by creating a simple yet immersive interface that users can easily interact with. Based on the focus-group interviews, we identified key functionalities in animation workflows. The resulting tool, SpArc, allows users to dive into animating without complex rigging and skinning process or learning multiple menu interactions. SpArc is designed for two-handed setups. Instead of using a conventional horizontal slider, we designed a radial time slider to reduce possible arm fatigue and enhance the accuracy of keyframe selection. The demo will showcase this interactive 3D animation tool.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Gainesville",
              "institution": "University of Florida",
              "dsl": ""
            }
          ],
          "personId": 67060
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Gainesville",
              "institution": "University of Florida",
              "dsl": "Digital Worlds Institute"
            },
            {
              "country": "United States",
              "state": "Florida",
              "city": "Gainesville",
              "institution": "University of Florida",
              "dsl": "Digital Worlds Institute"
            }
          ],
          "personId": 67157
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Gainesville",
              "institution": "Digital Worlds Institute",
              "dsl": "University of Florida"
            }
          ],
          "personId": 67085
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Gainesville",
              "institution": "Digital Worlds Institute",
              "dsl": "University of Florida"
            }
          ],
          "personId": 67070
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Gainesville",
              "institution": "University of Florida",
              "dsl": "Digital Worlds Institute"
            },
            {
              "country": "United States",
              "state": "Florida",
              "city": "Gainesville",
              "institution": "University of Florida",
              "dsl": "Digital Worlds Institute"
            }
          ],
          "personId": 67227
        }
      ]
    },
    {
      "id": 67295,
      "typeId": 11924,
      "title": "Double-Layered Cup-Shaped Device to Amplify Taste Sensation of Carbonation by the Electrical Stimulation on the Human Tongue",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489904"
        }
      },
      "trackId": 11344,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        67012
      ],
      "abstract": "It has been suggested that the electrical stimulation on the human tongue amplifies taste sensation of carbonated beverages. In this study, we describe an electric taste system for demonstrating to amplify taste sensation of carbonated beverages. In the demonstration of electric taste, we have to take care of hygiene because we use devices that touch the mouth. It is also necessary to implement device that are inexpensive and easy to manufacture so that many people can experience it. We propose a cup-shaped device with a double-layer structure that solves these problems.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Hosei University",
              "dsl": ""
            }
          ],
          "personId": 67223
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Hosei University",
              "dsl": ""
            }
          ],
          "personId": 67135
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Hosei University",
              "dsl": ""
            }
          ],
          "personId": 67121
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Hosei University",
              "dsl": ""
            }
          ],
          "personId": 67061
        }
      ]
    },
    {
      "id": 67296,
      "typeId": 11924,
      "title": "Dealing with a Panic Attack: a Virtual Reality Training Module for Postgraduate Psychology Students",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489926"
        }
      },
      "trackId": 11344,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        67011
      ],
      "abstract": "In this paper we present a virtual reality training simulator for postgraduate psychology students. This simulator features an interaction between the clinical psychologist (student) and a patient (virtual agent) suffering from Obsessive Compulsive Disorder (OCD). Our simulation focuses on the form of OCD treatment called ``Exposure Therapy''. The traditional way of learning how to perform Exposure Therapy (ET) currently involves watching video recordings and discussing those in the class. In our simulation we conduct an immersive exposure therapy session in VR. This session involves a live interaction with a patient that at one stage triggers a panic attack. Our hypothesis is that the immersive nature of the training session will affect the decision making process of the students so that they are more likely to cease the exposure task than those student participating in a less immersive form of learning (watching a video recording). We also hypothesise that participating in an immersive VR training session is more effective than watching videos as far as information retention goes.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "NSW",
              "city": "Parramatta",
              "institution": "Western Sydney University",
              "dsl": "School of Computer, Data and Mathematical Sciences"
            }
          ],
          "personId": 67088
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "NSW",
              "city": "Bankstown",
              "institution": "Western Sydney University",
              "dsl": "MARCS Institute"
            }
          ],
          "personId": 67212
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "NSW",
              "city": "Ultimo",
              "institution": "University of Technology Sydney",
              "dsl": ""
            }
          ],
          "personId": 67194
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "NSW",
              "city": "Parramatta",
              "institution": "Western Sydney University",
              "dsl": "School of Computer, Data and Mathematical Sciences"
            }
          ],
          "personId": 67177
        }
      ]
    },
    {
      "id": 67297,
      "typeId": 11924,
      "title": "Preliminary analysis of visual cognition estimation in VR toward effective assistance timing for iterative visual search tasks",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489954"
        }
      },
      "trackId": 11344,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        67012
      ],
      "abstract": "This research aims to develop a method to assist iterative visual search tasks, and it focuses on visual cognition to achieve effective assistance. As a first step to this goal, we analyzed the participants' gaze behaviors when they visually recognized a target in a VR environment. In the experiment, the effect of visual cognition difficulty (VCD) is considered. Analysis results show that the participants could visually recognize lower-VCD targets at an earlier timing. This suggests that VCD-based guidance may improve task performance.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Hyogo",
              "city": "Himeji",
              "institution": "University of Hyogo",
              "dsl": ""
            }
          ],
          "personId": 67050
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "2-2-2 Hikaridai Seika-cho, Kyoto",
              "institution": "ATR",
              "dsl": "ISL Labs."
            },
            {
              "country": "Japan",
              "state": "",
              "city": "525-8577 Kusatsu, Shiga",
              "institution": "Ritsumeikan University",
              "dsl": "AIS Labs."
            }
          ],
          "personId": 67232
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Kyoto",
              "city": "2-2-2 Hikaridai Seika-cho",
              "institution": "ATR",
              "dsl": "ISL Labs."
            }
          ],
          "personId": 67207
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Hyogo",
              "city": "Himeji",
              "institution": "University of Hyogo",
              "dsl": ""
            }
          ],
          "personId": 67211
        }
      ]
    },
    {
      "id": 67298,
      "typeId": 11924,
      "title": "A Perceptual Evaluation of the Ground Inclination with a Simple VR Walking Platform",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3489849.3489903"
        }
      },
      "trackId": 11344,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        67010
      ],
      "abstract": "We evaluate how highly realistic the inclination of the ground can be perceived with our simple VR walking platform. Firstly we prepared seven maps with different ground inclinations of -30 to 30 degrees and every 10 degrees. Then we con-ducted a perception experiment of the inclination feeling with each of the treadmill and our proposed platform, and questionnaire evaluation about the presence, the fatigue, and the exhilaration. As a result, it was clarified that even if our proposed platform is used, not only the feeling of presence equivalent to that of the treadmill can be felt, but also the inclination of the ground up and down can be perceived.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Hiroshima",
              "city": "Hiroshima",
              "institution": "Hiroshima City University",
              "dsl": "Graduate School of Information Sciences"
            }
          ],
          "personId": 67158
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Hiroshima",
              "city": "Hiroshima",
              "institution": "Hiroshima City University",
              "dsl": "Graduate School of Information Sciences"
            }
          ],
          "personId": 67172
        }
      ]
    },
    {
      "id": 67306,
      "typeId": 11924,
      "title": "Headspin Experience (invited from IVRC2021)",
      "trackId": 11344,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        67012
      ],
      "abstract": "",
      "authors": []
    },
    {
      "id": 67310,
      "typeId": 11925,
      "title": "Walk-up-and-use experiential interactions in Mixed-Reality",
      "trackId": 11351,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        67007
      ],
      "abstract": "My research is driven by a vision to deliver novel multi-sensory experiences to users without instrumenting them with wearable or head-mounted displays. In the last decade my team has been exploring ways to create new forms of walk-up-and-use mixed-reality interfaces for immersive visual, tactile, olfactory, and gustatory experiences. As one example, we explored the use of fog screens and soap bubbles to create mid-air displays that provide visual and olfactory feedback to the user. As another example, we built ultrasonic speaker arrays to suspend tiny objects in the acoustic field and manipulate them to create persistence of vision displays. We do this by computing acoustic phase-only holograms that are delivered using the speakers to shape the wavefront. Similarly, we create tactile sensations by focusing the waves on the palm of the user. All these examples create sensation in mid-air - so users do not have to touch or hold any device to experience it.  This talk will cover our recent efforts on these topics and hopefully outline a vision for mixed-reality that embraces more modalities while encouraging walk-up-and-use interactions.",
      "authors": []
    }
  ],
  "people": [
    {
      "id": 66757,
      "firstName": "Pascal",
      "lastName": "Chiu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66758,
      "firstName": "Kathleen",
      "lastName": "Feeney",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66759,
      "firstName": "Dinh Tung",
      "lastName": "Le",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66760,
      "firstName": "Jarrod",
      "lastName": "Knibbe",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66761,
      "firstName": "Aslı",
      "lastName": "Günay",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66762,
      "firstName": "Tonja-Katrin",
      "lastName": "Machulla",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66763,
      "firstName": "Richard",
      "lastName": "Noeske",
      "middleInitial": "Huynh",
      "affiliations": []
    },
    {
      "id": 66764,
      "firstName": "Florian",
      "lastName": "Lang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66765,
      "firstName": "Yatharth",
      "lastName": "Singhal",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66766,
      "firstName": "Géry",
      "lastName": "Casiez",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66767,
      "firstName": "Michael",
      "lastName": "McGuffin",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66768,
      "firstName": "Erica",
      "lastName": "Musser",
      "middleInitial": "D.",
      "affiliations": []
    },
    {
      "id": 66769,
      "firstName": "Bruce",
      "lastName": "Thomas",
      "middleInitial": "H",
      "affiliations": []
    },
    {
      "id": 66770,
      "firstName": "Felix",
      "lastName": "Thiel",
      "middleInitial": "Johannes",
      "affiliations": []
    },
    {
      "id": 66771,
      "firstName": "Kemal",
      "lastName": "Kuscu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66772,
      "firstName": "Baptiste",
      "lastName": "Gauthier",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66773,
      "firstName": "Susanne",
      "lastName": "Schmidt",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66774,
      "firstName": "Louis",
      "lastName": "Albert",
      "middleInitial": "Philippe",
      "affiliations": []
    },
    {
      "id": 66775,
      "firstName": "Daniele",
      "lastName": "Giunchi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66776,
      "firstName": "Jeremy",
      "lastName": "Cooperstock",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66777,
      "firstName": "Terry",
      "lastName": "Eskenazi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66778,
      "firstName": "Jan",
      "lastName": "Egger",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66779,
      "firstName": "Leila",
      "lastName": "Takayama",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66780,
      "firstName": "Alexandre",
      "lastName": "Gomes de Siqueira",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66781,
      "firstName": "Martin",
      "lastName": "HACHET",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66782,
      "firstName": "Steven H. D.",
      "lastName": "Haddock",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66783,
      "firstName": "Robert",
      "lastName": "Xiao",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66784,
      "firstName": "Thibault",
      "lastName": "Friedrich",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66785,
      "firstName": "Uwe",
      "lastName": "Gruenefeld",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66786,
      "firstName": "Jiwan",
      "lastName": "Lee",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66787,
      "firstName": "Zhu",
      "lastName": "Wang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66788,
      "firstName": "Eduard",
      "lastName": "Gröller",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66789,
      "firstName": "Gavin",
      "lastName": "Paul",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66790,
      "firstName": "Antoine",
      "lastName": "Weill-Duflos",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66791,
      "firstName": "Benjamin",
      "lastName": "Hughes",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66792,
      "firstName": "Natalia",
      "lastName": "Bartłomiejczyk",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66793,
      "firstName": "Tor-Salve",
      "lastName": "Dalsgaard",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66794,
      "firstName": "Frank",
      "lastName": "Steinicke",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66795,
      "firstName": "Ali",
      "lastName": "Vatansever",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66796,
      "firstName": "Alexander",
      "lastName": "Boden",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66797,
      "firstName": "Marc Erich",
      "lastName": "Latoschik",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66798,
      "firstName": "Krzysztof",
      "lastName": "Grudzień",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66799,
      "firstName": "Jonathan",
      "lastName": "Liebers",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66800,
      "firstName": "Jiawei",
      "lastName": "Huang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66801,
      "firstName": "Julia",
      "lastName": "Dominiak",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66802,
      "firstName": "Chi",
      "lastName": "Guo",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66803,
      "firstName": "Ben",
      "lastName": "Congdon",
      "middleInitial": "J",
      "affiliations": []
    },
    {
      "id": 66804,
      "firstName": "Jaime",
      "lastName": "Maldonado",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66805,
      "firstName": "Sarah",
      "lastName": "Bloch-Elkouby",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66806,
      "firstName": "Sinem",
      "lastName": "Semsioglu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66807,
      "firstName": "Ruth",
      "lastName": "Gibson",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66808,
      "firstName": "Everardo",
      "lastName": "Gonzalez",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66809,
      "firstName": "Anokhi",
      "lastName": "Bafna",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66810,
      "firstName": "Bowen",
      "lastName": "Zhang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66811,
      "firstName": "Heng",
      "lastName": "Yao",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66812,
      "firstName": "Marco",
      "lastName": "Gillies",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66813,
      "firstName": "Eric",
      "lastName": "Whitmire",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66814,
      "firstName": "Ken",
      "lastName": "Perlin",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66815,
      "firstName": "Stuart",
      "lastName": "James",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66816,
      "firstName": "Vanessa",
      "lastName": "Cobus",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66817,
      "firstName": "Anat",
      "lastName": "Lubetzky",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66818,
      "firstName": "Christoph",
      "lastName": "Zetzsche",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66819,
      "firstName": "Lucie",
      "lastName": "Kruse",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66820,
      "firstName": "Kakani",
      "lastName": "Katija",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66821,
      "firstName": "Christoph",
      "lastName": "Heinzl",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66822,
      "firstName": "Masatoshi",
      "lastName": "Ishikawa",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66823,
      "firstName": "Jaejun",
      "lastName": "Park",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66824,
      "firstName": "Bruno",
      "lastName": "Herbelin",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66825,
      "firstName": "Simon",
      "lastName": "Kimmel",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66826,
      "firstName": "Sven",
      "lastName": "Mayer",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66827,
      "firstName": "Nicola",
      "lastName": "Plant",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66828,
      "firstName": "Alastair",
      "lastName": "Shipman",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66829,
      "firstName": "Yuki",
      "lastName": "Shimomura",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66830,
      "firstName": "Hirohito",
      "lastName": "Sato",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66831,
      "firstName": "Simone",
      "lastName": "Kühn",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66832,
      "firstName": "Yaxuan",
      "lastName": "Li",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66833,
      "firstName": "Piotr",
      "lastName": "Sikorski",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66834,
      "firstName": "Peilin",
      "lastName": "Yu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66835,
      "firstName": "Fotis",
      "lastName": "Liarokapis",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66836,
      "firstName": "Jianning",
      "lastName": "Li",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66837,
      "firstName": "Johanna",
      "lastName": "Pirker",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66838,
      "firstName": "Christian",
      "lastName": "Burschik",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66839,
      "firstName": "Chris",
      "lastName": "Creed",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66840,
      "firstName": "Marc",
      "lastName": "Baloup",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66841,
      "firstName": "Saeed",
      "lastName": "Safikhani",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66842,
      "firstName": "Jack",
      "lastName": "Ratcliffe",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66843,
      "firstName": "Laurissa",
      "lastName": "Tokarchuk",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66844,
      "firstName": "Magdalena",
      "lastName": "Wróbel-Lachowska",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66845,
      "firstName": "Jenny",
      "lastName": "Gabel",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66846,
      "firstName": "Pauline",
      "lastName": "Bimberg",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66847,
      "firstName": "Ayush",
      "lastName": "Bhardwaj",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66848,
      "firstName": "Lisa",
      "lastName": "Izzouzi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66849,
      "firstName": "Wilko",
      "lastName": "Heuten",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66850,
      "firstName": "Tomohiro",
      "lastName": "Sueishi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66851,
      "firstName": "Sebastian",
      "lastName": "Friston",
      "middleInitial": "J",
      "affiliations": []
    },
    {
      "id": 66852,
      "firstName": "Beomsu",
      "lastName": "Lim",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66853,
      "firstName": "Andreea Dalia",
      "lastName": "Blaga",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66854,
      "firstName": "Lior",
      "lastName": "Somin",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66855,
      "firstName": "Benjamin",
      "lastName": "Lok",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66856,
      "firstName": "Asim Evren",
      "lastName": "Yantac",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66857,
      "firstName": "Onur",
      "lastName": "Gurkan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66858,
      "firstName": "Nelly",
      "lastName": "Klassen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66859,
      "firstName": "Mengyu",
      "lastName": "Chen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66860,
      "firstName": "Dac Dang Khoa",
      "lastName": "Nguyen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66861,
      "firstName": "Ken",
      "lastName": "Sugimori",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66862,
      "firstName": "Dieter",
      "lastName": "Schmalstieg",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66863,
      "firstName": "Judith",
      "lastName": "Hartfill",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66864,
      "firstName": "Alison",
      "lastName": "Crosby",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66865,
      "firstName": "Lauren",
      "lastName": "Lloveras",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66866,
      "firstName": "Andrzej",
      "lastName": "Romanowski",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66867,
      "firstName": "Olaf",
      "lastName": "Blanke",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66868,
      "firstName": "Sheree May",
      "lastName": "Saßmannshausen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66869,
      "firstName": "Roberto",
      "lastName": "Martuzzi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66870,
      "firstName": "Arnulph",
      "lastName": "Fuhrmann",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66871,
      "firstName": "Aakar",
      "lastName": "Gupta",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66872,
      "firstName": "Tim",
      "lastName": "Weissker",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66873,
      "firstName": "James",
      "lastName": "Walsh",
      "middleInitial": "A",
      "affiliations": []
    },
    {
      "id": 66874,
      "firstName": "Kevin",
      "lastName": "Riebandt",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66875,
      "firstName": "Thomas",
      "lastName": "Pietrzak",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66876,
      "firstName": "Jonas",
      "lastName": "Auda",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66877,
      "firstName": "Anthony",
      "lastName": "Steed",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66878,
      "firstName": "Alexander",
      "lastName": "Gall",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66879,
      "firstName": "A. Ege",
      "lastName": "Unlu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66880,
      "firstName": "Gizem",
      "lastName": "Erdem",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66881,
      "firstName": "Benjamin",
      "lastName": "Erwin",
      "middleInitial": "Eric",
      "affiliations": []
    },
    {
      "id": 66882,
      "firstName": "Aviv",
      "lastName": "Elor",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66883,
      "firstName": "Yoshifumi",
      "lastName": "Kitamura",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66884,
      "firstName": "Tiffany",
      "lastName": "Thang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66885,
      "firstName": "Thanh",
      "lastName": "Vu",
      "middleInitial": "Long",
      "affiliations": []
    },
    {
      "id": 66886,
      "firstName": "Yongjae",
      "lastName": "Yoo",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66887,
      "firstName": "Clarice",
      "lastName": "Hilton",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66888,
      "firstName": "Maite",
      "lastName": "Frutos-Pascual",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66889,
      "firstName": "Misha",
      "lastName": "Sra",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66890,
      "firstName": "Bernd",
      "lastName": "Froehlich",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66891,
      "firstName": "Sheila",
      "lastName": "Sutjipto",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66892,
      "firstName": "Michael",
      "lastName": "Holly",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66893,
      "firstName": "Amy",
      "lastName": "Phung",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66894,
      "firstName": "Shoichi",
      "lastName": "Hasegawa",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66895,
      "firstName": "Phoenix",
      "lastName": "Perry",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66896,
      "firstName": "Christina",
      "lastName": "Gsaxner",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66897,
      "firstName": "Zachary",
      "lastName": "McKendrick",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66898,
      "firstName": "Nils",
      "lastName": "Verheyen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66899,
      "firstName": "Yuki",
      "lastName": "Ban",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66900,
      "firstName": "Riccardo",
      "lastName": "Bovo",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66901,
      "firstName": "Bruno",
      "lastName": "Martelli",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66902,
      "firstName": "Jin Ryong",
      "lastName": "Kim",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66903,
      "firstName": "Arnaud",
      "lastName": "Prouzeau",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66904,
      "firstName": "Mikołaj",
      "lastName": "Woźniak",
      "middleInitial": "P.",
      "affiliations": []
    },
    {
      "id": 66905,
      "firstName": "Marko",
      "lastName": "Peljhan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66906,
      "firstName": "Hrvoje",
      "lastName": "Benko",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66907,
      "firstName": "James",
      "lastName": "Baumeister",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66908,
      "firstName": "Florian",
      "lastName": "Jasche",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66909,
      "firstName": "David",
      "lastName": "Swapp",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66910,
      "firstName": "Hyunjae",
      "lastName": "Gil",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66911,
      "firstName": "Stefan",
      "lastName": "Schneegass",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66912,
      "firstName": "yang",
      "lastName": "Liu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66913,
      "firstName": "Shinichi",
      "lastName": "Warisawa",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66914,
      "firstName": "Patrick",
      "lastName": "Horn",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66915,
      "firstName": "Saliha",
      "lastName": "Akbas",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66916,
      "firstName": "Alexander",
      "lastName": "Kainz",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66917,
      "firstName": "Michael",
      "lastName": "Zbyszynski",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66918,
      "firstName": "Eric",
      "lastName": "Martin",
      "middleInitial": "J.",
      "affiliations": []
    },
    {
      "id": 66919,
      "firstName": "Kensho",
      "lastName": "Oguri",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66920,
      "firstName": "Jenelle",
      "lastName": "Richards",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66921,
      "firstName": "Hironori",
      "lastName": "Mitake",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66922,
      "firstName": "Ehud",
      "lastName": "Sharlin",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66923,
      "firstName": "Farshid",
      "lastName": "Salemi Parizi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66924,
      "firstName": "Panayiotis",
      "lastName": "Charalambous",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66925,
      "firstName": "Rebecca",
      "lastName": "Fiebrink",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66926,
      "firstName": "Joanna",
      "lastName": "Bergström",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66927,
      "firstName": "Junghoon",
      "lastName": "Chae",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66928,
      "firstName": "Stephanie",
      "lastName": "Morris",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66929,
      "firstName": "Veronika",
      "lastName": "Krauß",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66930,
      "firstName": "Patrick",
      "lastName": "Finn",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66931,
      "firstName": "Carlos",
      "lastName": "González Díaz",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66932,
      "firstName": "Seungmoon",
      "lastName": "Choi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66933,
      "firstName": "Martin",
      "lastName": "Misiak",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66934,
      "firstName": "Dan",
      "lastName": "Archer",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66935,
      "firstName": "Thomas",
      "lastName": "Ludwig",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66936,
      "firstName": "Antonio",
      "lastName": "Pepe",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66937,
      "firstName": "Otto",
      "lastName": "Olkkonen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66938,
      "firstName": "Igor",
      "lastName": "Galynker",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66939,
      "firstName": "Nick",
      "lastName": "Ballou",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66940,
      "firstName": "Huyin",
      "lastName": "Zhang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66941,
      "firstName": "Haokun",
      "lastName": "Wang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66942,
      "firstName": "Sangyoon",
      "lastName": "Han",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66943,
      "firstName": "Liraz",
      "lastName": "Arie",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66944,
      "firstName": "Alexander",
      "lastName": "Kulik",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66945,
      "firstName": "Sebastian",
      "lastName": "Weiß",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66946,
      "firstName": "Klara",
      "lastName": "Brandstätter",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66947,
      "firstName": "Thomas",
      "lastName": "Heinis",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66948,
      "firstName": "Wolf",
      "lastName": "Kienzle",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67006,
      "firstName": "Sriram",
      "lastName": "Subramanian",
      "affiliations": [
        {
          "country": "United Kingdom",
          "institution": "University College London"
        }
      ]
    },
    {
      "id": 67013,
      "firstName": "Ian",
      "lastName": "Williams",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67014,
      "firstName": "Nadia",
      "lastName": "Fereydooni",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67015,
      "firstName": "Kazuma",
      "lastName": "Nagata",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67016,
      "firstName": "Patric",
      "lastName": "Ljung",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67017,
      "firstName": "Zhuo",
      "lastName": "YANG",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67018,
      "firstName": "David",
      "lastName": "Phoenix",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67019,
      "firstName": "Kyohei",
      "lastName": "Masuko",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67020,
      "firstName": "Jesper",
      "lastName": "Gaarsdal",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67021,
      "firstName": "Sabin",
      "lastName": "Tabirca",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67022,
      "firstName": "George",
      "lastName": "Ghinea",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67023,
      "firstName": "Thore",
      "lastName": "Keser",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67024,
      "firstName": "Nilotpal",
      "lastName": "Biswas",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67025,
      "firstName": "Holger",
      "lastName": "Regenbrecht",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67026,
      "firstName": "Murthy",
      "lastName": "L R D",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67027,
      "firstName": "Vishal",
      "lastName": "Jangid",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67028,
      "firstName": "Tom",
      "lastName": "De Weyer",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67029,
      "firstName": "Christian",
      "lastName": "Eckhardt",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67030,
      "firstName": "Takao",
      "lastName": "Onoye",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67031,
      "firstName": "Jingjing",
      "lastName": "Kang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67032,
      "firstName": "Kota",
      "lastName": "Arai",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67033,
      "firstName": "Sung-Uk",
      "lastName": "Jung",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67034,
      "firstName": "Oliver",
      "lastName": "Happel",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67035,
      "firstName": "Nuno",
      "lastName": "Correia",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67036,
      "firstName": "Matthias",
      "lastName": "Popp",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67037,
      "firstName": "Motoya",
      "lastName": "Izuhara",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67038,
      "firstName": "Yuhan",
      "lastName": "Dong",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67039,
      "firstName": "Vincent",
      "lastName": "Havard",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67040,
      "firstName": "Takashi",
      "lastName": "Ijiri",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67041,
      "firstName": "Shuxia",
      "lastName": "Wang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67042,
      "firstName": "Kazuyuki",
      "lastName": "Fujita",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67043,
      "firstName": "Yann",
      "lastName": "Glémarec",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67044,
      "firstName": "Gun",
      "lastName": "Lee",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67045,
      "firstName": "Cédric",
      "lastName": "Buche",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67046,
      "firstName": "Pradipta",
      "lastName": "Biswas",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67047,
      "firstName": "Tanja",
      "lastName": "Blascheck",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67048,
      "firstName": "David",
      "lastName": "Fernes",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67049,
      "firstName": "Weiping",
      "lastName": "He",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67050,
      "firstName": "Syunsuke",
      "lastName": "Yoshida",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67051,
      "firstName": "Bram",
      "lastName": "van Deurzen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67052,
      "firstName": "Riku",
      "lastName": "Fukuyama",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67053,
      "firstName": "Vali",
      "lastName": "Lalioti",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67054,
      "firstName": "Thomas",
      "lastName": "Neteler",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67055,
      "firstName": "Bogdan",
      "lastName": "Wiszniewski",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67056,
      "firstName": "Yoshihiko",
      "lastName": "Okubo",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67057,
      "firstName": "B.",
      "lastName": "Wohl",
      "middleInitial": "S.",
      "affiliations": []
    },
    {
      "id": 67058,
      "firstName": "Mick",
      "lastName": "Grierson",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67059,
      "firstName": "Jacek",
      "lastName": "Lebiedz",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67060,
      "firstName": "Bingyu",
      "lastName": "Li",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67061,
      "firstName": "Takafumi",
      "lastName": "Koike",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67062,
      "firstName": "Hugo",
      "lastName": "Lino",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67063,
      "firstName": "Kris",
      "lastName": "Luyten",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67064,
      "firstName": "Roisin",
      "lastName": "Hunt",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67065,
      "firstName": "Sangheon",
      "lastName": "Park",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67066,
      "firstName": "Yasuo",
      "lastName": "Kawai",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67067,
      "firstName": "Haruna",
      "lastName": "Miyakawa",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67068,
      "firstName": "Hendrik",
      "lastName": "Knoche",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67069,
      "firstName": "Sirisilp",
      "lastName": "Kongsilp",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67070,
      "firstName": "Maria",
      "lastName": "Blokhina",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67071,
      "firstName": "Florian",
      "lastName": "Kern",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67072,
      "firstName": "Tomoyuki",
      "lastName": "Nagami",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67073,
      "firstName": "Miles",
      "lastName": "Aikens",
      "middleInitial": "S",
      "affiliations": []
    },
    {
      "id": 67074,
      "firstName": "Steven G.",
      "lastName": "Wheeler",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67075,
      "firstName": "Shouxia",
      "lastName": "Wang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67076,
      "firstName": "Will",
      "lastName": "Kitchen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67077,
      "firstName": "Claus",
      "lastName": "Madsen",
      "middleInitial": "B.",
      "affiliations": []
    },
    {
      "id": 67078,
      "firstName": "Gelareh",
      "lastName": "Mohammadi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67079,
      "firstName": "Jean-Luc",
      "lastName": "Lugrin",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67080,
      "firstName": "Qinglan",
      "lastName": "Shan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67081,
      "firstName": "Keiichi",
      "lastName": "Zempo",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67082,
      "firstName": "Peiwu",
      "lastName": "Qin",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67083,
      "firstName": "Ryan",
      "lastName": "Skull",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67084,
      "firstName": "Barrett",
      "lastName": "Ens",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67085,
      "firstName": "Linda",
      "lastName": "Kirova",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67086,
      "firstName": "Masahiro",
      "lastName": "Shimizu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67087,
      "firstName": "Tamura",
      "lastName": "Yuichi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67088,
      "firstName": "Anton",
      "lastName": "Bogdanovych",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67089,
      "firstName": "Rui",
      "lastName": "Nóbrega",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67090,
      "firstName": "Kotori",
      "lastName": "Tsutsumi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67091,
      "firstName": "Wataru",
      "lastName": "Omori",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67092,
      "firstName": "Allan",
      "lastName": "Schjørring",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67093,
      "firstName": "Hyeong-geon",
      "lastName": "Kim",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67094,
      "firstName": "Tatsuki",
      "lastName": "Takano",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67095,
      "firstName": "Johan Winther",
      "lastName": "Kristensen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67096,
      "firstName": "Hsiang-Ting",
      "lastName": "Chen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67097,
      "firstName": "Sana",
      "lastName": "Behnam-Asl",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67098,
      "firstName": "Xinyi",
      "lastName": "Su",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67099,
      "firstName": "Sophia",
      "lastName": "Ppali",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67100,
      "firstName": "Jie",
      "lastName": "Xu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67101,
      "firstName": "Payod",
      "lastName": "Panda",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67102,
      "firstName": "Junichi",
      "lastName": "Hoshino",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67103,
      "firstName": "Jungah",
      "lastName": "Son",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67104,
      "firstName": "Tomasz",
      "lastName": "Bednarz",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67105,
      "firstName": "Mone",
      "lastName": "Konno",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67106,
      "firstName": "Alexandra",
      "lastName": "Covaci",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67107,
      "firstName": "Chanho",
      "lastName": "Park",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67108,
      "firstName": "Joel",
      "lastName": "Teixeira",
      "middleInitial": "Anthony",
      "affiliations": []
    },
    {
      "id": 67109,
      "firstName": "Samit",
      "lastName": "Bhattacharya",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67110,
      "firstName": "Yuki",
      "lastName": "Oishi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67111,
      "firstName": "Takayoshi",
      "lastName": "Yamada",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67112,
      "firstName": "Lei",
      "lastName": "Shao",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67113,
      "firstName": "Debangshu",
      "lastName": "Banerjee",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67114,
      "firstName": "Lucas",
      "lastName": "Plabst",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67115,
      "firstName": "Chihiro",
      "lastName": "Asada",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67116,
      "firstName": "Hiroki",
      "lastName": "Uchida",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67117,
      "firstName": "Howe",
      "lastName": "Zhu",
      "middleInitial": "Yuan",
      "affiliations": []
    },
    {
      "id": 67118,
      "firstName": "Chee Siang",
      "lastName": "Ang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67119,
      "firstName": "Yaqi",
      "lastName": "Xie",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67120,
      "firstName": "Hikaru",
      "lastName": "Nagasaka",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67121,
      "firstName": "sousuke",
      "lastName": "nakamura",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67122,
      "firstName": "Xuechen",
      "lastName": "Zhao",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67123,
      "firstName": "Nguyen Thanh Trung",
      "lastName": "Le",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67124,
      "firstName": "Yasuji",
      "lastName": "Kitabatake",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67125,
      "firstName": "Umut",
      "lastName": "Dinç",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67126,
      "firstName": "Suriya",
      "lastName": "Dakshina Murthy",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67127,
      "firstName": "G S Rajshekar",
      "lastName": "Reddy",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67128,
      "firstName": "Jennifer",
      "lastName": "Jacobs",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67129,
      "firstName": "Thammathip",
      "lastName": "Piumsomboon",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67130,
      "firstName": "Sebastien",
      "lastName": "Miellet",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67131,
      "firstName": "Vedad",
      "lastName": "Hulusic",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67132,
      "firstName": "Fairouz",
      "lastName": "Grioui",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67133,
      "firstName": "Nathan",
      "lastName": "Allison",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67134,
      "firstName": "Yuta",
      "lastName": "Otsuka",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67135,
      "firstName": "Noriki",
      "lastName": "Mochizuki",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67136,
      "firstName": "Heeyoon",
      "lastName": "Jeong",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67137,
      "firstName": "Chong",
      "lastName": "Cao",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67138,
      "firstName": "Tobias",
      "lastName": "Höllerer",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67139,
      "firstName": "Byungsoo",
      "lastName": "Kim",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67140,
      "firstName": "Shunji",
      "lastName": "Muto",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67141,
      "firstName": "Subhankar",
      "lastName": "Ghosh",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67142,
      "firstName": "Huiwen",
      "lastName": "Zhao",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67143,
      "firstName": "Elif",
      "lastName": "Sener",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67144,
      "firstName": "Daniel Agerholm",
      "lastName": "Johansen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67145,
      "firstName": "Wei Hong",
      "lastName": "Lo",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67146,
      "firstName": "Mark",
      "lastName": "Tangney",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67147,
      "firstName": "Myungho",
      "lastName": "Lee",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67148,
      "firstName": "Venkata VB",
      "lastName": "Yallapragada",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67149,
      "firstName": "Chanho",
      "lastName": "Park",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67150,
      "firstName": "Shin",
      "lastName": "Takahashi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67151,
      "firstName": "Taiyo",
      "lastName": "Natomi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67152,
      "firstName": "Alwin",
      "lastName": "Chuan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67153,
      "firstName": "Andrew",
      "lastName": "Zhang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67154,
      "firstName": "Tianshu",
      "lastName": "Xu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67155,
      "firstName": "Ikkaku",
      "lastName": "Kawaguchi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67156,
      "firstName": "Tatsuo",
      "lastName": "Nakajima",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67157,
      "firstName": "Ines",
      "lastName": "Said",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67158,
      "firstName": "Keito",
      "lastName": "Morisaki",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67159,
      "firstName": "Geoffrey",
      "lastName": "Bourgoin",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67160,
      "firstName": "Katsunari",
      "lastName": "Sato",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67161,
      "firstName": "Ohung",
      "lastName": "Kwon",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67162,
      "firstName": "Sune",
      "lastName": "Wolff",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67163,
      "firstName": "Yui",
      "lastName": "Lo",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67164,
      "firstName": "Kei",
      "lastName": "Kobayashi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67165,
      "firstName": "Mingqi",
      "lastName": "Wen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67166,
      "firstName": "Ali",
      "lastName": "Samini",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67167,
      "firstName": "Jaeeun",
      "lastName": "Kim",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67168,
      "firstName": "Leonardo",
      "lastName": "Stella",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67169,
      "firstName": "Alex",
      "lastName": "Kelly",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67170,
      "firstName": "Deniz",
      "lastName": "Mevlevioğlu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67171,
      "firstName": "Michelle",
      "lastName": "Gräsle",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67172,
      "firstName": "Wataru",
      "lastName": "Wakita",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67173,
      "firstName": "Kazuki",
      "lastName": "Sakata",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67174,
      "firstName": "Alex",
      "lastName": "Mikkelsen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67175,
      "firstName": "Yi",
      "lastName": "Li",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67176,
      "firstName": "Hong",
      "lastName": "Guang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67177,
      "firstName": "Tomas",
      "lastName": "Trescak",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67178,
      "firstName": "Takuji",
      "lastName": "Narumi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67179,
      "firstName": "Siyi",
      "lastName": "Liu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67180,
      "firstName": "Yutaro",
      "lastName": "Hirao",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67181,
      "firstName": "Buntarou",
      "lastName": "Shizuki",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67182,
      "firstName": "Lizhou",
      "lastName": "Cao",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67183,
      "firstName": "Ragnar",
      "lastName": "Hrafnkelsson",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67184,
      "firstName": "Stefanie",
      "lastName": "Zollmann",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67185,
      "firstName": "Takayuki",
      "lastName": "Kawamura",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67186,
      "firstName": "Azusa",
      "lastName": "Yamazaki",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67187,
      "firstName": "David",
      "lastName": "Baudry",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67188,
      "firstName": "Anne-Gwenn",
      "lastName": "Bosser",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67189,
      "firstName": "Patrik",
      "lastName": "Goorts",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67190,
      "firstName": "Kentarou",
      "lastName": "Yanase",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67191,
      "firstName": "Dae Seok",
      "lastName": "Kang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67192,
      "firstName": "Jason",
      "lastName": "Ku",
      "middleInitial": "B",
      "affiliations": []
    },
    {
      "id": 67193,
      "firstName": "Stephen",
      "lastName": "Palmisano",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67194,
      "firstName": "Bethany",
      "lastName": "Wootton",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67195,
      "firstName": "Andrew",
      "lastName": "Thomas",
      "middleInitial": "J",
      "affiliations": []
    },
    {
      "id": 67196,
      "firstName": "Rukshani",
      "lastName": "Somarathna",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67197,
      "firstName": "Yuichi",
      "lastName": "Itoh",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67198,
      "firstName": "Shigeo",
      "lastName": "Yoshida",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67199,
      "firstName": "David",
      "lastName": "Murphy",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67200,
      "firstName": "Hyunwoo",
      "lastName": "Cho",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67201,
      "firstName": "Austin",
      "lastName": "Stanbury",
      "middleInitial": "J.",
      "affiliations": []
    },
    {
      "id": 67202,
      "firstName": "Sebastian",
      "lastName": "Oberdörfer",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67203,
      "firstName": "Kaori",
      "lastName": "Ikematsu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67204,
      "firstName": "Fred",
      "lastName": "Charles",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67205,
      "firstName": "Florian",
      "lastName": "Niebling",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67206,
      "firstName": "Hao",
      "lastName": "Wang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67207,
      "firstName": "Akira",
      "lastName": "Utsumi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67208,
      "firstName": "Kazuki",
      "lastName": "Takashima",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67209,
      "firstName": "Gerard",
      "lastName": "Kim",
      "middleInitial": "Jounghyun",
      "affiliations": []
    },
    {
      "id": 67210,
      "firstName": "Sylvain",
      "lastName": "Noblecourt",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67211,
      "firstName": "Hirotake",
      "lastName": "Yamazoe",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67212,
      "firstName": "Karen",
      "lastName": "Moses",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67213,
      "firstName": "Fernando",
      "lastName": "Birra",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67214,
      "firstName": "Chao",
      "lastName": "Peng",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67215,
      "firstName": "Peter",
      "lastName": "Kullmann",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67216,
      "firstName": "Elisabeth",
      "lastName": "Ganal",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67217,
      "firstName": "Connor",
      "lastName": "Mackenzie",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67218,
      "firstName": "Yuichi",
      "lastName": "Mashiba",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67219,
      "firstName": "Simon",
      "lastName": "Hoermann",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67220,
      "firstName": "Li",
      "lastName": "Dong",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67221,
      "firstName": "Yangzi",
      "lastName": "Dong",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67222,
      "firstName": "Abhishek",
      "lastName": "Mukhopadhyay",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67223,
      "firstName": "Ibuki",
      "lastName": "Nomura",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67224,
      "firstName": "Davy",
      "lastName": "Vanacken",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67225,
      "firstName": "Robert",
      "lastName": "Lindeman",
      "middleInitial": "W.",
      "affiliations": []
    },
    {
      "id": 67226,
      "firstName": "Keito",
      "lastName": "Kamimura",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67227,
      "firstName": "Hyo Jeong",
      "lastName": "Kang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67228,
      "firstName": "Bruce",
      "lastName": "Walker",
      "middleInitial": "N.",
      "affiliations": []
    },
    {
      "id": 67229,
      "firstName": "Naoya",
      "lastName": "Hara",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67230,
      "firstName": "Donghyeon",
      "lastName": "Kim",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67231,
      "firstName": "Chang-Gyu",
      "lastName": "Lee",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67232,
      "firstName": "Makoto",
      "lastName": "Sei",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 67320,
      "firstName": "Ryo",
      "lastName": "Suzuki",
      "affiliations": [
        {
          "country": "Canada",
          "institution": "University of Calgary"
        }
      ]
    },
    {
      "id": 67321,
      "firstName": "Duy",
      "lastName": "Le",
      "affiliations": [
        {
          "country": "Sweden",
          "institution": "ABB Research & UEH University"
        }
      ]
    },
    {
      "id": 67322,
      "firstName": "Mehdi",
      "lastName": "Ammi",
      "affiliations": [
        {
          "country": "France",
          "institution": "Univ. Paris 8"
        }
      ]
    },
    {
      "id": 67323,
      "firstName": "Sabah",
      "lastName": "Boustila",
      "affiliations": [
        {
          "country": "United Kingdom",
          "institution": "Manchester Metropolitan University"
        }
      ]
    },
    {
      "id": 67324,
      "firstName": "Chang",
      "lastName": "Liu",
      "affiliations": [
        {
          "country": "Japan",
          "institution": "Kyoto University"
        }
      ]
    },
    {
      "id": 67325,
      "firstName": "Diego",
      "lastName": "Plasencia",
      "middleInitial": "Martinez",
      "affiliations": [
        {
          "country": "United Kingdom",
          "institution": "University College London"
        }
      ]
    }
  ],
  "recognitions": [],
  "publicationInfo": {
    "hideLinksBeforeConference": false,
    "version": 51,
    "publicationStatus": "PUBLISHED",
    "isProgramEnabled": true,
    "isDraft": false,
    "isRegistrationEnabled": false,
    "publicationDate": "2021-12-07 13:46:23+00"
  }
}